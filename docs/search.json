[
  {
    "objectID": "01-introductions.html",
    "href": "01-introductions.html",
    "title": "Welcome to the course!",
    "section": "",
    "text": "Welcome to Intermediate Data Science! Last semester was the first offering of this course–I really enjoyed it. I am even more thrilled to have made improvements that will make it a better learning experience for you!"
  },
  {
    "objectID": "01-introductions.html#plan-for-today",
    "href": "01-introductions.html#plan-for-today",
    "title": "Welcome to the course!",
    "section": "Plan for today",
    "text": "Plan for today\n\nWhat is this course about?\nGet to know your classmates\nShaping our syllabus together\nBrainstorming project ideas and connecting with potential project partners via the 12 Favorite Problems framework\nWarming up our wrangling and visualization skills with Tidy Tuesday!\n\n(When I go through announcements at the end of class, I’ll also go over some syllabus highlights.)"
  },
  {
    "objectID": "01-introductions.html#what-is-this-course-about",
    "href": "01-introductions.html#what-is-this-course-about",
    "title": "Welcome to the course!",
    "section": "What is this course about?",
    "text": "What is this course about?\n\nExpanding your abilities for self-reflection in service of:\n\nYour lifelong independent learning\nOur course community\n\nExpanding your data science toolbox:\n\nVisualization\nWrangling\nData acquisition\nData storytelling\n\n\nI’ve intentionally put reflection first and data science skills second not necessarily in order of importance but because cultivating data science skills will come automatically—reflection and community-building won’t."
  },
  {
    "objectID": "01-introductions.html#get-to-know-your-classmates",
    "href": "01-introductions.html#get-to-know-your-classmates",
    "title": "Welcome to the course!",
    "section": "Get to know your classmates",
    "text": "Get to know your classmates\nIn groups, introduce yourselves with the following prompts: (~2 minutes/person)\n\nName, preferred pronouns\nMacalester connections (e.g., majors/minors/concentrations, clubs, teams, events regularly attended)\nHow are you feeling about the coming semester?\nWhat is one thing you are excited to talk about in conversation?\nIf you could use data to investigate anything, what would it be and why?"
  },
  {
    "objectID": "01-introductions.html#syllabus-shaping-learning-goals",
    "href": "01-introductions.html#syllabus-shaping-learning-goals",
    "title": "Welcome to the course!",
    "section": "Syllabus shaping: learning goals",
    "text": "Syllabus shaping: learning goals\nNavigate to the Course learning goals section of our syllabus.\nPart 1: Reflect (~3 min)\nWrite a few sentences responding to the following questions:\n\nWhat are your goals in taking this class?\nDo you see your goals reflected in the course learning goals? If not, how would you like to see the course goals amended to see your goals reflected in them?\n\nPart 2: Share (~5 min)\nAt your tables, take turns sharing your responses to the above questions. As a group, summarize your discussion in this Google Doc.\nBefore we meet again next Tuesday, I will look over your comments in the Google Doc and add my own responses. I’ll address your comments in class next Tuesday."
  },
  {
    "objectID": "01-introductions.html#course-project-brainstorming",
    "href": "01-introductions.html#course-project-brainstorming",
    "title": "Welcome to the course!",
    "section": "Course project: brainstorming",
    "text": "Course project: brainstorming\nIn a data science course, a course project is essential for synthesizing our tools in a meaningful context.\nOur course project will be a semester-long experience because I believe that this longer time span will improve the quality of the projects.\nWe will start brainstorming ideas today using a framework called the 12 Favorite Problems (12FP)."
  },
  {
    "objectID": "01-introductions.html#favorite-problems-context",
    "href": "01-introductions.html#favorite-problems-context",
    "title": "Welcome to the course!",
    "section": "12 Favorite Problems: context",
    "text": "12 Favorite Problems: context\nRichard Feynman was a Nobel prize-winning physicist whose contributions fundamentally reshaped our understanding of the physical world.\nA major part of his success was a method for viewing the world: a mindset of viewing the world through the lens of several open-ended questions. Feynman called these his “favorite problems.” He said of these problems:\n\nYou have to keep a dozen of your favorite problems constantly present in your mind, although by and large they will lay in a dormant state. Every time you hear or read a new trick or a new result, test it against each of your twelve problems to see whether it helps. Every once in a while there will be a hit, and people will say, “How did [they] do it? [They] must be a genius!”\nQuote source: Forte Labs"
  },
  {
    "objectID": "01-introductions.html#the-12-favorite-problems-framework",
    "href": "01-introductions.html#the-12-favorite-problems-framework",
    "title": "Welcome to the course!",
    "section": "The 12 Favorite Problems framework",
    "text": "The 12 Favorite Problems framework\nA favorite problem is a meaningful, open-ended question that allows you to learn, explore, and act with intention on your biggest interests in life. Here are two of mine:\n\nHow can I be the kind of mother I can feel proud of without losing myself?\nHow can I have a fulfilling career without burning out?\n\nFormulating several of these favorite problems can lead to several benefits:\n\n\nDedicate your time and attention to ideas that truly spark your curiosity\nSee how a piece of information might be useful and why it’s worth keeping\nSee insightful patterns across multiple subjects that seem unrelated, but might share a common thread\nFocus the impact of your work on problems where you can make a real difference\nPrime your subconscious to notice helpful solutions to your biggest challenges in the world around you\nAttract like-minded people who have the same interests and goals as you\n\nSource: Forte Labs\n\nRelevance to our course: Brainstorming 12 favorite problems can help us determine some questions that the course project can address."
  },
  {
    "objectID": "01-introductions.html#brainstorming-our-12-favorite-problems-fps",
    "href": "01-introductions.html#brainstorming-our-12-favorite-problems-fps",
    "title": "Welcome to the course!",
    "section": "Brainstorming our 12 favorite problems (FPs)",
    "text": "Brainstorming our 12 favorite problems (FPs)\n\nOpen up a blank file in which to type.\nNavigate to this article by Tiago Forte, and scroll down to the first step “Get started with these prompts.”\nUsing these prompts, take about 15 minutes to brainstorm your own 12 favorite problems.\n\nTiago Forte provides examples of his 12 FPs in his post. Feel free to also look at my own for more examples. (I’m working on updating my 12 FPs today alongside you!)\n\n\nSave your 12FP file in a place you’ll be able to find easily."
  },
  {
    "objectID": "01-introductions.html#sharing-and-refining-our-12-fps",
    "href": "01-introductions.html#sharing-and-refining-our-12-fps",
    "title": "Welcome to the course!",
    "section": "Sharing and refining our 12 FPs",
    "text": "Sharing and refining our 12 FPs\nIn groups, each person will have ~2 minutes to share their top 2 FPs and get some feedback from the group. The group should give feedback to help make the FPs more specific, counterintuitive, and interdiscipinary:\n\nSpecific:\n\nOriginal: “How can I be a better leader?” is a little broad.\nPossible improvement: “How can I be a better leader as an introvert?”\n\nCounterintuitive:\n\nOriginal: “How can I improve the standard of living in the global south?”\nPossible improvement: “How can I improve the standard of living in the global south without further contributing to the climate change that threatens those regions the most?”\n\nInterdisciplinary:\n\nOriginal: How can I improve education?”\nPossible improvement: “How can I improve education by borrowing ideas from video games?”\n\n\n(Examples from Forte Labs)"
  },
  {
    "objectID": "01-introductions.html#broadcast-your-signal-to-start-finding-your-people",
    "href": "01-introductions.html#broadcast-your-signal-to-start-finding-your-people",
    "title": "Welcome to the course!",
    "section": "Broadcast your signal to start finding your people",
    "text": "Broadcast your signal to start finding your people\n\nJoin our course Slack workspace via this invite link.\n\nIf you’ve already joined, navigate to our Slack workspace here.\n\nIn the #general channel, write a very brief post in which you:\n\nIntroduce yourself however you see fit.\nDescribe the general areas that your 12 favorite problems tend to cover.\nIf you already feel a pull towards a project area, share that too."
  },
  {
    "objectID": "01-introductions.html#project-opportunity-collaborating-with-a-community-partner",
    "href": "01-introductions.html#project-opportunity-collaborating-with-a-community-partner",
    "title": "Welcome to the course!",
    "section": "Project opportunity: collaborating with a community partner",
    "text": "Project opportunity: collaborating with a community partner\nIgnite Afterschool is looking to partner with data science students on a few fronts:\n\nThey are looking to update the data briefs that they use to communicate with families about the impact of afterschool programming.\n\nThis will involve looking at data from the Minnesota Student Survey. (I already have access to this data.)\nThere is opportunity for creative input from students on how best to display this information.\n\nThey are also looking for help with a data-driven policy analysis surrounding the impact of the recent cannabis legalization laws on youth outcomes. There is potential to explore this with the MN Student Survey data as well as other data.\n\nIf you are interested in this opportunity, please reach out to me by Tuesday, January 23."
  },
  {
    "objectID": "01-introductions.html#tidy-tuesday",
    "href": "01-introductions.html#tidy-tuesday",
    "title": "Welcome to the course!",
    "section": "Tidy Tuesday!",
    "text": "Tidy Tuesday!\nFor the remainder of the class period, we’ll work on the most recent Tidy Tuesday challenge.\nFeel free to clarify anything about the course with me during this time!"
  },
  {
    "objectID": "01-introductions.html#announcements",
    "href": "01-introductions.html#announcements",
    "title": "Welcome to the course!",
    "section": "Announcements",
    "text": "Announcements\nBefore class on Tuesday, please do the following:\n\nSet up R and RStudio using these instructions.\nUpdate your Slack profile with preferred name, pronouns, name pronunciation. (To find your profile, click on your name under Direct Messages on the left menu, and click “Edit Profile”.)\nComplete the pre-course survey.\nLook at the Guiding Questions for next Tuesday’s class on advanced visualization with ggplot2.\nTake a look at Homework 1."
  },
  {
    "objectID": "02-adv-ggplot.html",
    "href": "02-adv-ggplot.html",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nNavigate the ggplot2 reference page to find the functions needed to create a desired visualization\nUse the information on a function help page to construct desired plot features\n\nScan the information in the Usage section to identify function arguments that must be set\nUnderstand how the function arguments work by using information in the Arguments section\nUse the information in the the Aesthetics and Examples sections to control plot appearance\n\nIdentify when it would be necessary to use different data arguments within the ggplot() and geom_() layers"
  },
  {
    "objectID": "02-adv-ggplot.html#the-goal",
    "href": "02-adv-ggplot.html#the-goal",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "The goal",
    "text": "The goal\nWe are going to recreate this NYT visualization on record setting temperatures by expanding our ggplot2 toolbox using data from San Francisco (SFO) in 2011.\n\n\n\nScreenshot of NYTimes visualization from 2015"
  },
  {
    "objectID": "02-adv-ggplot.html#setup",
    "href": "02-adv-ggplot.html#setup",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "Setup",
    "text": "Setup\nFirst load the tidyverse package, and read in the San Francisco weather data.\n\nlibrary(tidyverse)\nweather &lt;- read_csv(\"https://lmyint.github.io/212_spring_2024/data/sfo_weather.csv\")"
  },
  {
    "objectID": "02-adv-ggplot.html#codebook",
    "href": "02-adv-ggplot.html#codebook",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "Codebook",
    "text": "Codebook\nYou will need to refer to the variable codebook below throughout this activity.\n\nMonth: Month of the year (1-12)\nDay: Day within the month (1-31)\nLow/High: Low/high temperature this day\nNormalLow/NormalHigh: Typical low/high temperature for this day of the year\nRecordLow/RecordHigh: Record low/high temperature for this day of the year\nLowYr/HighYr: Year in which the record low/high was observed\nPrecip: Amount of precipitation (inches) this day\nRecordPrecip: Record amount of precipitation for this day of the year\nPrecipYr: Year in which the record precipitation was observed\ndate: The actual date in 2011 for this day in YYYY-MM-DD format\ndateInYear: What day of the year is it? (1-365)\nRecord: Logical (TRUE/FALSE) indicating whether this day had a high temperature record\nRecordText: Text that displays the record high for this day (\"Record high: ##\")\nRecordP: Logical (TRUE/FALSE) indicating whether this day had a precipitation record\nCulmPrec: Cumulative precipitation for the month up to this day"
  },
  {
    "objectID": "02-adv-ggplot.html#class-exercises",
    "href": "02-adv-ggplot.html#class-exercises",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "Class exercises",
    "text": "Class exercises\nClass exercise 1: Examine the temperature visualization in the original NYT article.\n\nData storytelling: Relate the intro paragraph (“Scientists declared that 2015 was Earth’s hottest year on record…”) to the design of the visualization: Based on the intro paragraph, what key message/claim does NYT want readers to be able to explore? How did this goal inform what information is displayed in the visualization?\nTechnical implementation: What specific variables (from the codebook) underlie the visualization, and how do they map to visual elements (e.g., position, size, shape, and color of the glyphs)?\n\n\nWe can explore the “Geoms” section of the ggplot2 reference page to find a geom that corresponds to the visual elements in the temperature plot.\nClass exercise 2: Using both the small example visuals on the right and the names of the geom’s, brainstorm some possibilities for geom’s we might use to recreate the temperature visualization.\n\nWe need to explore further by opening up the geom reference pages to understand if a particular geom is suitable for our task. We’ll look at the geom_point documentation page to learn the process for reading a documentation page.\nWhen looking at a help page, it is useful to first look at the Usage and Arguments sections.\nThe Usage section shows all of the possible inputs (arguments) to the geom–these are all of the ways that a geom can be customized. Just looking at the argument names can help give a hint as to what arguments might fit our needs.\nThe Arguments section explains in detail what each argument does and the possible values the argument can take. The mapping, data, and ... arguments will be the most commonly used by far.\n\nmapping: This is the argument that is being used when you specify a plots aesthetics (the code inside aes()).\ndata: This is where you specify the dataset containing the variables that the geom is using.\n...: You will tend to use this for fixed aesthetics (ones that don’t correspond to a variable). For example, this is where you can set the color of all points (e.g., with color = \"red\") or the size of all points (e.g., with size = 3).\n\nA note about the data argument: Previously you have used one dataset per plot by specifying that as the first argument of ggplot(). For example, the code below makes a scatterplot of price vs. carat in the diamonds dataset, and the only data argument is in ggplot() (none in geom_point()).\n\ndata(diamonds)\nhead(diamonds)\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\nggplot(diamonds, aes(x = carat, y = price)) +\n    geom_point() +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n# Note that we can make the same plot by specifying the argument names explicitly:\nggplot(data = diamonds, mapping = aes(x = carat, y = price)) +\n    geom_point() +\n    theme_classic()\n\nSometimes we may want to use more than one dataset in a plot. For example, we have a separate dataset that contains average prices of diamonds by carat:\n\nhead(diamonds_avg_price)\n\n# A tibble: 6 × 2\n  carat avg_price\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  0.2       365.\n2  0.23      486.\n3  0.26      551.\n4  0.29      601.\n5  0.32      720.\n6  0.35      801.\n\n\nWe can use this separate diamonds_avg_price dataset in the geom_point() layer to add average price information to our scatterplot:\n\nggplot(diamonds, aes(x = carat, y = price)) +\n    geom_point() +\n    geom_point(data = diamonds_avg_price, aes(x = carat, y = avg_price), color = \"deepskyblue\", size = 3)\n\n\n\n\n\n\n\n\nThe Aesthetics section of a geom documentation page gives information on how the visual elements of the geom correspond to data. For example, the geom_point documentation page shows that the familiar x and y aesthetics are available. It also shows some new aesthetics like stroke.\nWe can use the same process to look at the geom_linerange documentation page and start off our temperature visualization with the record lows and highs:\n\nggplot(weather) +\n    geom_linerange(aes(x = dateInYear, ymin = RecordLow, ymax = RecordHigh), color = \"#ECEBE3\", linewidth = 1.5) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeyboard shortcuts\n\n\n\nAs you work on this plot, try to use some new keyboard shortcuts. Focus on the following:\n\nInsert code chunk: Ctrl+Alt+I (Windows). Option+Command+I (Mac).\nRun current code chunk: Ctrl+Shift+Enter (Windows). Command+Shift+Return (Mac).\nRun current line/currently selected lines: Ctrl+Enter (Windows). Command+Return (Mac).\n\n\n\nClass exercise 3: Add to your temperature visualization to also display the usual temperatures (NormalLow and NormalHigh), and actual 2011 temperatures (Low and High). Your plot should look like the one below. The tan color for the usual temperatures is \"#C8B8BA\", and the red color for the actual temperatures is \"#A90248\".\n\n\n\n\n\n\n\n\n\n\nLet’s now try to recreate the visual demarcations of the months by adding vertical lines separating the months.\nClass exercise 4: Brainstorm with your groups how we might draw those vertical lines. What geom might we use? What subset of the data might we use in that geom layer to draw lines only at the month divisions? One person from your group should write your ideas on the board.\nOnce ideas are up on the board, we’ll work through this together as a class.\n\nNow let’s change the x-axis labels so that the month names display in the center of each month’s slice of the plot. (Note that R has built-in variables called month.abb and month.name that contain abbreviated and full month names.)\nClass exercise 5: We will explore two different approaches to figuring out this new challenge: Google search and AI.\n\nGoogle: Start by just using Google search queries. Collaborate with your group to try to word your search queries as carefully as possible (using the jargon that is most likely to return the most relevant results). Record search queries and your thought process in selecting which search results to look at first.\nAI: Next use ChatGPT. Collaborate with your group to brainstorm a series of prompts that will most efficiently get you the desired results. Record the chat prompts used and output given. Evaluate the output. Do you fully understand the code generated? How can you tell that the generated code is correct?\n\nAfter we debrief on these approaches, we’ll finalize this part of the plot together."
  },
  {
    "objectID": "02-adv-ggplot.html#group-work",
    "href": "02-adv-ggplot.html#group-work",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "Group work",
    "text": "Group work\nWork together until your precipitation plot looks as below.\n\nThe triangles point to precipitation records. Refer to the Codebook above for the RecordP variable.\nThe numbers on the plot indicate the total precipitation for the month. Do some searching about the hjust and vjust options to adjust the alignment of the numbers.\nThe blue and tan colors are \"#32a3d8\" and \"#ebeae2\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReminder: Record and observe\n\n\n\nAs you work through this new phase of the plot, makes notes in your personal class journal about anything that you tried that didn’t work the way you wanted: geoms that you tried but didn’t make the right plot, faulty aesthetic mappings, error messages, and warning messages.\nAlso be aware of your comforts and discomforts in this collaborative environment. Pay attention to the comforts and discomforts of your groupmates.\nWe’ll have a few minutes at the end of class to reflect holistically on today’s activity.\n\n\n\nIf you have time, keep working with each other on your plots to get them to look as below (which shows your goal for Challenge 1).\n\n\n\nSFO weather in 2011 (minimum requirements for Challenge 1 submission)"
  },
  {
    "objectID": "02-adv-ggplot.html#reflect",
    "href": "02-adv-ggplot.html#reflect",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "Reflect",
    "text": "Reflect\nIn terms of both coding and collaboration, what challenges did you face today? What did you do to address those challenges? What would you like to try for next time?\nTake a few minutes to write some thoughts in your personal course journal."
  },
  {
    "objectID": "03-file-org-github.html",
    "href": "03-file-org-github.html",
    "title": "File organization, GitHub",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nSet up an organized directory structure for data science projects\nExplain the difference between absolute and relative file paths and why relative file paths are preferred for reading in data\nConstruct relative file paths to read in data\nUse the following git commands within RStudio (whether via the GUI or by the Terminal command line): clone, add, commit, push"
  },
  {
    "objectID": "03-file-org-github.html#folderdirectory-structure",
    "href": "03-file-org-github.html#folderdirectory-structure",
    "title": "File organization, GitHub",
    "section": "Folder/directory structure",
    "text": "Folder/directory structure\nMinimal directory structure for a data science project: (Sub-bullets indicate folders that are inside other folders.) At minimum, a data science project should have a code, data, and results folder. Not having these folders and mixing code, data, and results files all in one folder can quickly get hard to navigate for even small projects.\n\nDocuments (This should be some place you can find easily through your Finder (Mac) or File Explorer (Windows).)\n\ndescriptive_project_name\n\ncode: All code files (.R, .Rmd, .qmd) should go here. Recommendation:\n\ncleaning.qmd: for data acquisition and wrangling. Save (write) the cleaned dataset at the end of this file with readr::write_csv().\nvisualizations.qmd: for exploratory and final plots\nmodeling.qmd: for any statistical or predictive modeling\n\ndata: All data files go here\nresults: e.g., plots saved as images, results tables\n\n\n\n\nMore involved directory structure for a data science project: If you have a larger-scale project that involves a lot more code and data, the expanded directory structure below is useful:\n\nDocuments\n\ndescriptive_project_name\n\ncode\n\nraw: For messy code that you’re actively working on\nclean: For code that you have cleaned up, documented, organized, and tested to run as expected\n\ndata\n\nraw: Original data that hasn’t been cleaned\nclean: Any non-original data that has been processed in some way\n\nresults\n\nfigures: Plots that will be used in communicating your project should go here. (Using screenshots of output in RStudio is not a good practice.)\ntables: Any sort of plain text file results (e.g., CSVs)"
  },
  {
    "objectID": "03-file-org-github.html#file-paths",
    "href": "03-file-org-github.html#file-paths",
    "title": "File organization, GitHub",
    "section": "File paths",
    "text": "File paths\n\n\n\n\n\n\nWhat are file paths?\n\n\n\nA file path is a text string that tells a computer how to navigate from one location to another. We use file paths to read in (and write out) data.\nEssentially, file paths are what go inside read_csv().\n\n\nThere are two types of paths: absolute and relative.\nAbsolute file paths start at the “root” directory in a computer system. Examples:\n\nMac: /Users/lesliemyint/Desktop/teaching/STAT212/2024_spring/activities/adv_ggplot/sfo_weather.csv\n\nOn a Mac the tilde ~ in a file path refers to the “Home” directory, which is /Users/lesliemyint. In this case, the path becomes ~/Desktop/teaching/STAT212/2024_spring/activities/adv_ggplot/sfo_weather.csv\n\nWindows: C:/Users/lesliemyint/Documents/teaching/STAT212/2024_spring/activities/adv_ggplot/sfo_weather.csv\n\nNote: Windows uses both / (forward slash) and \\ (backward slash) to separate folders in a file path.\n\n\n\n\n\n\n\n\nDON’T use absolute paths\n\n\n\nFor reading in data, absolute paths are not a good idea because if the code file is shared. The path will not work on a different computer.\n\n\n\nRelative file paths start wherever you are right now (the working directory (WD)). The WD when you’re working in a code file (.Rmd, .qmd) may be different from the working directory in the Console.\nDirectory setup 1:\n\nsome_folder\n\nyour_code_file.qmd\ndata.csv\n\n\nTo read in the data:\n\ndata &lt;- read_csv(\"data.csv\")\n\nDirectory setup 2:\n\nsome_folder\n\nyour_code_file.qmd\ndata\n\ndata.csv\n\n\n\nTo read in the data:\n\ndata &lt;- read_csv(\"data/data.csv\")\n\nDirectory setup 3:\n\nsome_folder\n\ndata.csv\ncode\n\nyour_code_file.qmd\n\n\n\nTo go “up” a folder in a relative path we use ../. To read in the data:\n\ndata &lt;- read_csv(\"../data.csv\")\n\nDirectory setup 4:\n\nsome_folder\n\ndata\n\ndata.csv\n\ncode\n\nyour_code_file.qmd\n\n\n\nTo read in the data:\n\ndata &lt;- read_csv(\"../data/data.csv\")\n\n\n\n\n\n\n\nDO use relative paths\n\n\n\nFor reading in data, relative paths are preferred because if the project directory structure is used on a different computer, the relative paths will still work."
  },
  {
    "objectID": "03-file-org-github.html#exercise",
    "href": "03-file-org-github.html#exercise",
    "title": "File organization, GitHub",
    "section": "Exercise",
    "text": "Exercise\nDownload this Zip file from Moodle, and save it to your class folder. After unzipping, open the code/clean/cleaning.qmd file in RStudio. Follow the instructions in that file.\nExercise goals:\n\nPractice using relative paths in a realistic project context\nPractice data wrangling\n\n\n\nSolution\n\nLoad packages and read in data.\n\nlibrary(tidyverse)\nweather &lt;- read_csv(\"../../data/raw/weather.csv\")\n\nAdd dateInYear variable.\n\nweather_clean &lt;- weather %&gt;% \n    arrange(Month, Day) %&gt;% \n    mutate(dateInYear = 1:365)\n\nAdd in 3-letter month abbreviations.\n\n# Option 1: via joins\nmonths &lt;- tibble(\n    Month = 1:12,\n    month_name = month.abb\n)\nweather_clean &lt;- weather_clean %&gt;% \n    left_join(months)\n\n# Option 2: via vector subsetting\nweather %&gt;% \n    mutate(month_name = month.abb[Month])\n\nWrite out clean data to a CSV file.\n\nwrite_csv(weather_clean, file = \"../../data/clean/weather_clean.csv\")"
  },
  {
    "objectID": "04-adv-maps.html",
    "href": "04-adv-maps.html",
    "title": "Advanced spatial visualization",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nUnderstand the basics of a CRS (coordinate reference system)\nUnderstand and recognize different spatial file types and data types in R\nImplement some of the basic plotting with the sf package\nUnderstand foundational ideas in working with spatial data (aggregating spatial point data to a spatial region, joining spatial data sets)"
  },
  {
    "objectID": "04-adv-maps.html#ellipsoid",
    "href": "04-adv-maps.html#ellipsoid",
    "title": "Advanced spatial visualization",
    "section": "Ellipsoid",
    "text": "Ellipsoid\n\nThe Earth is not a sphere.\n\nIt’s closer to a bumpy ellipsoid with a bulge at the equator.\n\nThe ellipsoid part of a CRS is a mathematical model giving a smooth approximation to Earth’s shape.\nCommon ellipsoid models: WGS84 and GRS80\n\n\n\n\nIllustration of ellipsoid model (black) and Earth’s irregular surface (red), centered to have an overall best fit. Source: www.icsm.gov.au"
  },
  {
    "objectID": "04-adv-maps.html#datum",
    "href": "04-adv-maps.html#datum",
    "title": "Advanced spatial visualization",
    "section": "Datum",
    "text": "Datum\n\nWhere do we center the ellipsoid? This center is called a datum.\nFor a given ellipsoid model, different datums are used to better fit different areas of the world.\n\ne.g., For the GRS80 ellipsoid, the NAD83 datum is a good fit in North America, but SIRGAS2000 is a better fit in South America.\nThe Global Positioning System (GPS) uses the WGS84 ellipsoid model and WGS84 datum. This provides an overall best fit of the Earth.\n\n\n\n\n\nIllustration of ellipsoid model and Earth’s irregular surface for a datum that better fits southern part (bottom right) of the Earth. Source: www.icsm.gov.au\n\n\n\n\n\n\n\n\nWhy do the ellipsoid and datum matter?\n\n\n\nIf you have longitude and latitude coordinates for a location, you need to know what datum and ellipsoid were used to define those positions in order to overlay those points correctly on a map.\nNote: In practice, the horizontal distance between WGS84 and NAD83 coordinates is about 3-4 feet in the US, which may not be significant for most applications."
  },
  {
    "objectID": "04-adv-maps.html#projection",
    "href": "04-adv-maps.html#projection",
    "title": "Advanced spatial visualization",
    "section": "Projection",
    "text": "Projection\nLastly, the Earth lives in a 3 dimensional (3D) world and most visualizations are on a 2 dimensional (2D) surface. We must choose a projection method to represent points, regions, and lines on Earth on a 2D map with distance units (typically meter, international foot, US survey foot). In that projection process, a 3D element will lose angle, area, and/or distance when projected onto a 2D surface, no matter which method is chosen.\n\nFor a good overview of common projection methods, see https://pubs.usgs.gov/gip/70047422/report.pdf.\nCommon projection: Mercator projection\n\nCylindrical map projection from the 1500’s\nUseful for navigation because it represented north as up and south as down everywhere and preserves local directions and shape\nDrawback: it inflates the size of regions far from the equator. Greenland, Antarctica, Canada, and Russia appear much bigger than they should. The illustration below compares country areas/shapes under the Mercator projection (light blue) with true areas/shapes (dark blue).\n\n\n\n\n\nSource: @neilrkaye\n\n\nBelow you can see four different world projections. Take note of what is lost in terms of angle, area, or distance in these projections.\n\nworld &lt;- rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Basic Map w/ labels\nggplot(data = world) + \n    geom_sf(color = \"black\", fill = \"#bada55\") +\n    labs(x = \"Longitude\", y = \"Latitude\", title = \"World Map - Mercator Projection\", subtitle = paste0(\"(\", length(unique(world$name)), \" countries)\")) +\n    theme_bw() \n\n\n\nggplot(data = world) +\n    geom_sf(color = \"black\", fill = \"#bada55\") +\n    coord_sf(crs = \"+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs\") + \n    labs(title = \"Lambert Azimuthal Equal-Area Projection\", subtitle = \"Correctly represents area but not angles\") + \n    theme_bw()\n\n\n\nggplot(data = world) +\n    geom_sf(color = \"black\", fill = \"#bada55\") +\n    coord_sf(crs = \"+proj=fouc\") + \n    labs(title = \"Foucaut Projection\", subtitle = \"Correctly represents area, lots of shape distortion in high latitudes\") + \n    theme_bw() \n\n\n\nggplot(data = world) +\n    geom_sf(color = \"black\", fill = \"#bada55\") +\n    coord_sf(crs = \"+proj=natearth2\") + \n    labs(title = \"Natural Earth II Projection\", subtitle = \"Represents globe shape, distorted at high latitudes\") + \n    theme_bw()"
  },
  {
    "objectID": "04-adv-maps.html#vector",
    "href": "04-adv-maps.html#vector",
    "title": "Advanced spatial visualization",
    "section": "Vector",
    "text": "Vector\nVector data represents the world as a set of spatial geometries that are defined in terms of location coordinates (with a specified CRS) with non-spatial attributes or properties.\nThe three basic vector geometries are:\n\nPoints: Locations defined based on a (x, y) coordinates.\n\ne.g., Cities\n\nLines: A set of ordered points connected by straight lines.\n\ne.g., Roads, rivers\n\nPolygons: A set of ordered points connected by straight lines, first and last point are the same.\n\ne.g., Geopolitical boundaries, bodies of water\n\n\nFile formats:\n\nText files (e.g., .csv)\n\nx and y columns: for coordinates (x for longitude and y for latitude)\ngroup id column: needed for lines and polygons\nadditional columns: attributes related to the areas in each row (e.g., population sizes, demographic information)\nText files do not store the CRS.\n\nShapefiles (.shp)\n\nWidely supported spatial vector file format (that includes the CRS).\n\nGeoJSON (.geojson) (Geographical Javascript Object Notation)\nKML (.kml) (Keyhole Markup Language)"
  },
  {
    "objectID": "04-adv-maps.html#raster",
    "href": "04-adv-maps.html#raster",
    "title": "Advanced spatial visualization",
    "section": "Raster",
    "text": "Raster\n\n\n\nDifference between vector and raster formats. Source: gis.stackexchange.com\n\n\n\nContinuous grid of cells where each cell has a single value.\n\nContinuous values (e.g., elevation, precipitation)\nCategorical values (e.g., land cover type, soil type)\n\nShape of cells\n\nGenerally square (like pixels)\nCells can be rotated and sheared. Rectilinear and curvilinear shapes are also possible, depending on the spatial region of interest and CRS.\n\n\n\n\n\nRaster cell shapes (Source)\n\n\n\n\n\n\n\n\nComputational time considerations\n\n\n\nHigh resolution raster data involves a large number of small cells. This results in large file sizes and objects which can make computation and visualization quite slow.\n\n\nFile formats:\n\nGeoTIFF (.tif or .tiff)\n\nMost popular\n\nNetCDF (.nc)\nHDF (.hdf)\n\nTo work with raster data in R, you’ll use the raster, terra, and the stars packages. If you are interested in learning more, check out https://r-spatial.github.io/stars/."
  },
  {
    "objectID": "04-adv-maps.html#get-county-boundaries",
    "href": "04-adv-maps.html#get-county-boundaries",
    "title": "Advanced spatial visualization",
    "section": "Get county boundaries",
    "text": "Get county boundaries\nWe’ve already read in city location and water information from external shapefiles. We can access county boundaries with the us_counties() function in the USAboundaries package.\n\n# Load country boundaries data as sf object\nmn_counties &lt;- USAboundaries::us_counties(resolution = \"high\", states = \"Minnesota\")\n\n# Take care of duplicate column names (there are two identical \"state_name\" columns)\nnames_counties &lt;- names(mn_counties)\nnames(mn_counties)[names_counties == \"state_name\"] &lt;- c(\"state_name1\", \"state_name2\")"
  },
  {
    "objectID": "04-adv-maps.html#unifying-crss-across-different-spatial-datasets",
    "href": "04-adv-maps.html#unifying-crss-across-different-spatial-datasets",
    "title": "Advanced spatial visualization",
    "section": "Unifying CRSs across different spatial datasets",
    "text": "Unifying CRSs across different spatial datasets\nWe first need to ensure that the CRS is the same for all spatial datasets.\nExercise:\n\nCheck the CRS for the mn_cities, mn_water, and mn_counties datasets.\nIf the datasets don’t all have the same CRS, use st_transform() to update the datasets to have the same CRS as mn_cities. You can use crs = st_crs(mn_cities) within st_transform().\n\n\n\nSolution\n\n\n# Check CRSs\nst_crs(mn_cities)\nst_crs(mn_water)\nst_crs(mn_counties) # mn_counties is different!\n\n# Transform the CRS of county data to the more local CRS of the cities\nmn_counties &lt;- mn_counties %&gt;%\n    st_transform(crs = st_crs(mn_cities))\n\n# Check the new CRS for mn_counties\nst_crs(mn_counties)"
  },
  {
    "objectID": "04-adv-maps.html#initial-map-counties-and-cities",
    "href": "04-adv-maps.html#initial-map-counties-and-cities",
    "title": "Advanced spatial visualization",
    "section": "Initial map: counties and cities",
    "text": "Initial map: counties and cities\nExercise: Create a map where city locations are overlaid on a map of county boundaries.\n\nYou will need to call geom_sf() twice.\nMake the map background white.\nInstall the ggthemes package, and add the following layer to use a clean map theme: + ggthemes::theme_map()\n\n\n\nSolution\n\n\n# Option 1\nggplot() + # plot frame\n    geom_sf(data = mn_counties, fill = \"white\") + # county boundary layer\n    geom_sf(data = mn_cities, size = 0.5) + # city point layer\n    ggthemes::theme_map()\n\n# Option 2\nggplot(mn_counties) + # plot frame\n    geom_sf(fill = \"white\") + # county boundary layer\n    geom_sf(data = mn_cities, size = 0.5) + # city point layer\n    ggthemes::theme_map()\n\n\nWe can use traditional ggplot2 aesthetics (e.g., fill, color) to display location specific attributes. Below we only plot large cities, and we color and size cities according to their population.\n\nggplot() +\n    geom_sf(data = mn_counties, fill = \"white\") + \n    geom_sf(data = mn_cities %&gt;% filter(Population &gt;= 10000), mapping = aes(color = Population, size = Population)) + # cities layer\n    scale_color_viridis_c() + # continuous (gradient) color scale\n    labs(title = \"Minnesota Cities with Population &gt;= 10,000\") + \n    ggthemes::theme_map() +\n    theme(legend.position = \"bottom\") # move legend\n\nExercise: Look up the scale_color_viridis_c() documentation via the ggplot2 reference.\n\nRead the function description at the top. What is the advantage of using this function for making color palettes?\nLook through the examples section. What is the difference between the _d(), _c(), and _b() variants of this function?\n\n\n\nSolution\n\nThe viridis color scale results in plots that can be interpreted analogously whether in color or black and white and is color-blind friendly.\n\nThe _d() variant is used when color is mapped to a discrete (categorical) variable.\nThe _c() variant is used when color is mapped to a continuous variable.\nThe _b() variant is used when color is mapped to a continuous variable but when we want that continuous variable to be binned so that there is a small set of colors."
  },
  {
    "objectID": "04-adv-maps.html#adding-elevation-data",
    "href": "04-adv-maps.html#adding-elevation-data",
    "title": "Advanced spatial visualization",
    "section": "Adding elevation data",
    "text": "Adding elevation data\nWhere are large cities located? Is there some relationship to local geography/terrain? To investigate these questions, we can obtain elevation data to include on the map using the elevatr package. We encounter two new functions here—we can look up their documentation to make sense of the code by entering the following in the Console:\n\n?elevatr::get_elev_raster\n?terra::as.data.frame\n\n\nelevation &lt;- elevatr::get_elev_raster(mn_counties, z = 5, clip = \"bbox\")\nraster::crs(elevation) &lt;- sf::st_crs(mn_counties)\n\n# Convert to data frame for plotting\nelev_df &lt;- elevation %&gt;% terra::as.data.frame(xy = TRUE)\ncolnames(elev_df) &lt;- c(\"x\", \"y\", \"elevation\")\n\nExercise: Build on our existing map by adding a raster layer for elevation as the background.\n\nLook up the documentation for geom_raster() to plot the elevation data from elev_df. This will be the first layer of the plot.\nLook at the documentation for scale_fill_gradient() to add the following elevation color scale: \"darkgreen\" represents the lowest elevations, and \"white\" represents the highest elevations.\nAdd in the layers from the map above to show the largest cities and the county outlines. To remove a background color, use fill = NA.\n\n\n\nSolution\n\n\nggplot() +\n    geom_raster(data = elev_df, aes(x = x, y = y, fill = elevation)) + # adding the elevation as first (bottom) layer\n    scale_fill_gradient(low = \"darkgreen\", high = \"white\", guide = FALSE) +\n    geom_sf(data = mn_counties, fill = NA, color = \"black\") + \n    geom_sf(data = mn_cities %&gt;% filter(Population &gt;= 10000), mapping = aes(color = Population, size = Population))+ # cities layer\n    scale_color_viridis_c() + # continuous (gradient) color scale\n    labs(title = \"Minnesota Cities with Population &gt;= 10,000\") + \n    ggthemes::theme_map() +\n    theme(legend.position = \"bottom\") # move legend"
  },
  {
    "objectID": "04-adv-maps.html#zoom-in-to-twin-cities-and-add-water",
    "href": "04-adv-maps.html#zoom-in-to-twin-cities-and-add-water",
    "title": "Advanced spatial visualization",
    "section": "Zoom in to Twin Cities and add water",
    "text": "Zoom in to Twin Cities and add water\nThe bulk of the interesting information in this map is in the Twin Cities area. Let’s zoom in to this area.\n\nWe can use the st_bbox() function to get the bounding box for a spatial object—we do this after filtering to the 7 counties in the Twin Cities.\nWe then use st_crop() to trim a spatial object to a given bounding box.\n\n\nseven_countyarea &lt;- mn_counties %&gt;%\n    filter(name %in% c(\"Anoka\", \"Hennepin\", \"Ramsey\", \"Dakota\", \"Carver\", \"Washington\", \"Scott\")) %&gt;% \n    st_bbox()\nseven_countyarea\n\nelevation &lt;- elevatr::get_elev_raster(mn_counties %&gt;% st_crop(seven_countyarea), z = 9, clip = \"bbox\")\nraster::crs(elevation) &lt;- sf::st_crs(mn_counties)\n\n# Convert to data frame for plotting\nelev_df &lt;- elevation %&gt;% terra::as.data.frame(xy = TRUE)\ncolnames(elev_df) &lt;- c(\"x\", \"y\", \"elevation\")\n\nIn the plot below, we add a layer for water information and a coord_sf() layer to restrict the x and y-axis limits to the Twin Cities bounding box. (Without this layer, the map would zoom back out to show all counties and bodies of water).\n\nggplot() +\n    geom_raster(data = elev_df, aes(x = x, y = y, fill = elevation)) + \n    geom_sf(data = mn_counties, fill = NA, color = \"black\") + # county boundary layer\n    geom_sf(data = mn_water, fill = \"lightsteelblue1\", color = \"lightsteelblue1\") + # NEW: river/lake layer\n    geom_sf(data = mn_cities %&gt;% filter(Population &gt;= 10000), mapping = aes(color = Population, size = Population)) + # cities layer\n    scale_color_viridis_c(option = \"magma\") + # continuous (gradient) color scale\n    scale_fill_gradient(low = \"darkgreen\", high = \"white\") + # continuous (gradient) fill scale\n    coord_sf(xlim = seven_countyarea[c(\"xmin\", \"xmax\")], ylim = seven_countyarea[c(\"ymin\", \"ymax\")]) + # NEW: crop map to Twin Cities bounding box\n    labs(title = \"Twin Cities with Population &gt;= 10,000\") + \n    ggthemes::theme_map() +\n    theme(legend.position = \"none\") # remove legend\n\nExercise: Let’s add to the above code chunk to save the map above to an image file called tc_map_zoom.png in the results folder. The code example below shows a general template for saving a plot to file. Choose a reasonable width and height. (There are also jpeg() and pdf() functions for writing images.)\n\npng(\"relative path to image\", width = width_in_pixels, height = height_in_pixels)\n# Code for creating plot\ndev.off()"
  },
  {
    "objectID": "04-adv-maps.html#twin-cities-map-with-leaflet",
    "href": "04-adv-maps.html#twin-cities-map-with-leaflet",
    "title": "Advanced spatial visualization",
    "section": "Twin Cities map with leaflet",
    "text": "Twin Cities map with leaflet\nBelow we show how to make the MN counties map in the leaflet package.\n\nlibrary(leaflet)\n\nmn_counties_leaf &lt;- mn_counties %&gt;% st_transform(4326) # Leaflet expects this CRS for vectors\nmn_cities_leaf &lt;- mn_cities %&gt;% st_transform(4326)\n\ncities_per_county &lt;- st_join(mn_cities_leaf, mn_counties_leaf) %&gt;%\n    st_drop_geometry() %&gt;% # removes geometry - makes the following calculation more efficient\n    count(name) \n\nmn_counties_leaf %&gt;% \n    filter(name %in% c(\"Anoka\", \"Hennepin\", \"Ramsey\", \"Dakota\", \"Carver\", \"Washington\", \"Scott\")) %&gt;%\n    left_join(cities_per_county) %&gt;%\n    leaflet() %&gt;% \n    addProviderTiles(\"CartoDB.Positron\") %&gt;% \n    addPolygons(\n        color = \"#444444\", weight = 1, smoothFactor = 0.5, opacity = 1.0,\n        fillOpacity = 0.5, fillColor = ~colorQuantile(\"YlOrRd\", n)(n),\n        highlightOptions = highlightOptions(color = \"white\", weight = 2, bringToFront = TRUE)) %&gt;%\n    addCircles(data = mn_cities_leaf %&gt;% filter(County %in% paste(c(\"Anoka\", \"Hennepin\", \"Ramsey\", \"Dakota\", \"Carver\", \"Washington\", \"Scott\"), \"County\")), color = \"#444444\")"
  },
  {
    "objectID": "05-interactive-viz.html",
    "href": "05-interactive-viz.html",
    "title": "Interactive visualization",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nEvaluate when it would be useful to use an interactive visualization or an animation and when it might not be necessary\nConstruct interactive visualizations and animations with plotly\nBuild a Shiny app that enables user to adjust visualization choices and explore linked visualizations"
  },
  {
    "objectID": "05-interactive-viz.html#pros-and-cons-of-interactivity",
    "href": "05-interactive-viz.html#pros-and-cons-of-interactivity",
    "title": "Interactive visualization",
    "section": "Pros and cons of interactivity",
    "text": "Pros and cons of interactivity\nPros\n\nUsers can click, hover, zoom, and pan to get more detailed information\nUsers can get quickly and deeply explore the data via linked data representations\nAllows guided exploration of results without needing to share data\n\nCons\n\nTakes longer to design\nAnalyst might spend longer exploring an interactive visualization than a series of static visualizations\nPoor design could result in information overload"
  },
  {
    "objectID": "05-interactive-viz.html#common-features-of-interactive-visualizations",
    "href": "05-interactive-viz.html#common-features-of-interactive-visualizations",
    "title": "Interactive visualization",
    "section": "Common features of interactive visualizations",
    "text": "Common features of interactive visualizations\nCommon features of interactive visualizations include (reference):\n\nChanging data representation: providing options to change the type of plot displayed (e.g., allowing users to visualize temperature patterns over a month vs. over years)\nFocusing and getting details: mousing over part of a visualization to see an exact data value, zooming and panning\nData transformation: e.g., changing color scale, switching to/from log scale\nData selection and filtering: highlighting and brushing regions of a plot to focus the selected points; reordering and filtering data show in tables\nFinding corresponding information in multiple views: linked views that update dynamically based on interaction in another plot (often by zooming, panning, or selecting certain points)"
  },
  {
    "objectID": "05-interactive-viz.html#app-exploration",
    "href": "05-interactive-viz.html#app-exploration",
    "title": "Interactive visualization",
    "section": "App exploration",
    "text": "App exploration\nONE person in each group should open the neighborhood diversity app and navigate around the app for the group. (We don’t want to overload the author’s Shiny server.)\n\nCatalog the app’s layout and interactivity features. Make note of where user inputs are located and how different parts of the app respond to user input.\nEvaluate the design of this app.\n\nIs the interactivity in this app needed? Does the interactivity actually help you gain more insight (and perhaps more quickly) than a series of static visualizations?\nWhat explorations/comparisons are you curious to explore that are not enabled well by the app?\n\n\n\nIn case the app doesn’t load, use the following screenshots:"
  },
  {
    "objectID": "05-interactive-viz.html#exercise-1-setup-and-getting-acquainted",
    "href": "05-interactive-viz.html#exercise-1-setup-and-getting-acquainted",
    "title": "Interactive visualization",
    "section": "Exercise 1: Setup and getting acquainted",
    "text": "Exercise 1: Setup and getting acquainted\nSetup part 1: Load required packages at the top of app.R: shiny, tidyverse, sf, and plotly.\nSetup part 2: Data download and folder setup\nNavigate to the “Data for interactive viz activity” folder on Moodle and save the two files with the folder setup below:\n\n📂 YOUR_CLASS_FOLDER\n\n📂 interactive_viz\n\n📂 neighborhood_diversity\n\napp.R\n📂 data\n\ndata_by_dist.rds\ndata_by_year.csv\n\n\n\n\n\nSetup part 3: Below your library() calls, add the following commands to read in the data:\n\ndata_by_dist &lt;- read_rds(\"Enter the correct relative path to data_by_dist.rds\")\ndata_by_year &lt;- read_csv(\"Enter the correct relative path to data_by_year.csv\")\n\nGetting acquainted with the app and underlying code: Take a few minutes to explore the code:\n\nIn the ui section: how are functions nested inside each other, and how does this seem to relate to the visual appearance of the app?\nWhat names/labels in the User Interface (ui) part of the app seem to be shared with the server part of the app?"
  },
  {
    "objectID": "05-interactive-viz.html#input-functions",
    "href": "05-interactive-viz.html#input-functions",
    "title": "Interactive visualization",
    "section": "*Input() functions",
    "text": "*Input() functions\n\nBackground\nWhat do these do? The *Input() functions collect inputs from the user.\nWhere are these functions on the cheatsheet? Right-hand side of the first page\nWhere do these go in the app?\n\nAll *Input() functions go in the ui part of the app.\nPay careful attention to the nesting of the functions in the ui section. For example, in a sidebarLayout(), these *Input() functions should go in the sidebarPanel() (as opposed to the mainPanel()).\nSeparate multiple *Input() functions with commas.\n\nHow do the function arguments work? In all the *Input() functions, the first two arguments are the same:\n\ninputId is how you will refer to this input in the server portion later. You can call this anything you want, but make this ID describe the information that the user is providing.\nlabel is how this will actually be labeled in your UI (what text shows up in the app).\n\nEach function has some additional arguments depending what you want to do.\n\n\nExercise 2: Add *Input()s\nAdd the following two user inputs to your app:\n\nDropdown to select the city name\nSlider to choose the span parameter for the scatterplot smooth\n\nThis parameter varies from 0 to 1. Lower values result in a wiggly smoothing line, and higher values result in a smoother line.\n\n\nUse the Shiny cheatsheet to find the *Input() functions that correspond to the two inputs above. Add them to the appropriate place within the ui object. Use commas to separate the inputs.\n\n\n\n\n\n\nParentheses Pandemonium\n\n\n\nCarefully formatting your code will be crucial here! With shiny UIs, it is very easy to lose or mismatch parentheses, which leads to frustrating errors. My suggestion is to place parentheses as follow:\nsliderInput(\n    argument1 = value1,\n    argument2 = value2,\n    argument3 = value3\n)\nNote how the left parenthesis is on the same line as the function, and the right parenthesis is on its own line and left-aligned with the start of the function name.\nHelpful tip: In RStudio, you can place your cursor next to any parenthesis to highlight the matching parenthesis (if there is one).\n\n\nYou will have to look at the documentation for the *Input() functions to know how to use arguments beyond inputId and label. To view this documentation, type ?function_name in the Console.\nTo get the collection of city names from the data_by_dist dataset, you can use the following:\nmetro_names &lt;- data_by_dist %&gt;% pull(metro_name) %&gt;% unique()\nPut this metro_names code just beneath where you read in the data.\nOnce you finish, run your app. Make sure you can select and move things around as expected. You won’t see any plots yet—we’ll work on those in the next exercises."
  },
  {
    "objectID": "05-interactive-viz.html#output-functions",
    "href": "05-interactive-viz.html#output-functions",
    "title": "Interactive visualization",
    "section": "*Output() functions",
    "text": "*Output() functions\n\nBackground\nWhat do these do? *Output() functions in the ui portion work with the render*() functions in the server portion to to add R output (like plots and tables) to the UI.\nWhere are these functions on the cheatsheet? Bottom-center of the first page\nWhere do these go in the app?\n\nAll *Output() functions go in the ui part of the app.\nPay careful attention to the nesting of the functions in the ui section. For example, in a sidebarLayout(), these *Output() functions should go in the mainPanel() (as opposed to the sidebarPanel()).\nSeparate multiple *Output() functions with commas.\n\nHow do the function arguments work? In all the *Output() functions, the first argument is the same:\n\noutputId works just like inputId for *Input() functions. This is how you will refer to this output in the server portion later. You can call this anything you want, but make this ID describe the output being created.\n\n\n\nExercise 3: Add *Output()s\nAdd 3 outputs to the ui that will eventually be:\n\nA scatterplot of diversity score (entropy) versus distance to city hall (distmiles) with a smoothing line (smoothness controlled by the span parameter on your slider input)\nA map of diversity scores across the counties in the selected city\nA bar chart of the overall race distribution in the selected city (i.e., the total number of people in each race category in the city)\n\nFor now, don’t worry that the layout of the plots exactly matches the original neighborhood diversity app. (You will update this in your homework.)\nRun the app with the output. Notice that nothing really changes. Think of the outputs you just placed as placeholders—the app knows there will be a plot in the UI, but the details of what the plots will look like and the R code to create them will be in the server portion. Let’s talk about that now!"
  },
  {
    "objectID": "05-interactive-viz.html#render-functions",
    "href": "05-interactive-viz.html#render-functions",
    "title": "Interactive visualization",
    "section": "render*() functions",
    "text": "render*() functions\n\nBackground\nWhat do these do? The render*() functions use R code (i.e., standard ggplot code) to communicate with (“listen to”) the user inputs to create the desired output.\nWhere are these functions on the cheatsheet? Bottom-center of the first page. The render*() function you use will depend on the desired output. The bottom center of the cheatsheet shows how *Output() and render*() functions connect.\nWhere do these go in the app? The render*() functions go in the server function of the app.\nIn general, the server section of code will look something like this:\n\n# Suppose the following are somewhere in the UI part\nnumericInput(inputId = \"min_year\")\nnumericInput(inputId = \"max_year\")\nplotOutput(outputId = \"plot_over_years\")\n\nserver &lt;- function(input, output) {\n    output$plot_over_years &lt;- renderPlot({ # Note the curly braces that enclose the R code below\n        ggplot(...) +\n            scale_x_continuous(limits = c(input$min_year, input$max_year))\n    })\n}\n\n\n\nExercise 4: Add renderPlot()\nWhile our main goals is to make 3 plots, you will just make one of them in this exercise.\nAdd a renderPlot() functions inside the server portion of the code to make the scatterplot of diversity score (entropy) versus distance to city hall (distmiles) with a smoothing line. Use the data_by_dist dataset. Reference the inputs you’ve already created in previous exercises by using filter() and ggplot() to render the desired interactive plot.\nNote: the geom_??? used to create the smoothing line has a span parameter. (Check out the documentation for that geom by entering ?geom_??? in the Console.)\nRun the app and check that the scatterplot displays and reacts to the chosen city and span parameter."
  },
  {
    "objectID": "05-interactive-viz.html#exercise-5-turn-plots-into-plotlys",
    "href": "05-interactive-viz.html#exercise-5-turn-plots-into-plotlys",
    "title": "Interactive visualization",
    "section": "Exercise 5: Turn plots into plotlys",
    "text": "Exercise 5: Turn plots into plotlys\nIn a web application, having plots be plotly objects is just nice by default because of the great mouseover, zoom, and pan features.\nInside app.R, change plotOutput to plotlyOutput and renderPlot to renderPlotly for the scatterplot and the barplot. Make sure to add calls to ggplotly() too."
  },
  {
    "objectID": "06-wrangling-1.html",
    "href": "06-wrangling-1.html",
    "title": "Data wrangling - Part 1",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nDetermine the class of a given object and identify concerns to be wary of when manipulating an object of that class (numerics, logicals, factors, dates, strings, data.frames)\nExplain what vector recycling is, when it can be a problem, and how to avoid those problems\nUse a variety of functions to wrangle numerical and logical data\nExtract date-time information using the lubridate package\nUse the forcats package to wrangle factor data\n\n\nYou can download a template Quarto file to start from here. Put this file in a folder called wrangling within a folder for this course."
  },
  {
    "objectID": "06-wrangling-1.html#exercises",
    "href": "06-wrangling-1.html#exercises",
    "title": "Data wrangling - Part 1",
    "section": "Exercises",
    "text": "Exercises\nLoad the diamonds dataset, and filter to the first 1000 diamonds.\n\ndata(diamonds)\ndiamonds &lt;- diamonds %&gt;% \n    slice_head(n = 1000)\n\nComplete the following:\n\nSubset to diamonds that are less than 400 dollars or more than 10000 dollars.\nSubset to diamonds that are between 500 and 600 dollars (inclusive).\nHow many diamonds are of either Fair, Premium, or Ideal cut (a total count)? What fraction of diamonds are of Fair, Premium, or Ideal cut (a total count)?\n\nFirst, do this a wrong way with ==. Predict the warning message that you will receive.\nSecond, do this the correct way with an appropriate logical operator.\n\nAre there any diamonds of Fair cut that are more than $3000? Are all diamonds of Ideal cut more than $2000?\nCreate two new categorized versions of price by looking up the documentation for if_else() and case_when():\n\nprice_cat1: “low” if price is less than 500 and “high” otherwise\nprice_cat2: “low” if price is less than 500, “medium” if price is between 500 and 1000 dollars inclusive, and “high” otherwise.\n\n\n\n\nSolution\n\n\n# 1\ndiamonds %&gt;% \n    filter(price &lt; 400 | price &gt; 10000)\n\n# A tibble: 30 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 20 more rows\n\n# 2\ndiamonds %&gt;% \n    filter(price &gt;= 500, price &lt;= 600)\n\n# A tibble: 90 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.35 Ideal     I     VS1      60.9  57     552  4.54  4.59  2.78\n 2  0.3  Premium   D     SI1      62.6  59     552  4.23  4.27  2.66\n 3  0.3  Ideal     D     SI1      62.5  57     552  4.29  4.32  2.69\n 4  0.3  Ideal     D     SI1      62.1  56     552  4.3   4.33  2.68\n 5  0.42 Premium   I     SI2      61.5  59     552  4.78  4.84  2.96\n 6  0.28 Ideal     G     VVS2     61.4  56     553  4.19  4.22  2.58\n 7  0.32 Ideal     I     VVS1     62    55.3   553  4.39  4.42  2.73\n 8  0.31 Very Good G     SI1      63.3  57     553  4.33  4.3   2.73\n 9  0.31 Premium   G     SI1      61.8  58     553  4.35  4.32  2.68\n10  0.24 Premium   E     VVS1     60.7  58     553  4.01  4.03  2.44\n# ℹ 80 more rows\n\n# 3\n## Wrong way with ==\ndiamonds %&gt;% \n    mutate(is_fpi = cut==c(\"Fair\", \"Premium\", \"Ideal\")) %&gt;% \n    summarize(num_fpi = sum(is_fpi), frac_fpi = mean(is_fpi))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `is_fpi = cut == c(\"Fair\", \"Premium\", \"Ideal\")`.\nCaused by warning in `==.default`:\n! longer object length is not a multiple of shorter object length\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n# A tibble: 1 × 2\n  num_fpi frac_fpi\n    &lt;int&gt;    &lt;dbl&gt;\n1     226    0.226\n\n## Right way with %in%\ndiamonds %&gt;% \n    mutate(is_fpi = cut %in% c(\"Fair\", \"Premium\", \"Ideal\")) %&gt;% \n    summarize(num_fpi = sum(is_fpi), frac_fpi = mean(is_fpi))\n\n# A tibble: 1 × 2\n  num_fpi frac_fpi\n    &lt;int&gt;    &lt;dbl&gt;\n1     685    0.685\n\n# 4\ndiamonds %&gt;% \n    filter(cut==\"Fair\") %&gt;% \n    summarize(any_high = any(price &gt; 3000))\n\n# A tibble: 1 × 1\n  any_high\n  &lt;lgl&gt;   \n1 FALSE   \n\ndiamonds %&gt;% \n    filter(cut==\"Ideal\") %&gt;% \n    summarize(all_high = all(price &gt; 2000))\n\n# A tibble: 1 × 1\n  all_high\n  &lt;lgl&gt;   \n1 FALSE   \n\n# 5\ndiamonds %&gt;% \n    mutate(\n        price_cat1 = if_else(price &lt; 500, \"low\", \"high\"),\n        price_cat2 = case_when(\n            price &lt; 500 ~ \"low\",\n            price &gt;= 500 & price &lt;= 1000 ~ \"medium\",\n            price &gt; 1000 ~ \"high\"\n        )\n    )\n\n# A tibble: 1,000 × 12\n   carat cut       color clarity depth table price     x     y     z price_cat1\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43 low       \n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31 low       \n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31 low       \n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63 low       \n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75 low       \n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48 low       \n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47 low       \n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53 low       \n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49 low       \n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39 low       \n# ℹ 990 more rows\n# ℹ 1 more variable: price_cat2 &lt;chr&gt;"
  },
  {
    "objectID": "06-wrangling-1.html#exercises-1",
    "href": "06-wrangling-1.html#exercises-1",
    "title": "Data wrangling - Part 1",
    "section": "Exercises",
    "text": "Exercises\nInstall the nycflights13 package for the data used in this exercise. You can look at the codebook for the flights dataset with ?flights. Each case represents one flight from a NYC airport in 2013.\n\nlibrary(nycflights13)\ndata(flights)\n\nUsing the flights dataset, complete the following:\n\nWhat is the most common departure hour? Use the dep_time variable.\nMake a plot of the distribution of the largest delay for each flight (the larger of dep_delay and arr_delay).\nWhich origin airport had the longest average delay? Should you use dep_delay or arr_delay here? Which had the largest proportion of missing values for this delay variable?\nWhich destination (dest) airport had the largest variability in delays in terms of the difference between the 25th and 75th percentiles? Should you use dep_delay or arr_delay here?\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Use lag() to explore how the average departure delay for an hour is related to the average departure delay for the previous hour.\n\n\n\nSolutions\n\n\n# 1\nflights %&gt;% \n    mutate(dep_hour = dep_time %/% 100)\n\n# 2\nflights %&gt;% \n    mutate(biggest_delay = pmax(dep_delay, arr_delay)) %&gt;% \n    filter(biggest_delay &gt; 0) %&gt;% # Filter out early flights\n    ggplot(aes(x = biggest_delay)) +\n        geom_density()\n\n# 3\nflights %&gt;% \n    group_by(origin) %&gt;% \n    summarize(\n        mean_dep_delay = mean(dep_delay, na.rm = TRUE),\n        med_dep_delay = median(dep_delay, na.rm = TRUE),\n        frac_missing = mean(is.na(dep_delay), na.rm = TRUE)\n    )\n\n# 4\nflights %&gt;% \n    group_by(dest) %&gt;% \n    summarize(\n        iqr_arr_delay = IQR(arr_delay, na.rm = TRUE)\n    ) %&gt;% \n    arrange(desc(iqr_arr_delay))\n\n# 5\nflights %&gt;% \n    mutate(dep_hour = dep_time %/% 100) %&gt;% \n    group_by(year, month, day, dep_hour) %&gt;% \n    summarize(\n        mean_dep_delay = mean(dep_delay, na.rm = TRUE)\n    ) %&gt;% \n    mutate(mean_dep_delay_prev_hour = lag(mean_dep_delay)) %&gt;% \n    ggplot(aes(x = mean_dep_delay_prev_hour, y = mean_dep_delay)) +\n        geom_point()"
  },
  {
    "objectID": "06-wrangling-1.html#exercises-2",
    "href": "06-wrangling-1.html#exercises-2",
    "title": "Data wrangling - Part 1",
    "section": "Exercises",
    "text": "Exercises\nUse the flights dataset to answer the following:\n\nCompare dep_time, sched_dep_time, and dep_delay. Are they consistent?\nOn what day of the week are delays least likely?\n\n\n\nSolutions\n\n\n# 1\nflights_parsed &lt;- flights %&gt;% \n    mutate(\n        dep_time_hour = dep_time %/% 100,\n        dep_time_min = dep_time %% 100,\n        sched_dep_time_hour = sched_dep_time %/% 100,\n        sched_dep_time_min = sched_dep_time %% 100,\n        dep_time = make_datetime(year = year, month = month, day = day, hour = dep_time_hour, min = dep_time_min, sec = 0, tz = \"EST\"),\n        sched_dep_time = make_datetime(year = year, month = month, day = day, hour = sched_dep_time_hour, min = sched_dep_time_min, sec = 0, tz = \"EST\"),\n        dep_delay_computed = dep_time - sched_dep_time\n    )\nflights_parsed %&gt;% \n    select(dep_delay, dep_delay_computed)\n\n# A tibble: 336,776 × 2\n   dep_delay dep_delay_computed\n       &lt;dbl&gt; &lt;drtn&gt;            \n 1         2  120 secs         \n 2         4  240 secs         \n 3         2  120 secs         \n 4        -1  -60 secs         \n 5        -6 -360 secs         \n 6        -4 -240 secs         \n 7        -5 -300 secs         \n 8        -3 -180 secs         \n 9        -3 -180 secs         \n10        -2 -120 secs         \n# ℹ 336,766 more rows\n\nflights_parsed %&gt;% \n    mutate(match = (dep_delay_computed/60)==dep_delay) %&gt;% \n    filter(!match) %&gt;% \n    select(dep_time, sched_dep_time, dep_delay, dep_delay_computed)\n\n# A tibble: 1,207 × 4\n   dep_time            sched_dep_time      dep_delay dep_delay_computed\n   &lt;dttm&gt;              &lt;dttm&gt;                  &lt;dbl&gt; &lt;drtn&gt;            \n 1 2013-01-01 08:48:00 2013-01-01 18:35:00       853 -35220 secs       \n 2 2013-01-02 00:42:00 2013-01-02 23:59:00        43 -83820 secs       \n 3 2013-01-02 01:26:00 2013-01-02 22:50:00       156 -77040 secs       \n 4 2013-01-03 00:32:00 2013-01-03 23:59:00        33 -84420 secs       \n 5 2013-01-03 00:50:00 2013-01-03 21:45:00       185 -75300 secs       \n 6 2013-01-03 02:35:00 2013-01-03 23:59:00       156 -77040 secs       \n 7 2013-01-04 00:25:00 2013-01-04 23:59:00        26 -84840 secs       \n 8 2013-01-04 01:06:00 2013-01-04 22:45:00       141 -77940 secs       \n 9 2013-01-05 00:14:00 2013-01-05 23:59:00        15 -85500 secs       \n10 2013-01-05 00:37:00 2013-01-05 22:30:00       127 -78780 secs       \n# ℹ 1,197 more rows\n\n# 2\nflights %&gt;% \n    mutate(\n        any_delay = dep_delay &gt; 0 | arr_delay &gt; 0,\n        day_of_week = wday(time_hour, label = TRUE)\n    ) %&gt;% \n    group_by(day_of_week) %&gt;% \n    summarize(frac_delays = mean(any_delay, na.rm = TRUE))\n\n# A tibble: 7 × 2\n  day_of_week frac_delays\n  &lt;ord&gt;             &lt;dbl&gt;\n1 Sun               0.492\n2 Mon               0.524\n3 Tue               0.502\n4 Wed               0.512\n5 Thu               0.565\n6 Fri               0.546\n7 Sat               0.453"
  },
  {
    "objectID": "06-wrangling-1.html#creating-factors",
    "href": "06-wrangling-1.html#creating-factors",
    "title": "Data wrangling - Part 1",
    "section": "Creating factors",
    "text": "Creating factors\nIn R, factors are made up of two components: the actual values of the data and the possible levels within the factor. Creating a factor requires supplying both pieces of information.\n\nmonths &lt;- c(\"Mar\", \"Dec\", \"Jan\",  \"Apr\", \"Jul\")\n\nHowever, if we were to sort this vector, R would sort this vector alphabetically.\n\n# alphabetical sort\nsort(months)\n\n[1] \"Apr\" \"Dec\" \"Jan\" \"Jul\" \"Mar\"\n\n\nWe can fix this sorting by creating a factor version of months. The levels argument is a character vector that specifies the unique values that the factor can take. The order of the values in levels defines the sorting of the factor.\n\nmonths_fct &lt;- factor(months, levels = month.abb) # month.abb is a built-in variable\nmonths_fct\n\n[1] Mar Dec Jan Apr Jul\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\nsort(months_fct)\n\n[1] Jan Mar Apr Jul Dec\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nWe can access the levels of a factor with levels():\n\nlevels(months_fct)\n\n [1] \"Jan\" \"Feb\" \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\"\n\n\nWhat if we try to create a factor with values that aren’t in the levels? (e.g., a typo in a month name)\n\nmonths2 &lt;- c(\"Jna\", \"Mar\")\nfactor(months2, levels = month.abb)\n\n[1] &lt;NA&gt; Mar \nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nBecause the NA is introduced silently (without any error or warnings), this can be dangerous. It might be better to use the fct() function in the forcats package instead:\n\nfct(months2, levels = month.abb)\n\nError in `fct()`:\n! All values of `x` must appear in `levels` or `na`\nℹ Missing level: \"Jna\"\n\n\nExercise: Create a factor version of the following data with the levels in a sensible order.\n\nratings &lt;- c(\"High\", \"Medium\", \"Low\")\n\n\n\nSolution\n\n\nratings_fct &lt;- fct(ratings, levels = c(\"Low\", \"Medium\", \"High\"))\nratings_fct\n\n[1] High   Medium Low   \nLevels: Low Medium High\n\n\n\nIn the remainder of the exercises and examples, we’ll use a subset of the General Social Survey (GSS) dataset available in the forcats pacakges.\n\ndata(gss_cat)"
  },
  {
    "objectID": "06-wrangling-1.html#reordering-factors",
    "href": "06-wrangling-1.html#reordering-factors",
    "title": "Data wrangling - Part 1",
    "section": "Reordering factors",
    "text": "Reordering factors\nReordering the levels of a factor can be useful in plotting when categories would benefit from being sorted in a particular way:\n\nrelig_summary &lt;- gss_cat %&gt;%\n    group_by(relig) %&gt;%\n    summarize(\n        tvhours = mean(tvhours, na.rm = TRUE),\n        n = n()\n    )\n\nggplot(relig_summary, aes(x = tvhours, y = relig)) + \n    geom_point() +\n    theme_classic()\n\n\n\n\nWe can use fct_reorder() in forcats.\n\nThe first argument is the factor that you want to reorder the levels of\nThe second argument determines how the factor is sorted (analogous to what you put inside arrange() when sorting the rows of a data frame.)\n\n\nggplot(relig_summary, aes(x = tvhours, y = fct_reorder(relig, tvhours))) +\n    geom_point() +\n    theme_classic()\n\n\n\n\nFor bar plots, we can use fct_infreq() to reorder levels from most to least common. This can be combined with fct_rev() to reverse the order (least to most common):\n\ngss_cat %&gt;%\n    ggplot(aes(x = marital)) +\n    geom_bar() +\n    theme_classic()\n\n\n\ngss_cat %&gt;%\n    mutate(marital = marital %&gt;% fct_infreq() %&gt;% fct_rev()) %&gt;%\n    ggplot(aes(x = marital)) +\n    geom_bar() +\n    theme_classic()"
  },
  {
    "objectID": "06-wrangling-1.html#modifying-factor-levels",
    "href": "06-wrangling-1.html#modifying-factor-levels",
    "title": "Data wrangling - Part 1",
    "section": "Modifying factor levels",
    "text": "Modifying factor levels\nWe talked about reordering the levels of a factor–what about changing the values of the levels themselves?\nFor example, the names of the political parties in the GSS could use elaboration (“str” isn’t a great label for “strong”) and clean up:\n\ngss_cat %&gt;% count(partyid)\n\n# A tibble: 10 × 2\n   partyid                n\n   &lt;fct&gt;              &lt;int&gt;\n 1 No answer            154\n 2 Don't know             1\n 3 Other party          393\n 4 Strong republican   2314\n 5 Not str republican  3032\n 6 Ind,near rep        1791\n 7 Independent         4119\n 8 Ind,near dem        2499\n 9 Not str democrat    3690\n10 Strong democrat     3490\n\n\nWe can use fct_recode() on partyid with the new level names going on the left and the old levels on the right. Any levels that aren’t mentioned explicitly (i.e., “Don’t know” and “Other party”) will be left as is:\n\ngss_cat %&gt;%\n    mutate(\n        partyid = fct_recode(partyid,\n            \"Republican, strong\"    = \"Strong republican\",\n            \"Republican, weak\"      = \"Not str republican\",\n            \"Independent, near rep\" = \"Ind,near rep\",\n            \"Independent, near dem\" = \"Ind,near dem\",\n            \"Democrat, weak\"        = \"Not str democrat\",\n            \"Democrat, strong\"      = \"Strong democrat\"\n        )\n    ) %&gt;%\n    count(partyid)\n\n# A tibble: 10 × 2\n   partyid                   n\n   &lt;fct&gt;                 &lt;int&gt;\n 1 No answer               154\n 2 Don't know                1\n 3 Other party             393\n 4 Republican, strong     2314\n 5 Republican, weak       3032\n 6 Independent, near rep  1791\n 7 Independent            4119\n 8 Independent, near dem  2499\n 9 Democrat, weak         3690\n10 Democrat, strong       3490\n\n\nTo combine groups, we can assign multiple old levels to the same new level (“Other” maps to “No answer”, “Don’t know”, and “Other party”):\n\ngss_cat %&gt;%\n    mutate(\n        partyid = fct_recode(partyid,\n            \"Republican, strong\"    = \"Strong republican\",\n            \"Republican, weak\"      = \"Not str republican\",\n            \"Independent, near rep\" = \"Ind,near rep\",\n            \"Independent, near dem\" = \"Ind,near dem\",\n            \"Democrat, weak\"        = \"Not str democrat\",\n            \"Democrat, strong\"      = \"Strong democrat\",\n            \"Other\"                 = \"No answer\",\n            \"Other\"                 = \"Don't know\",\n            \"Other\"                 = \"Other party\"\n        )\n    )\n\n# A tibble: 21,483 × 9\n    year marital         age race  rincome        partyid    relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Never married    26 White $8000 to 9999  Independe… Prot… Sout…      12\n 2  2000 Divorced         48 White $8000 to 9999  Republica… Prot… Bapt…      NA\n 3  2000 Widowed          67 White Not applicable Independe… Prot… No d…       2\n 4  2000 Never married    39 White Not applicable Independe… Orth… Not …       4\n 5  2000 Divorced         25 White Not applicable Democrat,… None  Not …       1\n 6  2000 Married          25 White $20000 - 24999 Democrat,… Prot… Sout…      NA\n 7  2000 Never married    36 White $25000 or more Republica… Chri… Not …       3\n 8  2000 Divorced         44 White $7000 to 7999  Independe… Prot… Luth…      NA\n 9  2000 Married          44 White $25000 or more Democrat,… Prot… Other       0\n10  2000 Married          47 White $25000 or more Republica… Prot… Sout…       3\n# ℹ 21,473 more rows\n\n\nWe can use fct_collapse() to collapse many levels:\n\ngss_cat %&gt;%\n    mutate(\n        partyid = fct_collapse(partyid,\n            \"Other\" = c(\"No answer\", \"Don't know\", \"Other party\"),\n            \"Republican\" = c(\"Strong republican\", \"Not str republican\"),\n            \"Independent\" = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n            \"Democrat\" = c(\"Not str democrat\", \"Strong democrat\")\n        )\n    ) %&gt;%\n    count(partyid)\n\n# A tibble: 4 × 2\n  partyid         n\n  &lt;fct&gt;       &lt;int&gt;\n1 Other         548\n2 Republican   5346\n3 Independent  8409\n4 Democrat     7180\n\n\nExercises: Using the gss_cat dataset, try the following:\n\nMake a plot that shows the relationship between marital status (marital) and age in a way that makes a trend clear.\nMake a plot that shows the relationship between religion followed (relig) and income rincome. Combine income categories for better readability.\n\n\n\nSolution\n\n\n# Before\nggplot(gss_cat, aes(x = age, y = marital)) +\n    geom_boxplot()\n\nWarning: Removed 76 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n# After\nggplot(gss_cat, aes(x = age, y = fct_reorder(marital, age))) +\n    geom_boxplot()\n\nWarning: `fct_reorder()` removing 76 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\n\n\nWarning: `fct_reorder()` removing 76 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\n\n\nWarning: Removed 76 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n# Before\nggplot(gss_cat, aes(x = relig, fill = rincome)) +\n    geom_bar(position = \"fill\")\n\n\n\n# After\ngss_cat %&gt;%\n    mutate(\n        income_clean = fct_collapse(rincome,\n            \"Unknowns\" = c(\"No answer\", \"Don't know\", \"Refused\"),\n            \"&lt; $7000\" = c(\"Lt $1000\", \"$1000 to 2999\", \"$3000 to 3999\", \"$4000 to 4999\", \"$5000 to 5999\", \"$6000 to 6999\"),\n            \"&gt;= $7000\" = c(\"$7000 to 7999\", \"$8000 to 9999\", \"$10000 - 14999\", \"$15000 - 19999\", \"$20000 - 24999\", \"$25000 or more\")\n        )\n    ) %&gt;%\n    ggplot(aes(x = relig, fill = income_clean)) +\n        geom_bar(position = \"fill\")"
  },
  {
    "objectID": "07-wrangling-2.html",
    "href": "07-wrangling-2.html",
    "title": "Data wrangling - Part 2",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nManipulate and explore strings using the stringr package\nConstruct regular expressions to find patterns in strings\n\n\nYou can download a template Quarto file to start from here. Put this file in a folder called wrangling within a folder for this course.\n\nThe stringr cheatsheet (HTML, PDF) will be useful to have open and reference."
  },
  {
    "objectID": "07-wrangling-2.html#creating-strings",
    "href": "07-wrangling-2.html#creating-strings",
    "title": "Data wrangling - Part 2",
    "section": "Creating strings",
    "text": "Creating strings\nCreating strings by hand is useful for testing out regular expressions.\nTo create a string, type any text in either double quotes (\") or single quotes '. Using double or single quotes doesn’t matter unless your string itself has single or double quotes.\n\nstring1 &lt;- \"This is a string\"\nstring2 &lt;- 'If I want to include a \"quote\" inside a string, I use single quotes'\n\nWe can view these strings “naturally” (without the opening and closing quotes) with str_view():\n\nstr_view(string1)\n\n[1] │ This is a string\n\nstr_view(string2)\n\n[1] │ If I want to include a \"quote\" inside a string, I use single quotes\n\n\nExercise: Create the string It's Thursday. What happens if you put the string inside single quotes? Double quotes?\n\n# Your code\n\n\n\nSolution\n\n\nx &lt;- \"It's Thursday\" # We need double quotes because of the apostrophe\nx\nx &lt;- 'It's Thursday'\n\nError: &lt;text&gt;:3:10: unexpected symbol\n2: x\n3: x &lt;- 'It's\n            ^\n\n\n\nBecause \" and ' are special characters in the creation of strings, R offers another way to put them inside a string. We can escape these special characters by putting a \\ in front of them:\n\nstring1 &lt;- \"This is a string with \\\"double quotes\\\"\"\nstring2 &lt;- \"This is a string with \\'single quotes\\'\"\nstr_view(string1)\n\n[1] │ This is a string with \"double quotes\"\n\nstr_view(string2)\n\n[1] │ This is a string with 'single quotes'\n\n\nGiven that \\ is a special character, how can we put the \\ character in strings? We have to escape it with \\\\.\nExercise: Create the string C:\\Users. What happens when you don’t escape the \\?\n\n# Your code\n\n\n\nSolution\n\n\nx &lt;- \"C:\\\\Users\"\nstr_view(x)\n\n# \\U is the start of special escape characters for Unicode characters\n# The \\U is expected to be followed by certain types of letters and numbers--like \\U0928\nx &lt;- \"C:\\Users\"\n\nError: '\\U' used without hex digits in character string (&lt;text&gt;:6:10)\n\n\n\nOther special characters include:\n\n\\t (Creates a tab)\n\\n (Creates a newline)\n\nBoth can be useful in plots to more neatly arrange text.\n\nstring1 &lt;- \"Record temp:\\t102\"\nstring2 &lt;- \"Record temp:\\n102\"\n\nstr_view(string1)\n\n[1] │ Record temp:{\\t}102\n\nstr_view(string2)\n\n[1] │ Record temp:\n    │ 102\n\n\nCan we get str_view() to show the tab instead of {\\t}? We can use the html argument to have the string displayed as if on a webpage:\n\nstr_view(string1, html = TRUE)\n\nOften we will want to create new strings within data frames. We can use str_c() or str_glue():\n\nWith str_c() the strings to be combined are all separate arguments separated by commas.\nWith str_glue() the desired string is written as a template with variable names inside curly braces {}.\n\n\ndf &lt;- tibble(\n    first_name = c(\"Arya\", \"Olenna\", \"Tyrion\", \"Melisandre\"),\n    last_name = c(\"Stark\", \"Tyrell\", \"Lannister\", NA)\n)\ndf\n\n# A tibble: 4 × 2\n  first_name last_name\n  &lt;chr&gt;      &lt;chr&gt;    \n1 Arya       Stark    \n2 Olenna     Tyrell   \n3 Tyrion     Lannister\n4 Melisandre &lt;NA&gt;     \n\ndf %&gt;%\n    mutate(\n        full_name1 = str_c(first_name, \" \", last_name),\n        full_name2 = str_glue(\"{first_name} {last_name}\")\n    )\n\n# A tibble: 4 × 4\n  first_name last_name full_name1       full_name2      \n  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;            &lt;glue&gt;          \n1 Arya       Stark     Arya Stark       Arya Stark      \n2 Olenna     Tyrell    Olenna Tyrell    Olenna Tyrell   \n3 Tyrion     Lannister Tyrion Lannister Tyrion Lannister\n4 Melisandre &lt;NA&gt;      &lt;NA&gt;             Melisandre NA   \n\n\nExercise: In the following data frame, create a full date string in month-day-year format using both str_c() and str_glue().\n\ndf_dates &lt;- tibble(\n    year = c(2000, 2001, 2002),\n    month = c(\"Jan\", \"Feb\", \"Mar\"),\n    day = c(3, 4, 5)\n)\n\n\n\nSolution\n\n\ndf_dates %&gt;%\n    mutate(\n        date1 = str_c(month, \"-\", day, \"-\", year),\n        date2 = str_glue(\"{month}-{day}-{year}\")\n    )\n\n# A tibble: 3 × 5\n   year month   day date1      date2     \n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;glue&gt;    \n1  2000 Jan       3 Jan-3-2000 Jan-3-2000\n2  2001 Feb       4 Feb-4-2001 Feb-4-2001\n3  2002 Mar       5 Mar-5-2002 Mar-5-2002"
  },
  {
    "objectID": "07-wrangling-2.html#extracting-information-from-strings",
    "href": "07-wrangling-2.html#extracting-information-from-strings",
    "title": "Data wrangling - Part 2",
    "section": "Extracting information from strings",
    "text": "Extracting information from strings\nThe str_length() counts the number of characters in a string.\n\ncomments &lt;- tibble(\n    name = c(\"Alice\", \"Bob\"),\n    comment = c(\"The essay was well organized around the core message and had good transitions.\", \"Good job!\")\n)\n\ncomments %&gt;%\n    mutate(\n        comment_length = str_length(comment)\n    )\n\n# A tibble: 2 × 3\n  name  comment                                                   comment_length\n  &lt;chr&gt; &lt;chr&gt;                                                              &lt;int&gt;\n1 Alice The essay was well organized around the core message and…             78\n2 Bob   Good job!                                                              9\n\n\nThe str_sub() function gets a substring of a string. The 2nd and 3rd arguments indicate the beginning and ending position to extract.\n\nNegative positions indicate the position from the end of the word. (e.g., -3 indicates “3rd letter from the end”)\nSpecifying a position that goes beyond the word won’t result in an error. str_sub() will just go as far as possible.\n\n\nx &lt;- c(\"Apple\", \"Banana\", \"Pear\")\nstr_sub(x, 1, 3)\n\n[1] \"App\" \"Ban\" \"Pea\"\n\nstr_sub(x, -3, -1)\n\n[1] \"ple\" \"ana\" \"ear\"\n\nstr_sub(\"a\", 1, 5)\n\n[1] \"a\"\n\n\nExercise: Find the middle letter of each word in the data frame below. (Challenge: How would you handle words with an even number of letters?)\n\ndf &lt;- tibble(\n    word_id = 1:3,\n    word = c(\"replace\", \"match\", \"pattern\")\n)\n\n\n\nSolution\n\n\ndf %&gt;%\n    mutate(\n        word_length = str_length(word),\n        middle_pos = ceiling(word_length/2),\n        middle_letter = str_sub(word, middle_pos, middle_pos)\n    )\n\n# A tibble: 3 × 5\n  word_id word    word_length middle_pos middle_letter\n    &lt;int&gt; &lt;chr&gt;         &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;        \n1       1 replace           7          4 l            \n2       2 match             5          3 t            \n3       3 pattern           7          4 t"
  },
  {
    "objectID": "07-wrangling-2.html#finding-patterns-in-strings-with-regular-expressions",
    "href": "07-wrangling-2.html#finding-patterns-in-strings-with-regular-expressions",
    "title": "Data wrangling - Part 2",
    "section": "Finding patterns in strings with regular expressions",
    "text": "Finding patterns in strings with regular expressions\nSuppose that you’re exploring text data looking for places where people describe happiness. There are many ways to search. We could search for the word “happy” but that excludes “happiness” so we might search for “happi”.\nRegular expressions (regex) are a powerful language for describing patterns within strings.\n\ndata(fruit)\ndata(words)\ndata(sentences)\n\nWe can use str_view() with the pattern argument to see what parts of a string match the regex supplied in the pattern argument. (Matches are enclosed in &lt;&gt;.)\n\nstr_view(fruit, \"berry\")\n\n [6] │ bil&lt;berry&gt;\n [7] │ black&lt;berry&gt;\n[10] │ blue&lt;berry&gt;\n[11] │ boysen&lt;berry&gt;\n[19] │ cloud&lt;berry&gt;\n[21] │ cran&lt;berry&gt;\n[29] │ elder&lt;berry&gt;\n[32] │ goji &lt;berry&gt;\n[33] │ goose&lt;berry&gt;\n[38] │ huckle&lt;berry&gt;\n[50] │ mul&lt;berry&gt;\n[70] │ rasp&lt;berry&gt;\n[73] │ salal &lt;berry&gt;\n[76] │ straw&lt;berry&gt;\n\n\nEssentials of forming a regex\n\nLetters and numbers in a regex are matched exactly and are called literal characters.\nMost punctuation characters, like ., +, *, [, ], and ?, have special meanings and are called metacharacters.\nQuantifiers come after a regex and control how many times a pattern can match:\n\n?: match the preceding pattern 0 or 1 times\n+: match the preceding pattern at least once\n*: match the preceding pattern at least 0 times (any number of times)\n\n\nExercise: Before running the code below, predict what matches will be made. Run the code to check your guesses. Note that in all regex’s below the ?, +, * applies to the b only (not the a).\n\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab?\")\n\n[1] │ &lt;a&gt;\n[2] │ &lt;ab&gt;\n[3] │ &lt;ab&gt;b\n\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab+\")\n\n[2] │ &lt;ab&gt;\n[3] │ &lt;abb&gt;\n\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab*\")\n\n[1] │ &lt;a&gt;\n[2] │ &lt;ab&gt;\n[3] │ &lt;abb&gt;\n\n\n\n\nSolution\n\n\n# This regex finds \"a\" then \"b\" at most once (can't have 2 or more b's in a row)\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab?\")\n\n[1] │ &lt;a&gt;\n[2] │ &lt;ab&gt;\n[3] │ &lt;ab&gt;b\n\n\n\n# There has to be an \"a\" followed by at least one b\n# This is why the first string \"a\" isn't matched\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab+\")\n\n[2] │ &lt;ab&gt;\n[3] │ &lt;abb&gt;\n\n\n\n# There must be an \"a\" and then any number of b's (including zero)\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab*\")\n\n[1] │ &lt;a&gt;\n[2] │ &lt;ab&gt;\n[3] │ &lt;abb&gt;\n\n\n\n\nWe can match any of a set of characters with [] (called a character class), e.g., [abcd] matches “a”, “b”, “c”, or “d”.\n\nWe can invert the match by starting with ^: [^abcd] matches anything except “a”, “b”, “c”, or “d”.\n\n\n\n# Match words that have vowel-x-vowel\nstr_view(words, \"[aeiou]x[aeiou]\")\n\n[284] │ &lt;exa&gt;ct\n[285] │ &lt;exa&gt;mple\n[288] │ &lt;exe&gt;rcise\n[289] │ &lt;exi&gt;st\n\n# Match words that have not_vowel-y-not_vowel\nstr_view(words, \"[^aeiou]y[^aeiou]\")\n\n[836] │ &lt;sys&gt;tem\n[901] │ &lt;typ&gt;e\n\n\nExercise Using the words data, find words that have two vowels in a row followed by an “m”.\n\n# Your code\n\n\n\nSolution\n\n\nstr_view(words, \"[aeiou][aeiou]m\")\n\n[154] │ cl&lt;aim&gt;\n[714] │ r&lt;oom&gt;\n[735] │ s&lt;eem&gt;\n[844] │ t&lt;eam&gt;\n\n\n\n\nThe alternation operator | can be read just like the logical operator | (“OR”) to pick between one or more alternative patterns. e.g., apple|banana searches for “apple” or “banana”.\n\n\nstr_view(fruit, \"apple|melon|nut\")\n\n [1] │ &lt;apple&gt;\n[13] │ canary &lt;melon&gt;\n[20] │ coco&lt;nut&gt;\n[52] │ &lt;nut&gt;\n[62] │ pine&lt;apple&gt;\n[72] │ rock &lt;melon&gt;\n[80] │ water&lt;melon&gt;\n\n\nExercise: Using the fruit data, find fruits that have a repeated vowel (“aa”, “ee”, “ii”, “oo”, or “uu”.)\n\n# Your code\n\n\n\nSolution\n\n\nstr_view(fruit, \"aa|ee|ii|oo|uu\")\n\n [9] │ bl&lt;oo&gt;d orange\n[33] │ g&lt;oo&gt;seberry\n[47] │ lych&lt;ee&gt;\n[66] │ purple mangost&lt;ee&gt;n\n\n\n\n\nThe ^ operator indicates the beginning of a string, and the $ operator indicates the end of a string. e.g., ^a matches strings that start with “a”, and a$ matches words that end with “a”.\nParentheses group together parts of a regular expression that should be taken as a bundle. (Much like parentheses in arithmetic statements.)\n\ne.g., ab+ is a little confusing. Does it match “ab” one or more times? Or does it match “a” first, then just “b” one or more times? (The latter, as we saw in an earlier example.) We can be very explicit and use a(b)+.\n\n\nExercise: Using the words data, find (1) words that start with “y” and (2) words that don’t start with “y”.\n\n# Your code\n\n\n\nSolution\n\n\n# Words that start with y\nstr_view(words, \"^y\")\n\n[975] │ &lt;y&gt;ear\n[976] │ &lt;y&gt;es\n[977] │ &lt;y&gt;esterday\n[978] │ &lt;y&gt;et\n[979] │ &lt;y&gt;ou\n[980] │ &lt;y&gt;oung\n\n# Words that don't start with y\nstr_view(words, \"^[^y]\")\n\n [1] │ &lt;a&gt;\n [2] │ &lt;a&gt;ble\n [3] │ &lt;a&gt;bout\n [4] │ &lt;a&gt;bsolute\n [5] │ &lt;a&gt;ccept\n [6] │ &lt;a&gt;ccount\n [7] │ &lt;a&gt;chieve\n [8] │ &lt;a&gt;cross\n [9] │ &lt;a&gt;ct\n[10] │ &lt;a&gt;ctive\n[11] │ &lt;a&gt;ctual\n[12] │ &lt;a&gt;dd\n[13] │ &lt;a&gt;ddress\n[14] │ &lt;a&gt;dmit\n[15] │ &lt;a&gt;dvertise\n[16] │ &lt;a&gt;ffect\n[17] │ &lt;a&gt;fford\n[18] │ &lt;a&gt;fter\n[19] │ &lt;a&gt;fternoon\n[20] │ &lt;a&gt;gain\n... and 954 more"
  },
  {
    "objectID": "07-wrangling-2.html#exploring-stringr-functions",
    "href": "07-wrangling-2.html#exploring-stringr-functions",
    "title": "Data wrangling - Part 2",
    "section": "Exploring stringr functions",
    "text": "Exploring stringr functions\nRead in the “Dear Abby” data underlying The Pudding’s 30 Years of American Anxieties article.\n\nposts &lt;- read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv\")\n\nRows: 20034 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): day, url, title, question_only\ndbl (3): year, month, letterId\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTake a couple minutes to scroll through the 30 Years of American Anxieties article to get ideas for themes that you might want to search for using regular expressions.\n\nThe following are core stringr functions that use regular expressions:\n\nstr_view() - View the first occurrence in a string that matches the regex\nstr_count() - Count the number of times a regex matches within a string\nstr_detect() - Determine if (TRUE/FALSE) the regex is found within string\nstr_subset() - Return subset of strings that match the regex\nstr_extract(), str_extract_all() - Return portion of each string that matches the regex. str_extract() extracts the first instance of the match. str_extract_all() extracts all matches.\nstr_replace(), str_replace_all() - Replace portion of string that matches the regex with something else. str_replace() replaces the first instance of the match. str_replace_all() replaces all instances of the match.\nstr_remove(), str_remove_all() - Removes the portion of the string that matches the pattern. Equivalent to str_replace(x, \"THE REGEX PATTERN\", \"\")\n\nExercise: Starting from str_count(), explore each of these functions by pulling up the function documentation page and reading through the arguments. Try out each function using the posts data."
  },
  {
    "objectID": "08-missing-data.html",
    "href": "08-missing-data.html",
    "title": "Missing data",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nExplain the difference between MCAR, MAR, and MNAR missing data mechanisms\nAssess what missing data mechanisms might be at play in a given dataset\nUse visualizations to explore missing data patterns\nExplain why multiple imputation is preferred to single imputation\nExplain how a simulation study can be used to investigate properties of statistical methods\n\n\nYou can download a template Quarto file to start from here. Put this file in a folder called missing_data within a folder for this course. Install the naniar and mice packages."
  },
  {
    "objectID": "08-missing-data.html#exercise",
    "href": "08-missing-data.html#exercise",
    "title": "Missing data",
    "section": "Exercise",
    "text": "Exercise\nFor each of the following situations, propose what missing data mechanism you think is most likely at play.\n\nIn a clinical trial, some patients dropped out before the end of the study. Their reasons for dropping out were not recorded.\nA weather station records temperature, humidity, and wind speed every hour. Some of the recorded values are missing.\nA social media platform collects data on user interactions, such as likes, comments, and shares. Some interactions are not recorded due to bugs in the code.  \n\n\n\nResponses\n\nA variety of responses would be reasonable here:\n\nMNAR probably: The chance of missingness is likely due to factors that aren’t recorded (like discomfort).\nCould be MCAR if the missing values are from technical glitches. Could be MAR if the missing values are related to other measured weather variables. Could be MNAR if the values tend to be missing when the values themselves are high (e.g., high temperature, humidity, and wind speed causing measurement devices to malfunction).\nCould be MCAR if the bugs affected the platform uniformly. Could be MAR if the bugs affected groups of users differently, but where the groupings are known (e.g., bugs affect mobile users more and mobile vs. desktop usage is measurable). Could be MNAR if the bugs lead to performance issues that affect users with slower internet more (where internet speed is likely not measured)."
  },
  {
    "objectID": "08-missing-data.html#missingness-by-variable",
    "href": "08-missing-data.html#missingness-by-variable",
    "title": "Missing data",
    "section": "Missingness by variable",
    "text": "Missingness by variable\nWe can explore how much missingness there is for each variable with the following functions:\n\nsummary(airquality) # Summary statistics in addition to number of NA's\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\nnaniar::vis_miss(airquality) # In what rows are NA's located?\n\n\n\nnaniar::miss_var_summary(airquality) # Information from vis_miss() in table form\n\n# A tibble: 6 × 3\n  variable n_miss pct_miss\n  &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;\n1 Ozone        37    24.2 \n2 Solar.R       7     4.58\n3 Wind          0     0   \n4 Temp          0     0   \n5 Month         0     0   \n6 Day           0     0"
  },
  {
    "objectID": "08-missing-data.html#missingness-by-case",
    "href": "08-missing-data.html#missingness-by-case",
    "title": "Missing data",
    "section": "Missingness by case",
    "text": "Missingness by case\nWe can explore how much missingness there is for each case with miss_case_summary(). For each case, this function calculates the number and percentage of variables with a missing value. If the pct_miss column is large for a case, we likely won’t be able to impute any of its missing values because there just isn’t enough known information–this case will have to be dropped from the analysis.\n\nmiss_case_summary(airquality)\n\n# A tibble: 153 × 3\n    case n_miss pct_miss\n   &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n 1     5      2     33.3\n 2    27      2     33.3\n 3     6      1     16.7\n 4    10      1     16.7\n 5    11      1     16.7\n 6    25      1     16.7\n 7    26      1     16.7\n 8    32      1     16.7\n 9    33      1     16.7\n10    34      1     16.7\n# ℹ 143 more rows"
  },
  {
    "objectID": "08-missing-data.html#exploring-missingness-mechanisms",
    "href": "08-missing-data.html#exploring-missingness-mechanisms",
    "title": "Missing data",
    "section": "Exploring missingness mechanisms",
    "text": "Exploring missingness mechanisms\nAssessing missingness mechanisms involves checking if missingness in a variable is related to other variables. Through our available data, we are really only able to explore the potential for MCAR or MAR mechanisms. There is always the chance that unobserved information (unobserved other variables or unobserved values of the variables we do have) is related to missingness for our variables, so to think through the potential for MNAR, more contextual information is necessary.\nTo explore these relationships, we can create TRUE/FALSE indicators of whether a variable is missing. In the plots below, we use is.na(Ozone) to explore whether cases with missing ozone values are noticeably different from cases with observed ozone values in terms of Solar.R.\n\nggplot(airquality, aes(x = is.na(Ozone), y = Solar.R)) + \n    geom_boxplot()\n\nWarning: Removed 7 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\nggplot(airquality, aes(x = Solar.R, color = is.na(Ozone))) + \n    geom_density()\n\nWarning: Removed 7 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\nThe above boxplots and density plots suggest that missing ozone is not strongly related to solar radiation levels. We still should check if ozone missingness is related to the Wind, Temp, Month, and Day variables (to be done in Exercises).\nIn addition to checking if the chance of ozone missingness is related to Solar.R, we should check if the values of ozone could be predicted by Solar.R. In the scaterrplot below, we look at the relationship between Ozone and Solar.R and use vertical lines to indicate the Solar.R values for cases that are missing Ozone.\n\nWe see that missing Ozone cases are within the observed span of Solar.R, so we would be ok with predicting Ozone from Solar.R because there would be no extrapolation.\n\n\nggplot(airquality, aes(x = Solar.R, y = Ozone)) +\n    geom_point() +\n    geom_smooth() +\n    geom_vline(data = airquality %&gt;% filter(is.na(Ozone)), mapping = aes(xintercept = Solar.R))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 42 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 42 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_vline()`).\n\n\n\n\n\nMini-exercise: Look at the boxplot+scatterplot pairs for Alternate Situations 1 and 2 below. How do these situations compare to our actual situation and to each other? What concerns might arise from using a model to impute Ozone?\n\nairquality_mod &lt;- airquality %&gt;% \n    mutate(\n        Solar.R_mod1 = if_else(is.na(Ozone), 250+Solar.R/4, Solar.R),\n        Solar.R_mod2 = if_else(is.na(Ozone), 250+Solar.R/2, Solar.R)\n    )\n\nggplot(airquality_mod, aes(x = is.na(Ozone), y = Solar.R_mod1)) + \n    geom_boxplot() +\n    labs(title = \"Alternate Situation 1\")\n\nWarning: Removed 7 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\nggplot(airquality_mod, aes(x = Solar.R_mod1, y = Ozone)) +\n    geom_point() +\n    geom_smooth() +\n    geom_vline(data = airquality_mod %&gt;% filter(is.na(Ozone)), mapping = aes(xintercept = Solar.R_mod1)) +\n    labs(title = \"Alternate Situation 1\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 42 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 42 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_vline()`).\n\n\n\n\nggplot(airquality_mod, aes(x = is.na(Ozone), y = Solar.R_mod2)) + \n    geom_boxplot() +\n    labs(title = \"Alternate Situation 2\")\n\nWarning: Removed 7 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\nggplot(airquality_mod, aes(x = Solar.R_mod2, y = Ozone)) +\n    geom_point() +\n    geom_smooth() +\n    geom_vline(data = airquality_mod %&gt;% filter(is.na(Ozone)), mapping = aes(xintercept = Solar.R_mod2)) +\n    labs(title = \"Alternate Situation 2\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 42 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 42 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_vline()`)."
  },
  {
    "objectID": "08-missing-data.html#exercise-1",
    "href": "08-missing-data.html#exercise-1",
    "title": "Missing data",
    "section": "Exercise",
    "text": "Exercise\nContinue the investigation of missingness for Ozone. We want to see how Month, Wind, and Temp relate to the chance of missingness for Ozone and to the value of Ozone.\nDoes it look like a linear regression model (perhaps with variable transformations) could be effective in imputing the missing ozone data?\n\n\nSolution\n\n\nggplot(airquality, aes(fill = is.na(Ozone), x = factor(Month))) +\n    geom_bar(position = \"fill\")\n\n\n\nggplot(airquality, aes(x = is.na(Ozone), y = Wind)) +\n    geom_boxplot()\n\n\n\nggplot(airquality, aes(x = is.na(Ozone), y = Temp)) +\n    geom_boxplot()\n\n\n\nggplot(airquality, aes(x = factor(Month), y = Ozone)) +\n    geom_boxplot()\n\nWarning: Removed 37 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\nggplot(airquality, aes(x = Wind, y = Ozone)) +\n    geom_point() +\n    geom_smooth() +\n    geom_vline(data = airquality %&gt;% filter(is.na(Ozone)), mapping = aes(xintercept = Wind))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 37 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 37 rows containing missing values (`geom_point()`).\n\n\n\n\nggplot(airquality, aes(x = Temp, y = Ozone)) +\n    geom_point() +\n    geom_smooth() +\n    geom_vline(data = airquality %&gt;% filter(is.na(Ozone)), mapping = aes(xintercept = Temp))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 37 rows containing non-finite values (`stat_smooth()`).\nRemoved 37 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "09-functions.html",
    "href": "09-functions.html",
    "title": "Functions",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nRecognize when it would be useful to write a function\nIdentify the core components of a function definition and explain their role (the function() directive, arguments, argument defaults, function body, return value)\nDescribe the difference between argument matching by position and by name\nWrite if-else, if-else if-else statements to conditionally execute code\nWrite your own function to carry out a repeated task\nProvide feedback on functions written by others\n\n\nYou can download a template Quarto file to start from here. Put this file in a folder called functions within a folder for this course."
  },
  {
    "objectID": "09-functions.html#why-functions",
    "href": "09-functions.html#why-functions",
    "title": "Functions",
    "section": "Why functions?",
    "text": "Why functions?\nGetting really good at writing useful and reusable functions is one of the best ways to increase your expertise in data science. It requires a lot of practice.\nIf you’ve copied and pasted code 3 or more times, it’s time to write a function.\n\nReducing errors: Copy+paste+modify is prone to errors (e.g., forgetting to change a variable name)\nEfficiency: If you need to update code, you only need to do it one place. This allows reuse of code within and across projects.\nReadability: Encapsulating code within a function with a descriptive name makes code more readable."
  },
  {
    "objectID": "09-functions.html#core-parts-of-a-function",
    "href": "09-functions.html#core-parts-of-a-function",
    "title": "Functions",
    "section": "Core parts of a function",
    "text": "Core parts of a function\nWhat does a function look like?\n\naverage &lt;- function(x, remove_nas) {\n    sum(x, na.rm = remove_nas)/length(x)\n}\n\naverage2 &lt;- function(x, remove_nas) {\n    return(sum(x, na.rm = remove_nas)/length(x))\n}\n\naverage3 &lt;- function(x, remove_nas = TRUE) {\n    sum(x, na.rm = remove_nas)/length(x)\n}\n\nThe core parts of a function include:\n\nThe function() directive\n\nThis is what allows tells R to create a function.\n\nArguments: the x and remove_nas – these are function inputs\n\nIn average3, the remove_nas argument has a default value of TRUE.\n\nFunction body\n\nThe code inside the curly braces {} is where all the work happens. This code uses the function arguments to perform computations.\n\nReturn value\n\nThe first value that gets computed and isn’t stored as an object is what the function returns. (This is generally the first line without an assignment operator &lt;-.)\nAs in average3(), we can also explicitly return an object by putting it inside return().\n\n\nWhen a function has default values for arguments, they don’t have to be explicitly named if you want to use the default value:\n\n# Both give the same result\naverage3(c(1, 2, 3, NA))\n\n[1] 1.5\n\naverage3(c(1, 2, 3, NA), remove_nas = TRUE)\n\n[1] 1.5\n\n\nPair programming exercise: There are two function-writing exercises coming up. You’ll swap driver and navigator roles between exercises. (The driver writes the code. The navigator oversees and provides guidance.) For the first exercise, the person whose birthday is coming up sooner will be the driver first. Swap for the second exercise.\nExercise: Write a function that rescales a numeric vector to be between 0 and 1. Test out your function on the following inputs:\n\nx = 2:4. Expected output: 0.0 0.5 1.0\nx = c(-1, 0, 5). Expected output: 0.0000000 0.1666667 1.0000000\nx = -3:-1. Expected output: 0.0 0.5 1.0\n\n\n\nSolution\n\n\nrescale01 &lt;- function(x) {\n    range_x &lt;- range(x, na.rm = TRUE)\n    # The [2] and [1] below extract the second and first element of a vector\n    (x - min(x, na.rm = TRUE)) / (range_x[2]-range_x[1])\n}\nrescale01(2:4)\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(-1, 0, 5))\n\n[1] 0.0000000 0.1666667 1.0000000\n\nrescale01(-3:-1)\n\n[1] 0.0 0.5 1.0\n\n\n\nExercise Write a function that formats a 10-digit phone number nicely as (###) ###-####. Your function should work on the following input: c(\"651-330-8661\", \"6516966000\", \"800 867 5309\"). It may help to refer to the stringr cheatsheet.\n\n\nSolution\n\n\nformat_phone_number &lt;- function(nums) {\n    nums &lt;- str_remove_all(nums, \"[^[:alnum:]]\")\n    area &lt;- str_sub(nums, 1, 3)\n    num3 &lt;- str_sub(nums, 4, 6)\n    num4 &lt;- str_sub(nums, 7, 10)\n    # str_glue(\"({area}) {num3}-{num4}\")\n    str_c(\"(\", area, \")\", num3, \"-\", num4)\n}\nformat_phone_number(c(\"651-330-8661\", \"6516966000\", \"800 867 5309\"))\n\n[1] \"(651)330-8661\" \"(651)696-6000\" \"(800)867-5309\""
  },
  {
    "objectID": "09-functions.html#argument-matching",
    "href": "09-functions.html#argument-matching",
    "title": "Functions",
    "section": "Argument matching",
    "text": "Argument matching\nWhen you supply arguments to a function, they can be matched by position and/or by name.\nWhen you call a function without argument = value inside the parentheses, you are using positional matching.\n\nggplot(diamonds, aes(x = carat, y = price)) + geom_point()\n\nThe above works because the first argument of ggplot is data and the second is mapping. (Pull up the documentation on ggplot with ?ggplot in the Console.) So the following doesn’t work:\n\nggplot(aes(x = carat, y = price), diamonds) + geom_point()\n\nError in `ggplot()`:\n! `mapping` should be created with `aes()`.\n✖ You've supplied a &lt;tbl_df&gt; object\n\n\nBut if we named the arguments (name matching), we would be fine:\n\nggplot(mapping = aes(x = carat, y = price), data = diamonds) + geom_point()\n\nSomewhat confusingly, we can name some arguments and not others. Below, mapping is named, but data isn’t. This works because when an argument is matched by name, it is “removed” from the argument list, and the remaining unnamed arguments are matched in the order that they are listed in the function definition. Just because this is possible doesn’t mean it’s a good idea–don’t do this!\n\nggplot(mapping = aes(x = carat, y = price), diamonds) + geom_point()\n\n\n\n\n\n\n\nArgument matching\n\n\n\nIn general, it is safest to match arguments by name and position for your peace of mind. For functions that you are very familiar with (and know the argument order), it’s ok to just use positional matching.\n\n\nExercise: Diagnose the error message in the example below:\nggplot() %&gt;%\n    geom_sf(census_data, aes(fill = population))\n    \nError in `layer_sf()`:\n! `mapping` must be created by `aes()`\n\n\nSolution\n\nUse ?geom_sf to look up the function documentation. We see that the order of the arguments is first mapping and second data. The error is due to R thinking that census_data is supplying aesthetics. This is an example of positional matching gone wrong."
  },
  {
    "objectID": "09-functions.html#the-if-else-if-else-control-structure",
    "href": "09-functions.html#the-if-else-if-else-control-structure",
    "title": "Functions",
    "section": "The if-else if-else control structure",
    "text": "The if-else if-else control structure\nOften in functions, you will want to execute code conditionally. In a programming language, control structures are parts of the language that allow you to control what code is executed. By far the most common is the `if-else if-else structure.\n\nif (logical_condition) {\n    # some code\n} else if (other_logical_condition) {\n    # some code\n} else {\n    # some code\n}\n\nmiddle &lt;- function(x) {\n    mean_x &lt;- mean(x, na.rm = TRUE)\n    median_x &lt;- median(x, na.rm = TRUE)\n    seems_skewed &lt;- (mean_x &gt; 1.5*median_x) | (mean_x &lt; (1/1.5)*median_x)\n    if (seems_skewed) {\n        median_x\n    } else {\n        mean_x\n    }\n}\n\nPair programming exercise: Whoever was driver most recently should start as navigator. Switch for the second exercise.\nExercise: Write a function for converting temperatures that takes as input a numeric value and a unit (either “C” for Celsius or “F” for Fahrenheit). The function should convert the temperature from one unit to the other based on the following formulas:\n\nTo convert Celsius to Fahrenheit: (Celsius * 9/5) + 32\nTo convert Fahrenheit to Celsius: (Fahrenheit - 32) * 5/9\n\n\n\nSolution\n\n\nconvert_temp &lt;- function(temp, unit) {\n    if (unit==\"F\") {\n        (temp - 32) * 5/9\n    } else if (unit==\"C\") {\n        (temp * 9/5) + 32\n    }\n}\n\nconvert_temp(0, unit = \"C\")\n\n[1] 32\n\nconvert_temp(32, unit = \"F\")\n\n[1] 0\n\n\n\nExercise: Write a function that extracts the domain name of a supplied email address. The function should return the domain name (e.g., “gmail.com”). If the input is not a valid email address, return “Invalid Email”. (A valid email ends in “dot something”.)\n\n\nSolution\n\n\nextract_domain &lt;- function(email) {\n    is_valid &lt;- str_detect(email, \"@.+\\\\..+\")\n    if (is_valid) {\n        str_extract(email, \"@.+$\") %&gt;%\n            str_remove(\"@\")\n    } else {\n        \"Invalid Email\"\n    }\n}\n\nextract_domain(email = \"les@mac.edu\")\n\n[1] \"mac.edu\"\n\nextract_domain(email = \"les@mac.net\")\n\n[1] \"mac.net\"\n\nextract_domain(email = \"les@mac.\")\n\n[1] \"Invalid Email\"\n\nextract_domain(email = \"les@macedu\")\n\n[1] \"Invalid Email\""
  },
  {
    "objectID": "09-functions.html#writing-functions-with-tidyverse-verbs",
    "href": "09-functions.html#writing-functions-with-tidyverse-verbs",
    "title": "Functions",
    "section": "Writing functions with tidyverse verbs",
    "text": "Writing functions with tidyverse verbs\nPerhaps we are using group_by() and summarize() a lot to compute group means. We might write this function:\n\ngroup_means &lt;- function(df, group_var, mean_var) {\n    df %&gt;%\n        group_by(group_var) %&gt;%\n        summarize(mean = mean(mean_var))\n}\n\nLet’s use it on the diamonds dataset to compute the mean size (carat) by diamond cut:\n\ngroup_means(diamonds, group_var = cut, mean_var = carat)\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `group_var` is not found.\n\n\nWhat if the problem is that the variable names need to be in quotes?\n\ngroup_means(diamonds, group_var = \"cut\", mean_var = \"carat\")\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `group_var` is not found.\n\n\nWhat’s going on??? The tidyverse uses something called tidy evaluation: this allows you to refer to a variable by typing it directly (e.g., no need to put it in quotes). So group_by(group_var) is expecting a variable that is actually called group_var, and mean(mean_var) is expecting a variable that is actually called mean_var.\nTo fix this we need to embrace the variables inside the function with {{ var }}:\n\ngroup_means &lt;- function(df, group_var, mean_var) {\n    df %&gt;%\n        group_by({{ group_var }}) %&gt;%\n        summarize(mean = mean({{ mean_var }}))\n}\n\nThe {{ var }} tells R to look at what the value of the variable var rather than look for var literally.\n\ngroup_means(diamonds, group_var = cut, mean_var = carat)\n\n# A tibble: 5 × 2\n  cut        mean\n  &lt;ord&gt;     &lt;dbl&gt;\n1 Fair      1.05 \n2 Good      0.849\n3 Very Good 0.806\n4 Premium   0.892\n5 Ideal     0.703\n\n\nLet’s group by both cut and color:\n\ngroup_means(diamonds, group_var = c(cut, color), mean_var = carat)\n\nError in `group_by()`:\nℹ In argument: `c(cut, color)`.\nCaused by error:\n! `c(cut, color)` must be size 53940 or 1, not 107880.\n\n\nOh no! What now?! When c(cut, color) is put inside {{ c(cut, color) }} within the function, R is actually running the code inside {{ }}. This combines the columns for those 2 variables into one long vector. What we really meant by c(cut, color) is “group by both cut and color”.\nTo fix this, we need the pick() function to get R to see {{ group_var }} as a list of separate variables (like the way select() works).\n\ngroup_means &lt;- function(df, group_var, mean_var) {\n    df %&gt;%\n        group_by(pick({{ group_var }})) %&gt;%\n        summarize(mean = mean({{ mean_var }}))\n}\n\nPair programming exercise: Partner with the person next to you again. Whoever was driver most recently should start as navigator. Switch for the second exercise.\nExercise: Create a new version of dplyr::count() that also shows proportions instead of just sample sizes. The function should be able to handle counting by multiple variables. Test your function with two different sets of arguments using the diamonds dataset.\n\n\nSolution\n\n\ncount_prop &lt;- function(df, count_vars) {\n    df %&gt;%\n        count(pick({{ count_vars }})) %&gt;%\n        mutate(prop = n/sum(n))\n}\n\ncount_prop(diamonds, count_vars = cut)\n\n# A tibble: 5 × 3\n  cut           n   prop\n  &lt;ord&gt;     &lt;int&gt;  &lt;dbl&gt;\n1 Fair       1610 0.0298\n2 Good       4906 0.0910\n3 Very Good 12082 0.224 \n4 Premium   13791 0.256 \n5 Ideal     21551 0.400 \n\ncount_prop(diamonds, count_vars = c(cut, color))\n\n# A tibble: 35 × 4\n   cut   color     n    prop\n   &lt;ord&gt; &lt;ord&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 Fair  D       163 0.00302\n 2 Fair  E       224 0.00415\n 3 Fair  F       312 0.00578\n 4 Fair  G       314 0.00582\n 5 Fair  H       303 0.00562\n 6 Fair  I       175 0.00324\n 7 Fair  J       119 0.00221\n 8 Good  D       662 0.0123 \n 9 Good  E       933 0.0173 \n10 Good  F       909 0.0169 \n# ℹ 25 more rows\n\n\n\nExercise: Create a function that creates a scatterplot from a user-supplied dataset with user-supplied x and y variables. The plot should also show a curvy smoothing line in blue, and a linear smoothing line in red. Test your function using the diamonds dataset.\n\n\nSolution\n\n\nscatter_with_smooths &lt;- function(df, x, y) {\n    ggplot(df, aes(x = {{ x }}, y = {{ y }})) +\n        geom_point() +\n        geom_smooth(color = \"blue\") +\n        geom_smooth(method = \"lm\", color = \"red\")\n}\n\nscatter_with_smooths(diamonds, x = carat, y = price)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "10-subset-str-shiny.html",
    "href": "10-subset-str-shiny.html",
    "title": "Subsetting, str(), Shiny debugging",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nSubset vectors and matrices with [ by index, name, logical vector, and indirectly with variables\nSubset data frames with $ and [[\nUse the str() function to examine the structure of an unfamiliar object and extract components from the object\nApply printing strategies with a Shiny app to streamline the debugging and development process\n\n\nYou can download a template Quarto file to start from here. Put this file in a folder called functions within a folder for this course."
  },
  {
    "objectID": "10-subset-str-shiny.html#selecting-many-elements-with",
    "href": "10-subset-str-shiny.html#selecting-many-elements-with",
    "title": "Subsetting, str(), Shiny debugging",
    "section": "Selecting many elements with [",
    "text": "Selecting many elements with [\nThere are five main types of things that you can subset a vector with, i.e., that can be the i in x[i]:\n\nA vector of positive integers. Subsetting with positive integers keeps the elements at those positions:\n\nx &lt;- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[c(3, 2, 5)]\n\n[1] \"three\" \"two\"   \"five\" \n\nx[2:4]\n\n[1] \"two\"   \"three\" \"four\" \n\n\nBy repeating a position, you can actually make a longer output than input, making the term “subsetting” a bit of a misnomer.\n\nx[c(1, 1, 2)]\n\n[1] \"one\" \"one\" \"two\"\n\n\nA vector of negative integers. Negative values drop the elements at the specified positions:\n\nx[c(-1, -3, -5)]\n\n[1] \"two\"  \"four\"\n\n\nA logical vector. Subsetting with a logical vector only keeps values corresponding to TRUE. This is generally used with comparison functions and operators.\n\nx &lt;- c(10, 3, NA, 5, 8, 1, NA)\n\n# All non-missing values of x\nx[!is.na(x)]\n\n[1] 10  3  5  8  1\n\n# All values greater than 5, with NAs\nx[x &gt; 5]\n\n[1] 10 NA  8 NA\n\n# All non-missing values greater than 5\nx[x &gt; 5 & !is.na(x)]\n\n[1] 10  8\n\n\nUnlike filter(), NA indices will be included in the output as NAs (filter() removes instances of missing values.)\nA character vector. If you have a named vector, you can subset it with a character vector:\n\nx &lt;- c(abc = 1, def = 2, xyz = 5)\nx[c(\"xyz\", \"def\")]\n\nxyz def \n  5   2 \n\n\nAs with subsetting with positive integers, you can use a character vector to duplicate individual entries.\nAn object. A named object may provide any of the previous 4 types of information and can be used to subset:\n\nx &lt;- c(first = \"one\", second = \"two\", third = \"three\", fourth = \"four\")\n# Note that x can also be created as follows\nx &lt;- c(\"one\", \"two\", \"three\", \"four\")\nnames(x) &lt;- c(\"first\", \"second\", \"third\", \"fourth\")\n\n# Subset with an integer object\nidx_pos &lt;- c(1, 3)\nidx_neg &lt;- c(-1, -3)\nx[idx_pos]\n\n  first   third \n  \"one\" \"three\" \n\nx[idx_neg]\n\nsecond fourth \n \"two\" \"four\" \n\n# Subset with a logical object\nbool &lt;- c(TRUE, FALSE, FALSE, TRUE)\nx[bool]\n\n first fourth \n \"one\" \"four\" \n\n# Subset with a character object\nwhich_names &lt;- c(\"first\", \"fourth\")\nx[which_names]\n\n first fourth \n \"one\" \"four\" \n\n\n\nAll of the above subsetting options can be combined with assignment &lt;-. Be very wary of vector recycling when doing this! The number of things that you’re inserting should either be 1 or the size of the x[i] subset.\n\nx &lt;- c(first = \"one\", second = \"two\", third = \"three\", fourth = \"four\")\nx\n\n  first  second   third  fourth \n  \"one\"   \"two\" \"three\"  \"four\" \n\nx[c(1, 3)] &lt;- \"new\" # Replacement length is 1\nx\n\n first second  third fourth \n \"new\"  \"two\"  \"new\" \"four\" \n\nx &lt;- c(first = \"one\", second = \"two\", third = \"three\", fourth = \"four\")\nx[c(1, 3)] &lt;- c(\"new1\", \"new2\") # Replacement length is 2, and length of subset is 2\nx\n\n first second  third fourth \n\"new1\"  \"two\" \"new2\" \"four\" \n\nx &lt;- c(first = \"one\", second = \"two\", third = \"three\", fourth = \"four\")\nx[c(1, 3, 4)] &lt;- c(\"new1\", \"new2\") # BAD! Replacement length is 2, and length of subset is 3\n\nWarning in x[c(1, 3, 4)] &lt;- c(\"new1\", \"new2\"): number of items to replace is\nnot a multiple of replacement length\n\nx\n\n first second  third fourth \n\"new1\"  \"two\" \"new2\" \"new1\" \n\nx &lt;- c(first = \"one\", second = \"two\", third = \"three\", fourth = \"four\")\nx[c(1, 3)] &lt;- c(\"new1\", \"new2\", \"new3\")\n\nWarning in x[c(1, 3)] &lt;- c(\"new1\", \"new2\", \"new3\"): number of items to replace\nis not a multiple of replacement length\n\nx\n\n first second  third fourth \n\"new1\"  \"two\" \"new2\" \"four\" \n\n\nAll of the above subsetting options can be used for subsetting matrices and data frames. Note that if the output has one row or one column, the output is a vector rather than a matrix.\n\nm &lt;- matrix(1:12, nrow = 3, ncol = 4)\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nm[1,] # Get 1st row\n\n[1]  1  4  7 10\n\nm[,1] # Get 1st column\n\n[1] 1 2 3\n\nm[1,3] # Get 1st row and 3rd column\n\n[1] 7\n\nm[c(1,3),] # Get 1st and 3rd rows\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    3    6    9   12\n\nm[,c(1,3)] # Get 1st and 3rd columns\n\n     [,1] [,2]\n[1,]    1    7\n[2,]    2    8\n[3,]    3    9\n\nm[c(1,3),c(1,3)] # Get 1st and 3rd rows and 1st and 3rd columns\n\n     [,1] [,2]\n[1,]    1    7\n[2,]    3    9\n\nm[-1,] # Get all rows except 1st\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    5    8   11\n[2,]    3    6    9   12\n\nm[c(TRUE, FALSE, FALSE),] # Get the 1st row via a logical\n\n[1]  1  4  7 10\n\n# Get the 1st row via a variable\nwhich_rows &lt;- 1\nm[which_rows,]\n\n[1]  1  4  7 10\n\n# Add row and column names to the matrix\ncolnames(m) &lt;- str_c(\"col\", 1:4)\nrownames(m) &lt;- str_c(\"row\", 1:3)\nm[\"row1\",]\n\ncol1 col2 col3 col4 \n   1    4    7   10 \n\nwhich_rows &lt;- c(\"row1\", \"row3\")\nm[which_rows,]\n\n     col1 col2 col3 col4\nrow1    1    4    7   10\nrow3    3    6    9   12"
  },
  {
    "objectID": "10-subset-str-shiny.html#selecting-a-single-element-with-and",
    "href": "10-subset-str-shiny.html#selecting-a-single-element-with-and",
    "title": "Subsetting, str(), Shiny debugging",
    "section": "Selecting a single element with $ and [[",
    "text": "Selecting a single element with $ and [[\nWe can use $ and [[ to extract a single column of a data frame. (The same can be used to subset lists, which we’ll talk about next week. A data frame is actually a special case of a list.)\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\nmtcars$mpg\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\nmtcars[[\"mpg\"]]\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\nwhich_var &lt;- \"mpg\"\nmtcars[[which_var]]\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\nmtcars %&gt;% pull(mpg)\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4"
  },
  {
    "objectID": "10-subset-str-shiny.html#exercises",
    "href": "10-subset-str-shiny.html#exercises",
    "title": "Subsetting, str(), Shiny debugging",
    "section": "Exercises",
    "text": "Exercises\nWrite functions that take a vector as input and return:\n\nThe elements at even-numbered positions. (Hint: use the seq() function.)\nEvery element except the last value.\nOnly even values (and no missing values).\n\n\n\nSolutions\n\n\nget_even_pos &lt;- function(x) {\n    if (length(x) &lt;= 1) {\n        print(\"No even positions\")\n    } else {\n        idx &lt;- seq(2, length(x), by = 2)\n        x[idx]\n    }\n}\nget_even_pos(1:10)\n\n[1]  2  4  6  8 10\n\nget_even_pos(1:9)\n\n[1] 2 4 6 8\n\nget_even_pos(1)\n\n[1] \"No even positions\"\n\n\n\nget_all_but_last &lt;- function(x) {\n    head(x, -1)\n    # x[1:(length(x)-1)]\n}\nget_all_but_last(1:10)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\n\nget_evens &lt;- function(x) {\n    x[x %% 2 == 0 & !is.na(x)]\n}\n\nget_evens(c(1, 2, 7, NA))\n\n[1] 2\n\nget_evens(c(1, 2, 7, 8, NA))\n\n[1] 2 8"
  },
  {
    "objectID": "10-subset-str-shiny.html#exercise",
    "href": "10-subset-str-shiny.html#exercise",
    "title": "Subsetting, str(), Shiny debugging",
    "section": "Exercise",
    "text": "Exercise\nWrite a function that takes the following inputs:\n\ndata: A dataset\nyvar: Outcome variable to be used in a linear model (a length-1 character vector)\npreds: Predictor variables to be used in a linear model (a character vector)\npred_of_interest: The variable whose coefficient estimate and confidence interval are of interest (a length-1 character vector and should be one of preds)\n\nYour function will fit a linear model on the dataset using the given outcome and predictor variables and return a data frame (tibble) with the coefficient estimate and CI for the predictor of interest.\nTest your function on the mtcars dataset.\nDevelopment tip: As you develop, it will help to create objects for the arguments so that you can see what output looks like interactively:\n\ndata &lt;- mtcars\nyvar &lt;- \"mpg\"\npreds &lt;- c(\"hp\", \"wt\")\npred_of_interest &lt;- \"hp\"\n\nWhen you’re done developing your function, remove these objects to declutter your environment by entering rm(data, yvar, preds, pred_of_interest) in the Console.\n\nfit_mod_and_extract &lt;- function(___) {\n    # Use str_c to create a string (formula_str) that looks like \"yvar ~ pred1 + pred2\"\n    # Look at the documentation for a helpful argument\n    mod_formula_str &lt;- \n    mod_form &lt;- as.formula(mod_formula_str)\n    \n    # Fit a linear model using the constructed formula and given data\n    mod &lt;- lm(mod_form, data = data)\n    \n    # Obtain 95% confidence interval\n    ci &lt;- confint(mod, level = 0.95)\n    \n    # Return the coefficient estimate and CI for the predictor of interest\n    tibble(\n        which_pred = pred_of_interest,\n        estimate = ___,\n        ci_lower = ___,\n        ci_upper = ___\n    )\n}\n\n\n\nSolutions\n\n\nfit_mod_and_extract &lt;- function(data, yvar, preds, pred_of_interest) {\n    # Use str_c to create a string (formula_str) that looks like \"yvar ~ pred1 + pred2\"\n    # Look at the documentation for a helpful argument\n    mod_formula_str &lt;- str_c(yvar, \"~\", str_c(preds, collapse = \"+\"))\n    mod_form &lt;- as.formula(mod_formula_str)\n    \n    # Fit a linear model using the constructed formula and given data\n    mod &lt;- lm(mod_form, data = data)\n    \n    # Obtain 95% confidence interval\n    ci &lt;- confint(mod, level = 0.95)\n    \n    # Return the coefficient estimate and CI for the predictor of interest\n    tibble(\n        which_pred = pred_of_interest,\n        estimate = mod$coefficients[pred_of_interest],\n        ci_lower = ci[pred_of_interest, \"2.5 %\"],\n        ci_upper = ci[pred_of_interest, \"97.5 %\"]\n    )\n}\n\n\nfit_mod_and_extract(data = mtcars, yvar = \"mpg\", preds = c(\"hp\", \"wt\"), pred_of_interest = \"hp\")\n\n# A tibble: 1 × 4\n  which_pred estimate ci_lower ci_upper\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 hp          -0.0318  -0.0502  -0.0133"
  },
  {
    "objectID": "11-iteration-1.html",
    "href": "11-iteration-1.html",
    "title": "Loops and iteration",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nUse the across() function to wrangle multiple variables simultaneously\n\nCompare across() to an approach with pivot_longer() and pivot_wider()\n\nWrite a for loop in R to handle repeated tasks\nUse the map() family of functions in the purrr package to handle repeated tasks\n\n\nYou can download a template Quarto file to start from here. Put this file in a folder called iteration within a folder for this course."
  },
  {
    "objectID": "11-iteration-1.html#exercises",
    "href": "11-iteration-1.html#exercises",
    "title": "Loops and iteration",
    "section": "Exercises",
    "text": "Exercises\n(Not a pair programming exercise, but check in with each other as you work)\nUsing the diamonds dataset:\n\nTransform the x, y, and z columns so that the units of millimeters are displayed (e.g., “4.0 mm”).\nConvert all numeric columns into character columns.\n\nHint: type is. and hit Tab in the Console. Scroll through the function options. Do the same with as.\n\n\n\n\nSolutions\n\n\nadd_mm &lt;- function(x) {\n    str_c(x, \" mm\")\n}\n\ndiamonds %&gt;% \n    mutate(across(.cols = x:z, .fns = add_mm))\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price x       y       z      \n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1  0.23 Ideal     E     SI2      61.5    55   326 3.95 mm 3.98 mm 2.43 mm\n 2  0.21 Premium   E     SI1      59.8    61   326 3.89 mm 3.84 mm 2.31 mm\n 3  0.23 Good      E     VS1      56.9    65   327 4.05 mm 4.07 mm 2.31 mm\n 4  0.29 Premium   I     VS2      62.4    58   334 4.2 mm  4.23 mm 2.63 mm\n 5  0.31 Good      J     SI2      63.3    58   335 4.34 mm 4.35 mm 2.75 mm\n 6  0.24 Very Good J     VVS2     62.8    57   336 3.94 mm 3.96 mm 2.48 mm\n 7  0.24 Very Good I     VVS1     62.3    57   336 3.95 mm 3.98 mm 2.47 mm\n 8  0.26 Very Good H     SI1      61.9    55   337 4.07 mm 4.11 mm 2.53 mm\n 9  0.22 Fair      E     VS2      65.1    61   337 3.87 mm 3.78 mm 2.49 mm\n10  0.23 Very Good H     VS1      59.4    61   338 4 mm    4.05 mm 2.39 mm\n# ℹ 53,930 more rows\n\n\n\ndiamonds %&gt;%\n    mutate(across(.cols = where(is.numeric), .fns = as.character))\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price x     y     z    \n   &lt;chr&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 0.23  Ideal     E     SI2     61.5  55    326   3.95  3.98  2.43 \n 2 0.21  Premium   E     SI1     59.8  61    326   3.89  3.84  2.31 \n 3 0.23  Good      E     VS1     56.9  65    327   4.05  4.07  2.31 \n 4 0.29  Premium   I     VS2     62.4  58    334   4.2   4.23  2.63 \n 5 0.31  Good      J     SI2     63.3  58    335   4.34  4.35  2.75 \n 6 0.24  Very Good J     VVS2    62.8  57    336   3.94  3.96  2.48 \n 7 0.24  Very Good I     VVS1    62.3  57    336   3.95  3.98  2.47 \n 8 0.26  Very Good H     SI1     61.9  55    337   4.07  4.11  2.53 \n 9 0.22  Fair      E     VS2     65.1  61    337   3.87  3.78  2.49 \n10 0.23  Very Good H     VS1     59.4  61    338   4     4.05  2.39 \n# ℹ 53,930 more rows\n\n\n\n\nWhat if we wanted to perform multiple transformations on each of many variables?\nWithin the different values of diamond cut, let’s summarize the mean, median, and standard deviation of the numeric variables. When we look at the .fns argument in the across() documentation, we see that we can provide a list of functions:\n\ndiamonds %&gt;%\n    group_by(cut) %&gt;% \n    summarize(across(.cols = where(is.numeric), .fns = list(mean = mean, med = median, sd = sd)))\n\n# A tibble: 5 × 22\n  cut     carat_mean carat_med carat_sd depth_mean depth_med depth_sd table_mean\n  &lt;ord&gt;        &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 Fair         1.05       1       0.516       64.0      65      3.64        59.1\n2 Good         0.849      0.82    0.454       62.4      63.4    2.17        58.7\n3 Very G…      0.806      0.71    0.459       61.8      62.1    1.38        58.0\n4 Premium      0.892      0.86    0.515       61.3      61.4    1.16        58.7\n5 Ideal        0.703      0.54    0.433       61.7      61.8    0.719       56.0\n# ℹ 14 more variables: table_med &lt;dbl&gt;, table_sd &lt;dbl&gt;, price_mean &lt;dbl&gt;,\n#   price_med &lt;dbl&gt;, price_sd &lt;dbl&gt;, x_mean &lt;dbl&gt;, x_med &lt;dbl&gt;, x_sd &lt;dbl&gt;,\n#   y_mean &lt;dbl&gt;, y_med &lt;dbl&gt;, y_sd &lt;dbl&gt;, z_mean &lt;dbl&gt;, z_med &lt;dbl&gt;,\n#   z_sd &lt;dbl&gt;\n\n\nWhat does the list of functions look like? What is the structure of this list object?\n\nlist_of_fcts &lt;- list(mean = mean, med = median, sd = sd)\nlist_of_fcts\n\n$mean\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x124b3f870&gt;\n&lt;environment: namespace:base&gt;\n\n$med\nfunction (x, na.rm = FALSE, ...) \nUseMethod(\"median\")\n&lt;bytecode: 0x12502c0a8&gt;\n&lt;environment: namespace:stats&gt;\n\n$sd\nfunction (x, na.rm = FALSE) \nsqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), \n    na.rm = na.rm))\n&lt;bytecode: 0x114a9c2c8&gt;\n&lt;environment: namespace:stats&gt;\n\nstr(list_of_fcts)\n\nList of 3\n $ mean:function (x, ...)  \n $ med :function (x, na.rm = FALSE, ...)  \n $ sd  :function (x, na.rm = FALSE)  \n\n\nWe’ll be working more with lists soon."
  },
  {
    "objectID": "11-iteration-1.html#exercises-1",
    "href": "11-iteration-1.html#exercises-1",
    "title": "Loops and iteration",
    "section": "Exercises",
    "text": "Exercises\nPair programming exercises: Whoever has most recently eaten dessert (broadly interpreted) should be the driver first. Switch after Exercise 2.\nWrite for-loops that do each of the following:\n\nPrints the even numbers from 2:20\n\nProduce the same output with the seq() function\n\nIterates over the month.name vector and stores a character vector of output containing strings like “Month 1: January”, “Month 2: February”.\n\nProduce the same output with str_c() only.\n\nOn the diamonds dataset, fit models of price vs. carat separately for each value of cut, and store the fitted models in a list storage container.\n\n\n\nSolutions\n\n\nfor (i in seq_len(10)) {\n    print(2*i)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n[1] 10\n[1] 12\n[1] 14\n[1] 16\n[1] 18\n[1] 20\n\nseq(from = 2, to = 20, by = 2)\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\n\nmonth_strings &lt;- vector(\"character\", length = length(month.name))\n\nfor (i in seq_along(month.name)) {\n    month_strings[i] &lt;- str_c(\"Month \", i, \": \", month.name[i])\n}\nmonth_strings\n\n [1] \"Month 1: January\"   \"Month 2: February\"  \"Month 3: March\"    \n [4] \"Month 4: April\"     \"Month 5: May\"       \"Month 6: June\"     \n [7] \"Month 7: July\"      \"Month 8: August\"    \"Month 9: September\"\n[10] \"Month 10: October\"  \"Month 11: November\" \"Month 12: December\"\n\nstr_c(\"Month \", 1:12, \": \", month.name)\n\n [1] \"Month 1: January\"   \"Month 2: February\"  \"Month 3: March\"    \n [4] \"Month 4: April\"     \"Month 5: May\"       \"Month 6: June\"     \n [7] \"Month 7: July\"      \"Month 8: August\"    \"Month 9: September\"\n[10] \"Month 10: October\"  \"Month 11: November\" \"Month 12: December\"\n\n\n\ndata(diamonds)\n\n# Fit models of price vs. carat separately for each value of cut\nunique_cuts &lt;- diamonds %&gt;% pull(cut) %&gt;% levels()\nlin_mod_results &lt;- vector(mode = \"list\", length = length(unique_cuts))\n\nfor (i in seq_along(unique_cuts)) {\n    this_cut &lt;- unique_cuts[i]\n    diamonds_sub &lt;- diamonds %&gt;%\n        filter(cut==this_cut)\n    # The double square brackets [[i]] accesses the ith element of a list\n    lin_mod_results[[i]] &lt;- lm(price ~ carat, data = diamonds_sub)\n}"
  },
  {
    "objectID": "11-iteration-1.html#exercises-2",
    "href": "11-iteration-1.html#exercises-2",
    "title": "Loops and iteration",
    "section": "Exercises",
    "text": "Exercises\nGoal: In the diamonds dataset, we want to understand the relationship between price and size (carat). We want to explore variation along two choices:\n\nThe variables included in the model. We’ll explore 3 sets of variables:\n\nNo further variables (just price and carat)\nAdjusting for cut\nAdjusting for cut and clarity\nAdjusting for cut, clarity, and color\n\nWhether or not to remove outliers in the carat variable. We’ll define outliers as cases whose carat is over 3 SDs away from the mean.\n\nWork with your partner on the following exercises (not in a pair-programming fashion). As you work, make note of what is challenging and any helpful thought processes/strategies that arise from the collaboration.\nExercise 1: Use crossing() to create the data frame of argument combinations for our analyses. Call it df_arg_combos. Note that you can create a list of formula objects in R with c(y ~ x1, y ~ x1 + x2). (Something like this will be the right hand side of an argument to crossing().)\n\n\nSolution\n\n\ndf_arg_combos &lt;- crossing(\n    mod_formula = c(price ~ carat, price ~ carat + cut,  price ~ carat + cut + clarity,  price ~ carat + cut + clarity + color),\n    remove_outliers = c(TRUE, FALSE)\n)\ndf_arg_combos\n\n# A tibble: 8 × 2\n  mod_formula remove_outliers\n  &lt;list&gt;      &lt;lgl&gt;          \n1 &lt;formula&gt;   FALSE          \n2 &lt;formula&gt;   TRUE           \n3 &lt;formula&gt;   FALSE          \n4 &lt;formula&gt;   TRUE           \n5 &lt;formula&gt;   FALSE          \n6 &lt;formula&gt;   TRUE           \n7 &lt;formula&gt;   FALSE          \n8 &lt;formula&gt;   TRUE           \n\n\n\nExercise 2: Write a function called remove_outliers that removes outliers in a dataset. The user should be able to supply the dataset (data), the variable to remove outliers in (what_var), and a threshold on the number of SDs away from the mean used to define outliers (sd_thresh). Write your function so that it runs as follows: remove_outliers(diamonds, what_var = carat, sd_thresh = 3).\n\n\nSolution\n\n\nremove_outliers &lt;- function(data, what_var, sd_thresh) {\n    data %&gt;% \n        mutate(zscore = ({{ what_var }} - mean({{ what_var }}, na.rm = TRUE))/sd({{ what_var }}, na.rm = TRUE)) %&gt;%\n        filter(zscore &lt;= sd_thresh)\n}\n\n\nExercise 3: Write a function called fit_model that implements the different settings for our diamonds analysis.\n\nfit_model &lt;- function(data, mod_formula, remove_outliers) {\n    # remove_outliers is a TRUE/FALSE flag of whether or not to remove outliers\n    # This function implements our specific use case: outliers are cases that are 3 SDs away from the mean for the carat variable\n    \n    # Use mod_formula as the first argument of lm()\n}\n\nfit_model(data = diamonds, mod_formula = price ~ carat, outlier_var = carat, )\n\n\n\nSolution\n\n\nfit_model &lt;- function(data, mod_formula, remove_outliers) {\n    if (remove_outliers) {\n        data_clean &lt;- remove_outliers(data, what_var = carat, sd_thresh = 3)\n    } else {\n        data_clean &lt;- data\n    }\n    \n    lm(mod_formula, data = data_clean)\n}\n\n\nExercise 4: Write a for loop that stores the fitted linear models from all versions of the analysis.\nRecall that you can pull out the contents of a single data frame column in many ways. For a data frame df with a variable named x:\n\ndf$x\ndf %&gt;% pull(x)\ndf[[\"x\"]]\n\nNote that in df_arg_combos:\n\nmod_formula: this column is a list and you can extract the 1st element with [[1]]\nremove_outliers: this column is a vector and you can extract the 1st element with [1]\n\n\n\nSolution\n\n\nlin_mod_res_for &lt;- vector(mode = \"list\", length = nrow(df_arg_combos))\n\nfor (i in seq_along(lin_mod_res_for)) {\n    this_formula &lt;- df_arg_combos$mod_formula[[i]] # Double [[ for the **list** of formulas\n    this_remove_outliers &lt;- df_arg_combos$remove_outliers[i] # Single [ for the **atomic vector** of logicals\n    lin_mod_res_for[[i]] &lt;- fit_model(\n        data = diamonds,\n        mod_formula = this_formula,\n        remove_outliers = this_remove_outliers\n    )\n}"
  },
  {
    "objectID": "homework1.html",
    "href": "homework1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Required parts\n\nDue by Monday, January 29 at midnight.\n\n\nGetting acquainted with Git and GitHub\nNothing to turn in for this section, but please do the following:\n\nWatch this video and follow along with the tutorial linked in the description.\nRead this tutorial to get more familiar with version control terminology.\n\n\nAsynchronous skills challenges\n\nAdvanced visualization with ggplot2 - Challenge 1: Navigate to the challenge on GitHub. This challenge involves finishing the plot that we started in our Advanced Data Visualization in ggplot2 class activity.\n\n\nPreparing for Skills Session 1\n\nSkills Session 1 is coming up the week of 2/5. The topic of this session is keyboard shortcuts. It’s a good idea to prepare for this session as soon as possible because the benefits of being fluent with keyboard shortcuts will be immediately useful, and it will take time to practice these shortcuts.\nStart preparing for this session by visiting the Skills Session 1 page.\n\n\nSetting up your personal website\nNothing to turn in for this section, but please do the following:\n\nFollow this guide written by Professor Brianna Heggeseth to set up your personal website (your digital portfolio) using Quarto.\n\n\n\n\nOptional\nIf you are aiming for an A in the course, recall from our syllabus that participating in 5 Tidy Tuesday challenges can move you toward this goal.\n\nFor further all-around practice, I encourage you to participate in the most recent Tidy Tuesday challenge."
  },
  {
    "objectID": "homework2.html",
    "href": "homework2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Required parts\n\nDue by Monday, February 5 at midnight.\n\n\nAsynchronous skills challenges\n\nAdvanced visualization with ggplot2 - Challenge 2: Navigate to the challenge on GitHub.\nAdvanced map visualization - Challenge 1: Navigate to this challenge on GitHub.\n\n\nPreparing for Skills Session 1\n\nSkills Session 1 is coming up the week of 2/5. The topic of this session is keyboard shortcuts. Keep practicing your keyboard shortcuts this week.\nVisit the Skills Session 1 page for details.\n\n\n\n\nOptional\nIf you are aiming for an A in the course, recall from our syllabus that participating in 5 Tidy Tuesday challenges can move you toward this goal.\n\nFor further all-around practice, I encourage you to participate in the most recent Tidy Tuesday challenge."
  },
  {
    "objectID": "homework3.html",
    "href": "homework3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Required parts\n\nDue by Monday, February 12 at midnight.\n\n\nAsynchronous skills challenges\n\nAdvanced map visualization - Challenge 2: Navigate to this challenge on GitHub.\nShiny - Challenge 1: Complete the app from our class activity. Submit your app.R file on Moodle.\n\nProject Milestone 1: See information on our Project page.\n\n\n\nOptional\nIf you are aiming for an A in the course, recall from our syllabus that participating in 5 Tidy Tuesday challenges can move you toward this goal.\n\nFor further all-around practice, I encourage you to participate in the most recent Tidy Tuesday challenge."
  },
  {
    "objectID": "homework4.html",
    "href": "homework4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Required parts\n\nDue by Monday, February 19 at midnight.\n\n\nAsynchronous skills challenges (Just one this week)\n\nShiny - Challenge 2: Complete the steps in the Advanced interactivity with plotly section of our in class Shiny activity. Submit your app.R file on Moodle.\n\nReflection 1: In a Google Doc, respond to the following prompts:\n\nComment on the evolution of your understanding from class activity, to Challenge 1, and to Challenge 2 for our core topics so far: advanced ggplot2 visualization, maps, and Shiny. What was challenging initially? How have your skills progressed? What would you still like to work on? What support or resources would help you make the progress you want?\nComment on the role of peers in your learning. How have you collaborated with peers so far? What would you like collaboration to look like? What can you and the instructor do to move towards that goal?\n\nPlease draw on observations from your personal class journal as you write so that you can add specific examples to your reflection.\nSubmit a link to your Google Doc on Moodle.\n\n\n\nOptional\nIf you are aiming for an A in the course, recall from our syllabus that participating in 5 Tidy Tuesday challenges can move you toward this goal.\n\nFor further all-around practice, I encourage you to participate in the most recent Tidy Tuesday challenge."
  },
  {
    "objectID": "homework5.html",
    "href": "homework5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Required parts\n\nDue by Monday, February 26 at midnight.\n\n\nAsynchronous skills challenges\n\nWrangling+functions - Challenge 1: Navigate to this challenge on GitHub.\n\nProject Milestone 2: See information on our Project page.\n\n\n\nOptional\nIf you are aiming for an A in the course, recall from our syllabus that participating in 5 Tidy Tuesday challenges can move you toward this goal.\n\nFor further all-around practice, I encourage you to participate in the most recent Tidy Tuesday challenge."
  },
  {
    "objectID": "homework6.html",
    "href": "homework6.html",
    "title": "Homework 6",
    "section": "",
    "text": "Required parts\n\nDue by Friday, March 8 at midnight.\n\n\nAsynchronous skills challenges\n\nWrangling+functions - Challenge 2: Navigate to this challenge on GitHub.\n\n\n\n\nOptional\nIf you are aiming for an A in the course, recall from our syllabus that participating in 5 Tidy Tuesday challenges can move you toward this goal.\n\nFor further all-around practice, I encourage you to participate in the most recent Tidy Tuesday challenge."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT/COMP 212: Intermediate Data Science (Spring 2024)",
    "section": "",
    "text": "This is the course website for STAT/COMP 212: Intermediate Data Science at Macalester College for the Spring 2024 semester taught by Professor Leslie Myint. Materials were developed by Leslie Myint and multiple faculty members in the Macalester MSCS department.\n\nDrop-in (office) hours\nLeslie\n\nOLRI 232\nMondays, Wednesdays, and Fridays 3:30-5:00pm\n\nI’m also happy to meet one-on-one if my normal drop-in hours don’t work. You can schedule a time to meet with me via Calendly.\nKyle\n\nOLRI SubHub (OLRI 102) (Area with glass windows in the middle of the 1st floor directly below the OLRI Hub)\nMondays 5:30-9:00pm\n\nNa\n\nSmail Gallery\nMondays, Wednesdays, and Fridays 2:00-4:00pm\n\nGraham\n\nOLRI SubHub (OLRI 102) (Area with glass windows in the middle of the 1st floor directly below the OLRI Hub)\nThursdays 4:00-7:00pm"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project",
    "section": "",
    "text": "The goal of the project is to build something awesome that you are proud to showcase on your personal website."
  },
  {
    "objectID": "project.html#category-thoughtful-engagement-with-data-context",
    "href": "project.html#category-thoughtful-engagement-with-data-context",
    "title": "Project",
    "section": "Category: Thoughtful engagement with data context",
    "text": "Category: Thoughtful engagement with data context\nOverview of this category:\n\nCareful consideration of the who, what, when, where, why, and how of your datasets and how that affects results and interpretation\nCareful consideration of how the data context relates to ethical considerations of how you investigate your data and how your results should be used/interpreted. It will help to think about the following:\n\nWho is affected by the project’s data acquisition and results?\nWhat (mis)interpretations or actions might result from the conduct of your investigations or your conclusions?\nHow might negative consequences be mitigated?\n\n\nExcellent work will:\n\nThoughtfully consider the items above by drawing on team members’ lived experiences AND perspectives from at least 2 other sources (e.g., news articles, research articles, blog posts, press releases, documentation from organizations affiliated with your data). These sources should be referenced in your final digital artifact.\n\nPassing work will:\n\nThoughtfully consider the items above by drawing on team members’ lived experiences\n\nNeeds Improvement work will:\n\nAttempt to consider the items above but need more thought"
  },
  {
    "objectID": "project.html#category-effective-data-storytelling",
    "href": "project.html#category-effective-data-storytelling",
    "title": "Project",
    "section": "Category: Effective data storytelling",
    "text": "Category: Effective data storytelling\nThe final digital artifact should:\n\nMotivate the importance of the topic\nLead the reader through the rationale for the narrowing/focusing of the scope via the main 2-3 broad questions\nTie results (plots and modeling output) to the broad questions and explain how all results fit together\nEnd with main takeaways, limitations, and future directions\nUse clear and concise communication throughout\n\nExcellent work will:\n\nMeet all above the above quality expectations\n\nPassing work will:\n\nMeet most above the above quality expectations\n\nNeeds Improvement work will:\n\nMeet some above the above quality expectations"
  },
  {
    "objectID": "project.html#category-code-quality-and-documentation",
    "href": "project.html#category-code-quality-and-documentation",
    "title": "Project",
    "section": "Category: Code quality and documentation",
    "text": "Category: Code quality and documentation\nOverview of this category:\n\nCode duplication is minimal to nonexistent with the use of functions and loops\nUse comments appropriately to document what is happening in different parts of code\nText space before and after code chunks is used for longer form (paragraph) documentation\n\nExcellent work will:\n\nUse functions in all instances where code would have been copied and pasted twice\nUse functions and loops in all instances where code would have been copied and pasted 3 or more times\nConsistently use (but not overuse) comments within R code to document code in a way that allows your future selves to remember what was going on. (Not every line of code needs to have a comment, but groups of lines that achieve a particular goal should have a comment.)\nThe text space before and after code chunks explains what is happening in each code chunk (e.g., why particular wrangling steps were performed, the motivation for fitting certain models, why different modeling outputs were extracted).\n\nPassing work will:\n\nOften use functions and loops to reduce code duplication but have a small number of missed opportunities\nOften use code comments and text space before and after code chunks for documentation but have a small number of missed opportunities\n\nNeeds Improvement work will:\n\nSometimes use functions and loops to reduce code duplication but have several missed opportunities\nSometimes use code comments and text space before and after code chunks for documentation but have several missed opportunities"
  },
  {
    "objectID": "project.html#category-file-organization-and-version-control",
    "href": "project.html#category-file-organization-and-version-control",
    "title": "Project",
    "section": "Category: File organization and version control",
    "text": "Category: File organization and version control\nOverview of this category:\n\nSeparate files for cleaning, plotting, and modeling\nClean datasets are saved at the end of code files devoted to cleaning and loaded in at the start of code files devoted to plotting and modeling\nAs seen through GitHub commits and file diffs, each team member should contribute roughly equally to the codebase. Note that contributions include both writing new lines of code AND modifying/deleting code (e.g., perhaps to reduce code duplication).\n\nNote: I am not going to count the number of lines added/modified by each team member. Rather, I will look at the GitHub commits and file diffs holistically.\n\n\nExcellent work will:\n\nMeet all above the above quality expectations\n\nPassing work will:\n\nGenerally meet the above quality expectations but will require more separation of tasks across files or more use of saving clean datasets\n\nNeeds Improvement work will:\n\nSometimes meet the above quality expectations but will require more separation of tasks across files, more use of saving clean datasets, and more even contributions from team members to the codebase"
  },
  {
    "objectID": "project.html#milestone-1",
    "href": "project.html#milestone-1",
    "title": "Project",
    "section": "Milestone 1",
    "text": "Milestone 1\nDue date: Monday, February 12\nPurpose: The goal of Milestone 1 is to get the project moving early on in the semester to have time to make the final product as high quality as possible. You will form teams, lay the vision for your project, and make progress on that vision with one dataset.\nTask (requirements for passing this Milestone):\nPut the following information in a PDF, and submit on Moodle. (Only one team member per group needs to submit.)\n\nWrite down names of all team members\nBriefly describe your topic/scope in a phrase/sentence.\nDescribe 2-3 broad questions that you wish to explore within this topic. (Not all of them might be able to be investigated with the data source you find for this Milestone—that’s fine.)\nFind one data source, and read that data into R.\nThoroughly describe the data context (who, what, when, where, why, and how? questions related to the dataset).\nWrite up a data codebook. That is, describe the type and meaning of the variables in your dataset. Group your variables into categories (e.g., demographic variables, neighborhood variables).\n\nIf you have a lot of variables, it may not be necessary/feasible to describe every variable individually. Rather, you can describe groups of similar variables.\n\nBased on the data context write-up and your codebook, describe which of your 2-3 broad questions can be addressed with your dataset and how.\n\nWrite a plan for addressing these questions. Make sure that the steps in this plan are reasonable to complete in 2 weeks for Milestone 2. You will receive feedback on this plan and will be expected to integrate this feedback for Milestone 2."
  },
  {
    "objectID": "project.html#milestone-2",
    "href": "project.html#milestone-2",
    "title": "Project",
    "section": "Milestone 2",
    "text": "Milestone 2\nDue date: Monday, February 26\nPurpose: The goal of Milestone 2 is to make progress on the goals you set out earlier and get tailored feedback on next steps to make the final product as high quality as possible.\nTask (requirements for passing this Milestone):\n\nCreate a team GitHub repository with a folder structure as follows:\n\nyour_github_project_folder\n\ncode\ndata\nresults\n\n\nCreate a .gitignore file.\n\nWhat is this? .gitignore is a special file that tells Git what files and folders to ignore in version control.\nSteps to create this file:\n\nOpen the Terminal in RStudio. Enter the command pwd. Make sure that a path to your project folder is displayed. If not, use the command cd RELATIVE/PATH/TO/YOUR/PROJECT/FOLDER to change the directory to your project folder. The part that comes after cd is a relative path from your current location to your project folder. If you need to use cd, use pwd again afterward to confirm that you are in your project folder.\nEnter the command touch .gitignore.\nEnter the command ls -a. You should see all files in this directory (including hidden files that start with .). You should see the .gitignore file.\nEnter the command open .gitignore. Note that you can use tab completion in the Terminal for typing shortcuts. After you type open .giti hit Tab to auto-complete the rest of the .gitignore file name. This will open the .gitignore file in RStudio or your computer’s plain text editor.\nAdd the following lines to your .gitignore file, and save the file.\ndata/\n.DS_Store (a hidden Mac file)\n.Rhistory\n.Rproj.user\n*.Rproj\n.quarto/\n\nAdd, commit, and push your code/ folder and the .gitignore file. Unless you change your .gitignore later, this is the only time you will need to add, commit, and push your .gitignore file.\n\nAdd the instructor as a collaborator to your project GitHub repository. (GH username: lmyint)\nAdd a code chunk to the end of all of your .Rmd/.qmd documents with sessionInfo()\n\nWhen rendering your markdown file to HTML, this adds information about all packages used in the file as well as their package versions. As packages get updated over time, old code may break, so it is good to know what version of a package was used to complete your work so that you can restore that particular package version.\n\nComplete the steps in your plan from Milestone 1 (the plan with feedback from the instructional team)\nWrite a plan for further pursuing your 2-3 broad questions. Make sure that the steps in this plan are reasonable to complete in 2 weeks for Milestone 3. You will receive feedback on this plan and will be expected to integrate this feedback for Milestone 3. Questions to think about as you develop this plan:\n\nDo your 2-3 original broad questions need to be revised?\nIs it time to start looking for additional datasets?"
  },
  {
    "objectID": "project.html#milestone-3",
    "href": "project.html#milestone-3",
    "title": "Project",
    "section": "Milestone 3",
    "text": "Milestone 3\nDue date: Wednesday, March 27\nPurpose: The goal of Milestone 3 is to make progress on the goals you set out earlier and get tailored feedback on next steps to make the final product as high quality as possible.\nTask (requirements for passing this Milestone):\n\nComplete the steps in your plan from Milestone 2 (the plan with feedback from the instructional team)\nWrite a short blog post (no more than 1000 words) about your results so far. Use a .qmd to generate an HTML file for this post. In this post, you should:\n\nMotivate the importance of the topic\nLead the reader through the rationale for the narrowing/focusing of the scope via the main 2-3 broad questions\nTie results (plots and modeling output) to the broad questions and explain how all results fit together\nEnd with main takeaways, limitations with regard to the data context and ethical considerations, and future directions"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Readings in the schedule below will sometimes be marked with abbreviations:\n\nR4DS refers to the online book R for Data Science (2e) by Wickham, Cetinkaya-Rundel, and Grolemund.\nIDSNotes refers to Intermediate Data Science Notes, a set of notes that I have written for this course.\n\nGuiding questions for pre-class readings and videos are available at the bottom of this page.\n\n\n\n  \n    Week\n    Tuesday\n    Thursday\n    Announcements\n  \n\n\n\n  \n    1\n    \n    \n    \n    \n      1/18: Welcome! Getting to know each other, brainstorming project ideas \n      Before class:\n      \n        Review the syllabus, and write down any questions you have.\n        Follow the Tech Setup directions.\n      \n    \n    \n    \n    \n  \n  \n  \n  \n  \n    2\n    \n    \n      1/23: Advanced visualization in ggplot2 \n      Before class:\n      \n        Review the construction of plots from STAT 112 and STAT 155 by answering the Guiding Questions at the bottom of this page.\n      \n    \n    \n    \n      1/25: File organization and paths, Git & GitHub \n      Before class:\n      \n        Read File organization, directory structure, and navigation in IDSNotes.\n        Work on the following parts of HW1: Getting acquainted with Git and GitHub, Preparing for Skills Session 1.\n      \n    \n    \n    \n      Turn in HW1 by midnight on Monday 1/29.\n    \n  \n  \n  \n  \n  \n    3\n    \n    \n      1/30: Advanced map visualization \n      Before class:\n      \n        Watch this video on Coordinate Reference Systems, and answer the Guiding Questions at the bottom of this page.\n        Run the package installation commands in the Guiding Questions section at the bottom of this page.\n      \n    \n    \n    \n      2/1: Advanced map visualization (continued) \n      Before class:\n    \n    \n    \n      Turn in HW2 by midnight on Monday 2/5. Prepare for Skills Session 1 next week (2/5-2/9).\n    \n  \n  \n  \n  \n  \n    4\n    \n    \n      2/6: Interactive visualization \n      Before class: Install the shiny and plotly R packages.\n      Listen to this podcast (timestamp 18:09-25:27). Reflect on the Guiding Question at the bottom of this page.\n    \n    \n    \n      2/8: Interactive visualization (continued) \n      Before class: \n    \n    \n    \n      Skills Session 1 will be happening this week. Turn in HW3 (includes Project Milestone 1) by midnight on Monday 2/12.\n    \n  \n  \n  \n  \n  \n    5\n    \n    \n      2/13: Data wrangling \n      Before class: Read Chapters 12, 13, 16, 17 in R4DS.\n    \n    \n    \n      2/15: Data wrangling and project work time \n      Before class: Read Chapters 14, 15, 19 in R4DS.\n    \n    \n    \n      Turn in HW4 and Reflection 1 by midnight on Monday 2/19.\n    \n  \n  \n  \n  \n  \n    6\n    \n    \n      2/20: Statistical modeling and missing data \n      Before class: \n    \n    \n    \n      2/22: Writing functions, more version control \n      Before class: Read R4DS Chapter 26 (Functions) and RPDS Section 13.1 (if-else).\n    \n    \n    \n      Turn in HW5 (includes Project Milestone 2) by midnight on Monday 2/26.\n    \n  \n  \n  \n  \n  \n    7\n    \n    \n      2/27: Functions and version control(continued) \n      Before class: \n    \n    \n    \n      2/29-3/1: Capstone Days! (No class but please attend talks to support your peers!)\n    \n    \n    \n      Turn in HW5 (includes Project Milestone 2) by midnight on Monday 2/26.\n    \n  \n  \n  \n  \n  \n    8\n    \n    \n      3/5: Loops and iteration \n      Before class: Read R4DS Chapter 27 (Iteration) and this tutorial.\n    \n    \n    \n      3/7: Loops and iteration (continued) \n      Before class: \n    \n    \n    \n      Turn in HW6 by midnight on FRIDAY 3/8.\n    \n  \n  \n  \n  \n  \n    9\n    \n      3/9-3/17: Spring Break!\n    \n  \n  \n  \n  \n  \n    10\n    \n    \n      3/19: Data acquisition: APIs \n      Before class: \n    \n    \n    \n      3/21: Data acquisition: APIs (continued) \n      Before class: \n    \n    \n    \n      Turn in HW7 (includes Project Milestone 3) by midnight on Wednesday 3/27.\n    \n  \n  \n  \n  \n  \n    11\n    \n    \n      3/26: Data acquisition: Scraping \n      Before class: Read the rvest vignette.\n    \n    \n    \n      3/28: Project work day \n      Before class: \n    \n    \n    \n      Turn in HW8 by midnight on Wednesday 4/3.\n    \n  \n  \n  \n  \n  \n    12\n    \n    \n      4/2: Data acquisition: databases \n      Before class: Read R4DS Chapter 22 (Databases).\n    \n    \n    \n      4/4: Data acquisition: databases (continued)\n    \n    \n    \n      Turn in HW9 by midnight on Wednesday 4/10.\n    \n  \n  \n  \n  \n  \n    13\n    \n    \n      4/9: Project work time\n    \n    \n    \n      4/11: Project work time\n    \n    \n    \n      Schedule Skills Session 3 for next week (4/15-4/19) via Calendly.\n    \n  \n  \n  \n  \n  \n    14\n    \n    \n      4/16: Project work time\n    \n    \n    \n      4/18: Project work time\n    \n    \n    \n      Skills Session 3 will be happening this week.\n    \n  \n  \n  \n  \n  \n    15\n    \n      4/23: \n    \n    \n    \n      4/25: Last day of class\n    \n    \n    \n      Turn in Reflection 3 by midnight on Friday 4/26."
  },
  {
    "objectID": "schedule.html#advanced-visualization-in-ggplot2",
    "href": "schedule.html#advanced-visualization-in-ggplot2",
    "title": "Schedule",
    "section": "1/23: Advanced visualization in ggplot2",
    "text": "1/23: Advanced visualization in ggplot2\nTo review plot creation skills from STAT/COMP 112 and STAT 155, use the diamonds dataset in the ggplot2 package to recreate the following visualizations:\n\nlibrary(ggplot2)\ndata(diamonds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\nlibrary(gridExtra)\np1 &lt;- ggplot(diamonds, aes(x = cut)) + geom_bar() + theme_classic()\np2 &lt;- ggplot(diamonds, aes(x = price)) + geom_histogram() + theme_classic()\np3 &lt;- ggplot(diamonds, aes(x = price)) + geom_density() + theme_classic()\ngrid.arrange(p1, p2, p3, nrow = 1)\n\np4 &lt;- ggplot(diamonds, aes(x = cut, y = price)) + geom_boxplot() + theme_classic()\np5 &lt;- ggplot(diamonds, aes(x = price, color = cut)) + geom_density() + theme_classic()\ngrid.arrange(p4, p5, nrow = 1)\n\np6 &lt;- ggplot(diamonds, aes(x = cut, fill = color)) + geom_bar() + theme_classic()\np7 &lt;- ggplot(diamonds, aes(x = cut, fill = color)) + geom_bar(position = \"dodge\") + theme_classic()\np8 &lt;- ggplot(diamonds, aes(x = cut, fill = color)) + geom_bar(position = \"fill\") + theme_classic()\ngrid.arrange(p6, p7, p8, nrow = 1)\n\np9 &lt;- ggplot(diamonds, aes(x = carat, y = price)) + geom_point() + theme_classic()\np10 &lt;- ggplot(diamonds, aes(x = carat, y = price, color = cut)) + geom_point() + geom_smooth(method = \"lm\") + theme_classic()\ngrid.arrange(p9, p10, nrow = 1)"
  },
  {
    "objectID": "schedule.html#advanced-map-visualization",
    "href": "schedule.html#advanced-map-visualization",
    "title": "Schedule",
    "section": "1/30: Advanced map visualization",
    "text": "1/30: Advanced map visualization\nAfter/while watching this video on Coordinate Reference Systems (CRS), answer the following questions:\n\nWhat is the shape of the Earth?\nWhat are the two components of a CRS/GCS?\nWhy do we use many different local CRSs rather than just one CRS for the whole earth?\nWhy is it insufficient to identify a location by its latitude and longitude?\nWhy do we need to be mindful about CRSs when working with different spatial datasets?\n\nRun the following package installation commands:\n\ninstall.packages(c(\"sf\", \"elevatr\", \"terra\", \"stars\", \"tidycensus\", \"remotes\"))\ninstall.packages(\"USAboundariesData\", repos = \"http://packages.ropensci.org\", type = \"source\")\nremotes::install_github(\"ropensci/USAboundaries\")"
  },
  {
    "objectID": "schedule.html#interactive-visualization-in-shiny",
    "href": "schedule.html#interactive-visualization-in-shiny",
    "title": "Schedule",
    "section": "2/6: Interactive visualization in Shiny",
    "text": "2/6: Interactive visualization in Shiny\nWhat was new, unexpected, or interesting in the discussion about animations, interactivity, and dashboards?"
  },
  {
    "objectID": "skills_session1.html",
    "href": "skills_session1.html",
    "title": "Skills Session 1",
    "section": "",
    "text": "Purpose\nGet really good with keyboard shortcuts for two main reasons:\n\nUsing shortcuts saves a ton of time\nIt feels really cool!\n\n\n\nTask\n\nReview and practice all shortcuts described here.\nDuring our in-person skills session (week of 2/5-2/9), I will ask you to demonstrate your ability to use most of these shortcuts within a roughly 5 minute span.\n\n\n\nRequirements for passing\nYou will Pass this Skills Session if you can do all of the commands I ask without using your mouse.\nIf you forget a few shortcuts, it’s ok! Just write down the ones you missed and show them to me the next day.\n\n\nScheduling\nBecause this is a very short Skills Session (SS), we don’t need to formally schedule exact times. Use any of the following options to complete this SS with me:\n\nCome by my office at any point during these times this week:\n\nMonday 2/5: 10:00am-2:30pm, 3:30-5:00pm\nWednesday 2/7: 3:30-5:00pm\nFriday 2/9: noon-5:00pm\n\nI’ll stay after class on Tuesday 2/6 and Thursday 2/8 for 10 minutes (11:10-11:20am).\n\nFor just this SS, I can check in with 2 students at a time."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Nature doesn’t reveal its secrets easily. - Thomas Kempa\n\nNor do data.\nBut that is exactly what can make data science so thrilling!\nThis course is about empowering you with the wisdom to ask the best questions of data–ones that are meaningful, adaptive, and equity-minded–and the technical savvy to answer them.\nBecause your careers (whether in data science or not), will all involve further learning and working with others, my other primary goal is for you to cultivate self-reflection skills with regards to your own learning and your collaboration with others. In this way, I hope that you feel confident learning new skills on your own in the future and contributing to a welcoming work community.\n\n\n\n\n\n\nCourse catalog description\n\n\n\n\n\nThis second course in the data science curriculum emphasizes advanced data wrangling and manipulation, interactive visualization, writing functions, working with data in databases, version control, and data ethics. Through open-ended and interdisciplinary projects, students practice the constant feedback loop of asking questions of the data, manipulating the data to help answer the question, and then returning to more questions. Prerequisite(s): COMP 112 and COMP 123 and STAT 155; STAT 253 recommended but not required.\n\n\n\n\n\nBy the end of this course you should be able to:\n\nSustain a reflection practice\n\nReflect on your learning process so that you are equipped for independent learning\nReflect on your collaborative work so that you can form community no matter where you go\n\nCreate effective visualizations and interactive applications\n\nCreate a variety of visualizations in ggplot2 that go beyond the plot types that you learned in STAT/COMP 112\nWrangle and visualize spatial data\nCreate interactive web applications and visualizations that adapt to user input\n\nWrangle arbitrarily messy data\n\nUse appropriate R tools to manage numeric, logical, date, strings, and factors\nUse appropriate R tools to write functions and loops\nUse appropriate methods when working with missing data\nDouble check your data cleaning steps to ensure accuracy\n\nAcquire data from a variety of sources\n\nWrite queries in structured query language (SQL) to access data from databases\nWrite code to access data from application programming interfaces (APIs)\nWrite code to scrape data from websites and evaluate the ethics of collecting such data\n\nCraft high quality data stories\n\nIterate on the question-explore-question cycle to craft compelling data stories with attention to data context and ethical considerations\nUse a combination of data acquisition, data wrangling, static and interactive visualization, and statistical modeling to further a data science investigation\n\nUse AI and search tools to figure out difficult tasks\n\nUse appropriate coding jargon to construct effective search queries (e.g., Google) and evaluate the accuracy of results that you find\nConstruct effective AI prompts (e.g., Chat GPT, Google Bard) and evaluate the accuracy of generated results\nArticulate the ethical and environmental considerations in using AI and search tools\n\nUse professional data science tools\n\nUse Git as a version control system\nMaintain a digital portfolio of your data science projects on your personal website\n\n\n\n\n\n\n\n\nReflect\n\n\n\n\nWhich of the learning goals above do you disagree with or want more clarity on?\nDo you have any goals that you’d like to include on this list?"
  },
  {
    "objectID": "syllabus.html#course-learning-goals",
    "href": "syllabus.html#course-learning-goals",
    "title": "Syllabus",
    "section": "",
    "text": "By the end of this course you should be able to:\n\nSustain a reflection practice\n\nReflect on your learning process so that you are equipped for independent learning\nReflect on your collaborative work so that you can form community no matter where you go\n\nCreate effective visualizations and interactive applications\n\nCreate a variety of visualizations in ggplot2 that go beyond the plot types that you learned in STAT/COMP 112\nWrangle and visualize spatial data\nCreate interactive web applications and visualizations that adapt to user input\n\nWrangle arbitrarily messy data\n\nUse appropriate R tools to manage numeric, logical, date, strings, and factors\nUse appropriate R tools to write functions and loops\nUse appropriate methods when working with missing data\nDouble check your data cleaning steps to ensure accuracy\n\nAcquire data from a variety of sources\n\nWrite queries in structured query language (SQL) to access data from databases\nWrite code to access data from application programming interfaces (APIs)\nWrite code to scrape data from websites and evaluate the ethics of collecting such data\n\nCraft high quality data stories\n\nIterate on the question-explore-question cycle to craft compelling data stories with attention to data context and ethical considerations\nUse a combination of data acquisition, data wrangling, static and interactive visualization, and statistical modeling to further a data science investigation\n\nUse AI and search tools to figure out difficult tasks\n\nUse appropriate coding jargon to construct effective search queries (e.g., Google) and evaluate the accuracy of results that you find\nConstruct effective AI prompts (e.g., Chat GPT, Google Bard) and evaluate the accuracy of generated results\nArticulate the ethical and environmental considerations in using AI and search tools\n\nUse professional data science tools\n\nUse Git as a version control system\nMaintain a digital portfolio of your data science projects on your personal website\n\n\n\n\n\n\n\n\nReflect\n\n\n\n\nWhich of the learning goals above do you disagree with or want more clarity on?\nDo you have any goals that you’d like to include on this list?"
  },
  {
    "objectID": "syllabus.html#meet-the-instructional-team",
    "href": "syllabus.html#meet-the-instructional-team",
    "title": "Syllabus",
    "section": "Meet the instructional team",
    "text": "Meet the instructional team\n\nLeslie Myint (instructor)\nAbout me: One of my greatest joys is sharing the beauty of data-driven thinking, so I’m thrilled to be teaching this course! I also get very excited talking about all things games! I love playing board games, Dungeons and Dragons (D&D), and Nintendo console games. I also love staying active with weightlifting and rock climbing and hoping to learn cross-country skiing this winter!\n  \nKyle Suelflow (preceptor)\nAbout me: I am super excited to be precepting this course! I am a Sophomore Data Science major. We did lots of cool things last semester, and I hope you all will enjoy it as much as I did. I am a captain of the open ultimate frisbee team here at Mac, and I love to go hiking. I am hoping to go abroad and hike somewhere over the summer. Like Leslie, I also really enjoy board games and playing cards.\n\n  \nNa Nguyen (preceptor)\nAbout me: Class of 2025 | Hanoi, Vietnam | Data Science and International Studies (major), Educational Studies (minor) | An EdTech enthusiast\n\n  \nGraham Elliot (preceptor)\nAbout me: Hi Everyone! My name is Graham and I am a senior Data Science major here at Mac. I am really excited to help all of you with anything you need this semester, data science or otherwise. I love everything sports and everything movies, and I also do a lot of running. Please feel free to reach out and come to office hours whenever you need help with anything."
  },
  {
    "objectID": "syllabus.html#how-to-contact-me",
    "href": "syllabus.html#how-to-contact-me",
    "title": "Syllabus",
    "section": "How to contact me",
    "text": "How to contact me\n\n\n\n\n\n\nCall me “Leslie”\n\n\n\nStudents sometimes wonder what to call their professors. I prefer to be called Leslie (lez-lee), but if you prefer to be more formal, I am also ok with Professor Myint (pronounced “mee-int”). My preferred gender pronouns are she/her/hers.\nPlease help me make sure that I call you by your preferred name and pronouns too!\n\n\nI love getting to talk to students outside of class time—whether about class-related topics or anything else. Come chat with me!\nI’ll be setting times for drop-in hours based on feedback from the pre-course survey. I’ll update my drop-in hours on our course homepage and Moodle when they’re finalized.\nI’m also happy to meet one-on-one if my normal drop-in hours don’t work. You can schedule a time to meet with me via Calendly."
  },
  {
    "objectID": "syllabus.html#discussion-board-slack",
    "href": "syllabus.html#discussion-board-slack",
    "title": "Syllabus",
    "section": "Discussion board (Slack)",
    "text": "Discussion board (Slack)\nSlack is a commonly used communication tool in industry and is useful to be familiar with, so we’ll be using it as our discussion board.\n\nIf you’re new to Slack, this video provides a quick overview.\nFirst join our STAT/COMP 212: Spring 2024 workspace here.\nAfter joining, you can access our workspace here. (You might want to bookmark this if you have Slack open in your web broswer.)"
  },
  {
    "objectID": "syllabus.html#community-is-key",
    "href": "syllabus.html#community-is-key",
    "title": "Syllabus",
    "section": "Community is key",
    "text": "Community is key\nA sense of community and connectedness can provide a powerful environment for learning: Research shows that learning is maximized when students feel a sense of belonging in the educational environment (e.g., Booker, 2016). A negative climate may create barriers to learning, while a positive climate can energize students’ learning (e.g., Pascarella & Terenzini, cited in How Learning Works, 2012).\nFor these reasons, I will be designing our in-class group activities to intentionally foster commmunity and connectedness. You can help cultivate our classroom community by being thoughtful about the way you engage with others in class."
  },
  {
    "objectID": "syllabus.html#reflection-is-paramount",
    "href": "syllabus.html#reflection-is-paramount",
    "title": "Syllabus",
    "section": "Reflection is paramount",
    "text": "Reflection is paramount\nThe content you learn will be cool (unbiased opinion!), but it is a guarantee that as technology evolves, some part of it will become out of date during your careers. What you will need to rely on when you leave Macalester is what I want to ensure you cultivate now: a good learning process. And the cornerstone of a good learning process is reflection.\nReflection is not just fundamental to learning content–it’s fundamental to learning any sort of intellectual, emotional, or physical skill. For this reason, I will be prioritizing reflection as a goal for our course in both content learning and collaborative activities. (Note that these reflection goals are the first two course learning goals.)"
  },
  {
    "objectID": "syllabus.html#mistakes-are-essential",
    "href": "syllabus.html#mistakes-are-essential",
    "title": "Syllabus",
    "section": "Mistakes are essential",
    "text": "Mistakes are essential\n\nAn expert is a person who has made all the mistakes which can be made in a narrow field. - Niels Bohr, Nobel Prize-winning physicist\n\nI don’t feel comfortable working with a new R package until I’ve seen the same errors over and over again. Seeing new errors helps me understand the constraints of the code and the assumptions that I was making about my data."
  },
  {
    "objectID": "syllabus.html#communication-is-a-superpower",
    "href": "syllabus.html#communication-is-a-superpower",
    "title": "Syllabus",
    "section": "Communication is a superpower",
    "text": "Communication is a superpower\nEvery time I go to a conference talk on a technical topic, it is striking how quickly laptops or phones come out because of the inability to follow. Academics notoriously struggle to make ideas accessible to others.\nI want communication to be very different for you.\nEvery time you communicate ideas–whether through writing, visuals, or oral presentation–I want you to be a total boss. The end product of strong communication is a better experience for all those who have given you their attention. What’s more, the process of crafting effective communication is invaluable for deepening your own understanding:\n\n\n\nRead to collect the dots, write to connect them pic.twitter.com/YbgnKKFUNn\n\n— David Perell (@david_perell) July 5, 2021"
  },
  {
    "objectID": "syllabus.html#outside-of-class",
    "href": "syllabus.html#outside-of-class",
    "title": "Syllabus",
    "section": "Outside of class",
    "text": "Outside of class\nPre-class videos/readings: Most class periods will have a required video or reading to review ideas from previous courses or to familiarize yourself with new concepts before seeing them again in class. My goal for these videos and readings is for you to get the most out of class time by being able to more easily follow explanations in class and to engage most fully in class activities. I will provide Guiding Questions for each video/reading to focus your attention.\n\n\n\n\n\n\nSuggestion\n\n\n\n\nScan the Guiding Questions before watching/reading to preview the main ideas. Fill in answers to these questions as you read.\nAsk (and answer!) questions in the #questions channel in our Slack workspace.\nRecord any reflections from in-class time about your learning process or interactions with peers while they are still fresh.\nAfter learning a new topic in class, it is helpful to immediately attempt the related exercises on the upcoming homework assignment.\nCome to instructor drop-in hours to chat about the course or anything else! 😃"
  },
  {
    "objectID": "syllabus.html#during-class",
    "href": "syllabus.html#during-class",
    "title": "Syllabus",
    "section": "During class",
    "text": "During class\nClass time will be a mix of interactive lecture and longer stretches of group work. During the lecture portion, I will pause explanation frequently to prompt a short exercise or ask questions that you’ll reflect on individually or together.\n\n\n\n\n\n\nSuggestion\n\n\n\nReview your learning process and group work reflections just before class to frame how you want to engage in class. (Perhaps you’ve noted a struggle and want to try a new strategy.)"
  },
  {
    "objectID": "syllabus.html#my-philosophy",
    "href": "syllabus.html#my-philosophy",
    "title": "Syllabus",
    "section": "My philosophy",
    "text": "My philosophy\nGrading is thorny issue for many educators because of its known negative effects on learning and motivation. Nonetheless, it is ever-present in the US education system and at Macalester. Because I am required to submit grades for this course, it’s worth me taking a minute to share my philosophy about grading with you.\nWhat excites me about being a teacher is your learning. Learning flourishes in an environment where you find meaning and value in what we’re exploring, feel safe engaging with challenging things, receive useful feedback, and regularly reflect on your learning.\nIf I didn’t have to give grades, I wouldn’t. But because I am required to, it is important to me to create a course structure and grading system that creates an environment for learning to flourish:\n\nFinding meaning and value: I am striving to achieve this by creating space for authentic connection between you, your peers, and myself and by encouraging you to explore a topic that intrigues you for our course project.\nSafety in engaging with challenges: The assignments and activities that we will use to learn are meant to be challenging, and it would be unreasonable for me to expect that you perform perfectly on the first try. For this reason, every assignment and assessment has an opportunity for unlimited revisions/reattempts without penalty. I hope that this alleviates stress considerably. If ever you are feeling overwhelmed by this course, please reach out to me. We’ll find a way to make things more manageable.\n\nNote: While the number of revisions you can submit is unlimited in theory, in practice, there is a limit to how quickly the preceptors and I can review revisions and give feedback.\n\nReceiving useful feedback and reflecting regularly: In order to learn maximally by pursuing a revision, you need BOTH good feedback and to reflect thoughtfully about misconceptions in your learning. Our preceptors and I will strive to give useful comments and prompts to spur reflection when we see room for improvement. A requirement for submitting a revision is to include a paragraph where you describe and reflect on your prior misconceptions."
  },
  {
    "objectID": "syllabus.html#assignments-and-assessments",
    "href": "syllabus.html#assignments-and-assessments",
    "title": "Syllabus",
    "section": "Assignments and assessments",
    "text": "Assignments and assessments\n\nAsynchronous skills demonstrations\nOur course learning goals will have associated challenges for practicing the tools/concepts. During weekly homework assignments, you will work on one challenge from the most recent class topic as well as the prior class topic. A challenge will either receive a grade of Pass (P) or Not Yet (NY). Requirements for passing will be clearly described in each challenge.\n\n\n\n\n\n\n\n\nSkills category\n\n\n\n\n\n\nAdvanced ggplot2\nChallenge 1\nChallenge 2\n\n\nSpatial visualization\nChallenge 1\nChallenge 2\n\n\nShiny\nChallenge 1\nChallenge 2\n\n\nData wrangling\nChallenge 1\nChallenge 2\n\n\nWriting functions\nChallenge 1\nChallenge 2\n\n\nWriting loops\nChallenge 1\nChallenge 2\n\n\nAPIs\nChallenge 1\nChallenge 2\n\n\nWeb scraping\nChallenge 1\nChallenge 2\n\n\nSQL\nChallenge 1\nChallenge 2\n\n\n\n\n\n\n\n\n\nPurpose\n\n\n\nThe purpose of skills challenges is to engage in targeted and repeated practice for core skills. The reason for interleaving different topics within a single homework assignment is to promote skills becoming more deeply ingrained by spacing out practice over time.\n\n\nRevising and resubmitting challenges: If you receive a grade of NY, you can revise and resubmit your work without penalty as long as you do the following:\n\nWrite a reflection paragraph at the top of your assignment in which you address: What improvements were you asked to make based on feedback on your previous submission? How has reviewing your feedback improved your understanding? (What do you understand better/differently than you did before?)\nSubmit your revised work by issuing another pull request on the GitHub Classroom challenge link.\n\n\n\nIn-person skills sessions\nA skills session (SS) is a discussion that you and I will have about course content. There will be 3 SS’s in the semester (weeks of 2/5, 3/4, and 4/15).\n\nSkills session 1: This session will be very short (5 minutes) and will focus on your fluency with keyboard shortcuts. Sometime during the week of 2/5-2/9 come talk to me to demonstrate your keyboard shortcut usage.\nSkills session 2: This session will be 30 minutes. Schedule this during the week of 3/4-3/8 via Calendly.\nSkills session 3: This session will be 30 minutes. Schedule this during the week of 4/15-4/19 via Calendly.\n\nOne week before the SS, I will provide a set of problems that you can (and should!) work on with peers. During the SS, we will talk through a subset of those problems. I will choose some problems that I’d like you to talk through, and in the time remaining, you will talk through a problem (or part of a problem) of your choosing.\n\n\n\n\n\n\nPurpose\n\n\n\nThe purpose of a SS is to encourage deep and collaborative study and to give us both a detailed understanding of your learning.\n\n\nBefore the SS I will provide a rubric that explains how I will assess your understanding. I will also provide requirements for a grade of Pass (P). If you do not Pass an SS, you will receive a grade of Not Yet (NY).\nRe-attempting a skills session: If you receive a grade of NY, you can re-attempt the SS without penalty as long as you do the following:\n\nSchedule another discussion of the same length as the original SS (via Calendly).\nRevise how you will talk through the problem (or parts of problems) that you struggled with.\nReflect on the following: What improvements were you asked to make based on feedback on your previous submission? How has reviewing your feedback improved your understanding? (What do you understand better/differently than you did before?) Be prepared to tell me about this reflection at the next SS.\n\n\n\nReflections\nRoughly 1, 2, and 3 months into the semester, you will write reflections in which you think about your goals, progress, and next steps. To provide observations that you can draw from in these reflections, I will be asking you to maintain a personal class journal in which you regularly record insights from working on class activities.\nReflections that show thoughtfulness with incorporation of concrete observations from the personal class journal will receive a grade of Pass (P).\nRevising and resubmitting reflections: If your reflection is not yet passing, I will give feedback on some areas for improvement/additional consideration and ask you to resubmit.\n\n\nProject\nThe best way to learn data science and feel like a data scientist is to work on meaningful data-driven projects. The course project will be a semester-long, collaborative experience in which you investigate a series of meaningful questions using multiple datasets.\n\n\n\n\n\n\nPurpose\n\n\n\nThe purpose of the project is to engage in a meaningful and collaborative data-driven experience and to build something that you would be proud to showcase to an employer on your personal website.\n\n\nThrough regular milestones (roughly every 2 weeks) throughout the semester you will set goals for future milestones, make progress on the goals you set out in the previous milestone, and integrate feedback from previous milestones. Details about project milestones and deliverables will be housed on the Project page.\nEach project milestone will receive a grade of Pass (P) or Not Yet (NY) based on the progress made relative to the goals that we agree upon.\nRevising and resubmitting milestones: If you receive a grade of NY, you can revise and resubmit your work without penalty, but it is important that we have a discussion about why goals were not met so that we can plan a reasonable path forward."
  },
  {
    "objectID": "syllabus.html#course-grading-system",
    "href": "syllabus.html#course-grading-system",
    "title": "Syllabus",
    "section": "Course grading system",
    "text": "Course grading system\n\nRequirements for a B\nIn order to earn a final letter grade of B, you will need to:\n\nAsynchronous skills demonstrations: Pass the first challenge in each skills category.\nIn-person skills sessions: Pass all 3 in-person skills sessions.\nReflection: Pass all 3 monthly reflections.\nProject: Pass all project checkpoints. Submit a passing code base and a passing digital artifact.\n\n\n\nRequirements for an A\nIn order to earn a final letter grade of A, you will need to meet the requirements for a B and do the following:\n\nAsynchronous skills demonstrations: Pass both challenges in each skills category.\nProject: Thoughtfully integrate peer and instructor feedback to create a codebase and digital artifact that go beyond the Passing requirements and meet the Excellent requirements in at least 2 areas.\nYour choice: One of the following:\n\nMake a good faith effort at 5 different Tidy Tuesday challenges. A good faith effort involves posing a research question, making a clean plot with good labeling that addresses that question, interpreting the plot in light of data limitations, and describing a next step in the investigation.\nLearn a new skill or an existing topic more deeply. If you choose this option, talk with me to discuss what this might look like. (Examples: Python, Tableau, writing R packages, a statistical modeling concept)"
  },
  {
    "objectID": "syllabus.html#late-work",
    "href": "syllabus.html#late-work",
    "title": "Syllabus",
    "section": "Late work",
    "text": "Late work\nHomework assignments will generally be due weekly on Mondays at midnight. (There are 2 assignments due on Fridays.) If you anticipate needing more time to complete an assignment, please email me ahead of time to discuss. Limited extensions will always be granted:\n\nThe ideal extension: Turn in the homework by the following Wednesday morning at 9am (a 1 day, 9 hour extension). The instructional team will often be working to give feedback on Wednesdays, so having an assignment turned in by Wednesday morning is helpful."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic integrity",
    "text": "Academic integrity\nAcademic integrity is the cornerstone of our learning community. Students are expected to be familiar with the college’s standards on academic integrity.\nI encourage you to work with your classmates to discuss material and ideas for assignments, but in order for you to receive individualized feedback on your own learning, you must submit your own work. This involves writing your own code and putting explanations into your own words. Always cite any sources you use, including AI (see section below)."
  },
  {
    "objectID": "syllabus.html#artificial-intelligence-ai-use",
    "href": "syllabus.html#artificial-intelligence-ai-use",
    "title": "Syllabus",
    "section": "Artificial intelligence (AI) use",
    "text": "Artificial intelligence (AI) use\nLearning to use AI tools is an emerging skill that we will explore together in this course. I expect you to use AI (ChatGPT, Google Bard)—in fact, some assignments may require it.\nPlease be aware of the limits of AI:\n\nAI does not always generate accurate output. If it gives you a number, fact, or code, assume it is wrong unless you either know the answer or can check in with another source. AI works best for topics you already understand to a sufficient extent.\nIf you provide minimum effort prompts, you will get low quality results. You will need to refine your prompts in order to get good outcomes. This will take work.\nBe thoughtful about when this tool is useful. Don’t use it if it isn’t appropriate for the case or circumstance.\nThe environmental impact of AI should not be ignored. The building and usage of AI tools consumes a lot of energy (see here and here). For this reason, we will be very thoughtful about when we use AI and will discuss other sustainability behaviors that we can incorporate into our lives to offset this usage.\nAI is a tool, but one that you need to acknowledge using. Any ideas, language, or code that is produced by AI must be cited, just like any other resource.\n\nHow to cite AI: Please include a paragraph at the end of any assignment that uses AI explaining what you used the AI for and what prompts you used to get the results. Failure to do so is in violation of the academic integrity policy at Macalester College.\n\n\nIf you have any questions about your use of AI tools, please contact me to discuss them."
  },
  {
    "objectID": "tech_setup.html",
    "href": "tech_setup.html",
    "title": "Tech Setup",
    "section": "",
    "text": "Follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions.\n\nRequired: Change the default file download location for your internet browser.\n\nGenerally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. You need to change this option so that your browser asks you where to save each file before downloading it.\nThis page has information on how to do this for the most common browsers.\n\n\nRequired: Re-install R and RStudio.\n\nFIRST: Download R here.\n\nIn the top section, tou will see three links “Download R for …”\nChoose the link that corresponds to your computer.\nAs of January 16, 2024, the latest version of R is 4.3.2 (“Eye Holes”).\n\nSECOND: Download RStudio here.\n\nClick the button under step 2 to install the version of RStudio recommended for your computer.\nAs of January 16, 2024, the latest version of RStudio is 2023.12.0-369.\n\nTHIRD: Check that when you go to File &gt; New Project &gt; New Directory, you see “Quarto Website” as an option.\n\n\nSuggested: Watch this video describing key configuration options for RStudio.\n\nRequired: Install required packages.\n\nAn R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways.\nOpen RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter.\n\n\ninstall.packages(c(\"tidyverse\"))\n\n\nYou will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again.\nEnter the command library(tidyverse) and hit Enter.\nIf you see an error message, then there was a problem installing the package. Post the full error message in the #questions channel in our Slack workspace and\nQuit RStudio. You’re done setting up!\n\nOptional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio.\n\nRequired: Set essential RStudio options.\nGo to Edit &gt; Preferences &gt; General\nNavigate to the “Workspace” section.\n\nRestore .RData into workspace at startup: Leave this unchecked\nSave workspace to .RData on exit: Select “Never”\n\nWithout doing this RStudio will save all of the objects in your Environment. In practice, this leads to all of the objects, datasets, etc that you have ever worked with at Macalester being loaded in when you start RStudio.\n\nThis can make startup slow.\nIt clutters the Environment. (e.g., You’re working on something and referring to diamonds not knowing that a diamonds that was used in class last year is already in the Environment.)"
  }
]