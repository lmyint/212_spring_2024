[
  {
    "objectID": "01-introductions.html",
    "href": "01-introductions.html",
    "title": "Welcome to the course!",
    "section": "",
    "text": "Welcome to Intermediate Data Science! Last semester was the first offering of this course–I really enjoyed it. I am even more thrilled to have made improvements that will make it a better learning experience for you!"
  },
  {
    "objectID": "01-introductions.html#plan-for-today",
    "href": "01-introductions.html#plan-for-today",
    "title": "Welcome to the course!",
    "section": "Plan for today",
    "text": "Plan for today\n\nWhat is this course about?\nGet to know your classmates\nShaping our syllabus together\nBrainstorming project ideas and connecting with potential project partners via the 12 Favorite Problems framework\nWarming up our wrangling and visualization skills with Tidy Tuesday!\n\n(When I go through announcements at the end of class, I’ll also go over some syllabus highlights.)"
  },
  {
    "objectID": "01-introductions.html#what-is-this-course-about",
    "href": "01-introductions.html#what-is-this-course-about",
    "title": "Welcome to the course!",
    "section": "What is this course about?",
    "text": "What is this course about?\n\nExpanding your abilities for self-reflection in service of:\n\nYour lifelong independent learning\nOur course community\n\nExpanding your data science toolbox:\n\nVisualization\nWrangling\nData acquisition\nData storytelling\n\n\nI’ve intentionally put reflection first and data science skills second not necessarily in order of importance but because cultivating data science skills will come automatically—reflection and community-building won’t."
  },
  {
    "objectID": "01-introductions.html#get-to-know-your-classmates",
    "href": "01-introductions.html#get-to-know-your-classmates",
    "title": "Welcome to the course!",
    "section": "Get to know your classmates",
    "text": "Get to know your classmates\nIn groups, introduce yourselves with the following prompts: (~2 minutes/person)\n\nName, preferred pronouns\nMacalester connections (e.g., majors/minors/concentrations, clubs, teams, events regularly attended)\nHow are you feeling about the coming semester?\nWhat is one thing you are excited to talk about in conversation?\nIf you could use data to investigate anything, what would it be and why?"
  },
  {
    "objectID": "01-introductions.html#syllabus-shaping-learning-goals",
    "href": "01-introductions.html#syllabus-shaping-learning-goals",
    "title": "Welcome to the course!",
    "section": "Syllabus shaping: learning goals",
    "text": "Syllabus shaping: learning goals\nNavigate to the Course learning goals section of our syllabus.\nPart 1: Reflect (~3 min)\nWrite a few sentences responding to the following questions:\n\nWhat are your goals in taking this class?\nDo you see your goals reflected in the course learning goals? If not, how would you like to see the course goals amended to see your goals reflected in them?\n\nPart 2: Share (~5 min)\nAt your tables, take turns sharing your responses to the above questions. As a group, summarize your discussion in this Google Doc.\nBefore we meet again next Tuesday, I will look over your comments in the Google Doc and add my own responses. I’ll address your comments in class next Tuesday."
  },
  {
    "objectID": "01-introductions.html#course-project-brainstorming",
    "href": "01-introductions.html#course-project-brainstorming",
    "title": "Welcome to the course!",
    "section": "Course project: brainstorming",
    "text": "Course project: brainstorming\nIn a data science course, a course project is essential for synthesizing our tools in a meaningful context.\nOur course project will be a semester-long experience because I believe that this longer time span will improve the quality of the projects.\nWe will start brainstorming ideas today using a framework called the 12 Favorite Problems (12FP)."
  },
  {
    "objectID": "01-introductions.html#favorite-problems-context",
    "href": "01-introductions.html#favorite-problems-context",
    "title": "Welcome to the course!",
    "section": "12 Favorite Problems: context",
    "text": "12 Favorite Problems: context\nRichard Feynman was a Nobel prize-winning physicist whose contributions fundamentally reshaped our understanding of the physical world.\nA major part of his success was a method for viewing the world: a mindset of viewing the world through the lens of several open-ended questions. Feynman called these his “favorite problems.” He said of these problems:\n\nYou have to keep a dozen of your favorite problems constantly present in your mind, although by and large they will lay in a dormant state. Every time you hear or read a new trick or a new result, test it against each of your twelve problems to see whether it helps. Every once in a while there will be a hit, and people will say, “How did [they] do it? [They] must be a genius!”\nQuote source: Forte Labs"
  },
  {
    "objectID": "01-introductions.html#the-12-favorite-problems-framework",
    "href": "01-introductions.html#the-12-favorite-problems-framework",
    "title": "Welcome to the course!",
    "section": "The 12 Favorite Problems framework",
    "text": "The 12 Favorite Problems framework\nA favorite problem is a meaningful, open-ended question that allows you to learn, explore, and act with intention on your biggest interests in life. Here are two of mine:\n\nHow can I be the kind of mother I can feel proud of without losing myself?\nHow can I have a fulfilling career without burning out?\n\nFormulating several of these favorite problems can lead to several benefits:\n\n\nDedicate your time and attention to ideas that truly spark your curiosity\nSee how a piece of information might be useful and why it’s worth keeping\nSee insightful patterns across multiple subjects that seem unrelated, but might share a common thread\nFocus the impact of your work on problems where you can make a real difference\nPrime your subconscious to notice helpful solutions to your biggest challenges in the world around you\nAttract like-minded people who have the same interests and goals as you\n\nSource: Forte Labs\n\nRelevance to our course: Brainstorming 12 favorite problems can help us determine some questions that the course project can address."
  },
  {
    "objectID": "01-introductions.html#brainstorming-our-12-favorite-problems-fps",
    "href": "01-introductions.html#brainstorming-our-12-favorite-problems-fps",
    "title": "Welcome to the course!",
    "section": "Brainstorming our 12 favorite problems (FPs)",
    "text": "Brainstorming our 12 favorite problems (FPs)\n\nOpen up a blank file in which to type.\nNavigate to this article by Tiago Forte, and scroll down to the first step “Get started with these prompts.”\nUsing these prompts, take about 15 minutes to brainstorm your own 12 favorite problems.\n\nTiago Forte provides examples of his 12 FPs in his post. Feel free to also look at my own for more examples. (I’m working on updating my 12 FPs today alongside you!)\n\n\nSave your 12FP file in a place you’ll be able to find easily."
  },
  {
    "objectID": "01-introductions.html#sharing-and-refining-our-12-fps",
    "href": "01-introductions.html#sharing-and-refining-our-12-fps",
    "title": "Welcome to the course!",
    "section": "Sharing and refining our 12 FPs",
    "text": "Sharing and refining our 12 FPs\nIn groups, each person will have ~2 minutes to share their top 2 FPs and get some feedback from the group. The group should give feedback to help make the FPs more specific, counterintuitive, and interdiscipinary:\n\nSpecific:\n\nOriginal: “How can I be a better leader?” is a little broad.\nPossible improvement: “How can I be a better leader as an introvert?”\n\nCounterintuitive:\n\nOriginal: “How can I improve the standard of living in the global south?”\nPossible improvement: “How can I improve the standard of living in the global south without further contributing to the climate change that threatens those regions the most?”\n\nInterdisciplinary:\n\nOriginal: How can I improve education?”\nPossible improvement: “How can I improve education by borrowing ideas from video games?”\n\n\n(Examples from Forte Labs)"
  },
  {
    "objectID": "01-introductions.html#broadcast-your-signal-to-start-finding-your-people",
    "href": "01-introductions.html#broadcast-your-signal-to-start-finding-your-people",
    "title": "Welcome to the course!",
    "section": "Broadcast your signal to start finding your people",
    "text": "Broadcast your signal to start finding your people\n\nJoin our course Slack workspace via this invite link.\n\nIf you’ve already joined, navigate to our Slack workspace here.\n\nIn the #general channel, write a very brief post in which you:\n\nIntroduce yourself however you see fit.\nDescribe the general areas that your 12 favorite problems tend to cover.\nIf you already feel a pull towards a project area, share that too."
  },
  {
    "objectID": "01-introductions.html#project-opportunity-collaborating-with-a-community-partner",
    "href": "01-introductions.html#project-opportunity-collaborating-with-a-community-partner",
    "title": "Welcome to the course!",
    "section": "Project opportunity: collaborating with a community partner",
    "text": "Project opportunity: collaborating with a community partner\nIgnite Afterschool is looking to partner with data science students on a few fronts:\n\nThey are looking to update the data briefs that they use to communicate with families about the impact of afterschool programming.\n\nThis will involve looking at data from the Minnesota Student Survey. (I already have access to this data.)\nThere is opportunity for creative input from students on how best to display this information.\n\nThey are also looking for help with a data-driven policy analysis surrounding the impact of the recent cannabis legalization laws on youth outcomes. There is potential to explore this with the MN Student Survey data as well as other data.\n\nIf you are interested in this opportunity, please reach out to me by Tuesday, January 23."
  },
  {
    "objectID": "01-introductions.html#tidy-tuesday",
    "href": "01-introductions.html#tidy-tuesday",
    "title": "Welcome to the course!",
    "section": "Tidy Tuesday!",
    "text": "Tidy Tuesday!\nFor the remainder of the class period, we’ll work on the most recent Tidy Tuesday challenge.\nFeel free to clarify anything about the course with me during this time!"
  },
  {
    "objectID": "01-introductions.html#announcements",
    "href": "01-introductions.html#announcements",
    "title": "Welcome to the course!",
    "section": "Announcements",
    "text": "Announcements\nBefore class on Tuesday, please do the following:\n\nSet up R and RStudio using these instructions.\nUpdate your Slack profile with preferred name, pronouns, name pronunciation. (To find your profile, click on your name under Direct Messages on the left menu, and click “Edit Profile”.)\nComplete the pre-course survey.\nLook at the Guiding Questions for next Tuesday’s class on advanced visualization with ggplot2.\nTake a look at Homework 1."
  },
  {
    "objectID": "02-adv-ggplot.html",
    "href": "02-adv-ggplot.html",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nNavigate the ggplot2 reference page to find the functions needed to create a desired visualization\nUse the information on a function help page to construct desired plot features\n\nScan the information in the Usage section to identify function arguments that must be set\nUnderstand how the function arguments work by using information in the Arguments section\nUse the information in the the Aesthetics and Examples sections to control plot appearance\n\nIdentify when it would be necessary to use different data arguments within the ggplot() and geom_() layers"
  },
  {
    "objectID": "02-adv-ggplot.html#the-goal",
    "href": "02-adv-ggplot.html#the-goal",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "The goal",
    "text": "The goal\nWe are going to recreate this NYT visualization on record setting temperatures by expanding our ggplot2 toolbox using data from San Francisco (SFO) in 2011.\n\n\n\nScreenshot of NYTimes visualization from 2015"
  },
  {
    "objectID": "02-adv-ggplot.html#setup",
    "href": "02-adv-ggplot.html#setup",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "Setup",
    "text": "Setup\nFirst load the tidyverse package, and read in the San Francisco weather data.\n\nlibrary(tidyverse)\nweather &lt;- read_csv(\"https://lmyint.github.io/212_spring_2024/data/sfo_weather.csv\")"
  },
  {
    "objectID": "02-adv-ggplot.html#codebook",
    "href": "02-adv-ggplot.html#codebook",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "Codebook",
    "text": "Codebook\nYou will need to refer to the variable codebook below throughout this activity.\n\nMonth: Month of the year (1-12)\nDay: Day within the month (1-31)\nLow/High: Low/high temperature this day\nNormalLow/NormalHigh: Typical low/high temperature for this day of the year\nRecordLow/RecordHigh: Record low/high temperature for this day of the year\nLowYr/HighYr: Year in which the record low/high was observed\nPrecip: Amount of precipitation (inches) this day\nRecordPrecip: Record amount of precipitation for this day of the year\nPrecipYr: Year in which the record precipitation was observed\ndate: The actual date in 2011 for this day in YYYY-MM-DD format\ndateInYear: What day of the year is it? (1-365)\nRecord: Logical (TRUE/FALSE) indicating whether this day had a high temperature record\nRecordText: Text that displays the record high for this day (\"Record high: ##\")\nRecordP: Logical (TRUE/FALSE) indicating whether this day had a precipitation record\nCulmPrec: Cumulative precipitation for the month up to this day"
  },
  {
    "objectID": "02-adv-ggplot.html#class-exercises",
    "href": "02-adv-ggplot.html#class-exercises",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "Class exercises",
    "text": "Class exercises\nClass exercise 1: Examine the temperature visualization in the original NYT article.\n\nData storytelling: Relate the intro paragraph (“Scientists declared that 2015 was Earth’s hottest year on record…”) to the design of the visualization: Based on the intro paragraph, what key message/claim does NYT want readers to be able to explore? How did this goal inform what information is displayed in the visualization?\nTechnical implementation: What specific variables (from the codebook) underlie the visualization, and how do they map to visual elements (e.g., position, size, shape, and color of the glyphs)?\n\n\nWe can explore the “Geoms” section of the ggplot2 reference page to find a geom that corresponds to the visual elements in the temperature plot.\nClass exercise 2: Using both the small example visuals on the right and the names of the geom’s, brainstorm some possibilities for geom’s we might use to recreate the temperature visualization.\n\nWe need to explore further by opening up the geom reference pages to understand if a particular geom is suitable for our task. We’ll look at the geom_point documentation page to learn the process for reading a documentation page.\nWhen looking at a help page, it is useful to first look at the Usage and Arguments sections.\nThe Usage section shows all of the possible inputs (arguments) to the geom–these are all of the ways that a geom can be customized. Just looking at the argument names can help give a hint as to what arguments might fit our needs.\nThe Arguments section explains in detail what each argument does and the possible values the argument can take. The mapping, data, and ... arguments will be the most commonly used by far.\n\nmapping: This is the argument that is being used when you specify a plots aesthetics (the code inside aes()).\ndata: This is where you specify the dataset containing the variables that the geom is using.\n...: You will tend to use this for fixed aesthetics (ones that don’t correspond to a variable). For example, this is where you can set the color of all points (e.g., with color = \"red\") or the size of all points (e.g., with size = 3).\n\nA note about the data argument: Previously you have used one dataset per plot by specifying that as the first argument of ggplot(). For example, the code below makes a scatterplot of price vs. carat in the diamonds dataset, and the only data argument is in ggplot() (none in geom_point()).\n\ndata(diamonds)\nhead(diamonds)\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\nggplot(diamonds, aes(x = carat, y = price)) +\n    geom_point() +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n# Note that we can make the same plot by specifying the argument names explicitly:\nggplot(data = diamonds, mapping = aes(x = carat, y = price)) +\n    geom_point() +\n    theme_classic()\n\nSometimes we may want to use more than one dataset in a plot. For example, we have a separate dataset that contains average prices of diamonds by carat:\n\nhead(diamonds_avg_price)\n\n# A tibble: 6 × 2\n  carat avg_price\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  0.2       365.\n2  0.23      486.\n3  0.26      551.\n4  0.29      601.\n5  0.32      720.\n6  0.35      801.\n\n\nWe can use this separate diamonds_avg_price dataset in the geom_point() layer to add average price information to our scatterplot:\n\nggplot(diamonds, aes(x = carat, y = price)) +\n    geom_point() +\n    geom_point(data = diamonds_avg_price, aes(x = carat, y = avg_price), color = \"deepskyblue\", size = 3)\n\n\n\n\n\n\n\n\nThe Aesthetics section of a geom documentation page gives information on how the visual elements of the geom correspond to data. For example, the geom_point documentation page shows that the familiar x and y aesthetics are available. It also shows some new aesthetics like stroke.\nWe can use the same process to look at the geom_linerange documentation page and start off our temperature visualization with the record lows and highs:\n\nggplot(weather) +\n    geom_linerange(aes(x = dateInYear, ymin = RecordLow, ymax = RecordHigh), color = \"#ECEBE3\", linewidth = 1.5) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeyboard shortcuts\n\n\n\nAs you work on this plot, try to use some new keyboard shortcuts. Focus on the following:\n\nInsert code chunk: Ctrl+Alt+I (Windows). Option+Command+I (Mac).\nRun current code chunk: Ctrl+Shift+Enter (Windows). Command+Shift+Return (Mac).\nRun current line/currently selected lines: Ctrl+Enter (Windows). Command+Return (Mac).\n\n\n\nClass exercise 3: Add to your temperature visualization to also display the usual temperatures (NormalLow and NormalHigh), and actual 2011 temperatures (Low and High). Your plot should look like the one below. The tan color for the usual temperatures is \"#C8B8BA\", and the red color for the actual temperatures is \"#A90248\".\n\n\n\n\n\n\n\n\n\n\nLet’s now try to recreate the visual demarcations of the months by adding vertical lines separating the months.\nClass exercise 4: Brainstorm with your groups how we might draw those vertical lines. What geom might we use? What subset of the data might we use in that geom layer to draw lines only at the month divisions? One person from your group should write your ideas on the board.\nOnce ideas are up on the board, we’ll work through this together as a class.\n\nNow let’s change the x-axis labels so that the month names display in the center of each month’s slice of the plot. (Note that R has built-in variables called month.abb and month.name that contain abbreviated and full month names.)\nClass exercise 5: We will explore two different approaches to figuring out this new challenge: Google search and AI.\n\nGoogle: Start by just using Google search queries. Collaborate with your group to try to word your search queries as carefully as possible (using the jargon that is most likely to return the most relevant results). Record search queries and your thought process in selecting which search results to look at first.\nAI: Next use ChatGPT. Collaborate with your group to brainstorm a series of prompts that will most efficiently get you the desired results. Record the chat prompts used and output given. Evaluate the output. Do you fully understand the code generated? How can you tell that the generated code is correct?\n\nAfter we debrief on these approaches, we’ll finalize this part of the plot together."
  },
  {
    "objectID": "02-adv-ggplot.html#group-work",
    "href": "02-adv-ggplot.html#group-work",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "Group work",
    "text": "Group work\nWork together until your precipitation plot looks as below.\n\nThe triangles point to precipitation records. Refer to the Codebook above for the RecordP variable.\nThe numbers on the plot indicate the total precipitation for the month. Do some searching about the hjust and vjust options to adjust the alignment of the numbers.\nThe blue and tan colors are \"#32a3d8\" and \"#ebeae2\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReminder: Record and observe\n\n\n\nAs you work through this new phase of the plot, makes notes in your personal class journal about anything that you tried that didn’t work the way you wanted: geoms that you tried but didn’t make the right plot, faulty aesthetic mappings, error messages, and warning messages.\nAlso be aware of your comforts and discomforts in this collaborative environment. Pay attention to the comforts and discomforts of your groupmates.\nWe’ll have a few minutes at the end of class to reflect holistically on today’s activity.\n\n\n\nIf you have time, keep working with each other on your plots to get them to look as below (which shows your goal for Challenge 1).\n\n\n\nSFO weather in 2011 (minimum requirements for Challenge 1 submission)"
  },
  {
    "objectID": "02-adv-ggplot.html#reflect",
    "href": "02-adv-ggplot.html#reflect",
    "title": "Advanced Data Visualization in ggplot2",
    "section": "Reflect",
    "text": "Reflect\nIn terms of both coding and collaboration, what challenges did you face today? What did you do to address those challenges? What would you like to try for next time?\nTake a few minutes to write some thoughts in your personal course journal."
  },
  {
    "objectID": "03-file-org-github.html",
    "href": "03-file-org-github.html",
    "title": "File organization, GitHub",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nSet up an organized directory structure for data science projects\nExplain the difference between absolute and relative file paths and why relative file paths are preferred for reading in data\nConstruct relative file paths to read in data\nUse the following git commands within RStudio (whether via the GUI or by the Terminal command line): clone, add, commit, push"
  },
  {
    "objectID": "03-file-org-github.html#folderdirectory-structure",
    "href": "03-file-org-github.html#folderdirectory-structure",
    "title": "File organization, GitHub",
    "section": "Folder/directory structure",
    "text": "Folder/directory structure\nMinimal directory structure for a data science project: (Sub-bullets indicate folders that are inside other folders.) At minimum, a data science project should have a code, data, and results folder. Not having these folders and mixing code, data, and results files all in one folder can quickly get hard to navigate for even small projects.\n\nDocuments (This should be some place you can find easily through your Finder (Mac) or File Explorer (Windows).)\n\ndescriptive_project_name\n\ncode: All code files (.R, .Rmd, .qmd) should go here. Recommendation:\n\ncleaning.qmd: for data acquisition and wrangling. Save (write) the cleaned dataset at the end of this file with readr::write_csv().\nvisualizations.qmd: for exploratory and final plots\nmodeling.qmd: for any statistical or predictive modeling\n\ndata: All data files go here\nresults: e.g., plots saved as images, results tables\n\n\n\n\nMore involved directory structure for a data science project: If you have a larger-scale project that involves a lot more code and data, the expanded directory structure below is useful:\n\nDocuments\n\ndescriptive_project_name\n\ncode\n\nraw: For messy code that you’re actively working on\nclean: For code that you have cleaned up, documented, organized, and tested to run as expected\n\ndata\n\nraw: Original data that hasn’t been cleaned\nclean: Any non-original data that has been processed in some way\n\nresults\n\nfigures: Plots that will be used in communicating your project should go here. (Using screenshots of output in RStudio is not a good practice.)\ntables: Any sort of plain text file results (e.g., CSVs)"
  },
  {
    "objectID": "03-file-org-github.html#file-paths",
    "href": "03-file-org-github.html#file-paths",
    "title": "File organization, GitHub",
    "section": "File paths",
    "text": "File paths\n\n\n\n\n\n\nWhat are file paths?\n\n\n\nA file path is a text string that tells a computer how to navigate from one location to another. We use file paths to read in (and write out) data.\nEssentially, file paths are what go inside read_csv().\n\n\nThere are two types of paths: absolute and relative.\nAbsolute file paths start at the “root” directory in a computer system. Examples:\n\nMac: /Users/lesliemyint/Desktop/teaching/STAT212/2024_spring/activities/adv_ggplot/sfo_weather.csv\n\nOn a Mac the tilde ~ in a file path refers to the “Home” directory, which is /Users/lesliemyint. In this case, the path becomes ~/Desktop/teaching/STAT212/2024_spring/activities/adv_ggplot/sfo_weather.csv\n\nWindows: C:/Users/lesliemyint/Documents/teaching/STAT212/2024_spring/activities/adv_ggplot/sfo_weather.csv\n\nNote: Windows uses both / (forward slash) and \\ (backward slash) to separate folders in a file path.\n\n\n\n\n\n\n\n\nDON’T use absolute paths\n\n\n\nFor reading in data, absolute paths are not a good idea because if the code file is shared. The path will not work on a different computer.\n\n\n\nRelative file paths start wherever you are right now (the working directory (WD)). The WD when you’re working in a code file (.Rmd, .qmd) may be different from the working directory in the Console.\nDirectory setup 1:\n\nsome_folder\n\nyour_code_file.qmd\ndata.csv\n\n\nTo read in the data:\n\ndata &lt;- read_csv(\"data.csv\")\n\nDirectory setup 2:\n\nsome_folder\n\nyour_code_file.qmd\ndata\n\ndata.csv\n\n\n\nTo read in the data:\n\ndata &lt;- read_csv(\"data/data.csv\")\n\nDirectory setup 3:\n\nsome_folder\n\ndata.csv\ncode\n\nyour_code_file.qmd\n\n\n\nTo go “up” a folder in a relative path we use ../. To read in the data:\n\ndata &lt;- read_csv(\"../data.csv\")\n\nDirectory setup 4:\n\nsome_folder\n\ndata\n\ndata.csv\n\ncode\n\nyour_code_file.qmd\n\n\n\nTo read in the data:\n\ndata &lt;- read_csv(\"../data/data.csv\")\n\n\n\n\n\n\n\nDO use relative paths\n\n\n\nFor reading in data, relative paths are preferred because if the project directory structure is used on a different computer, the relative paths will still work."
  },
  {
    "objectID": "03-file-org-github.html#exercise",
    "href": "03-file-org-github.html#exercise",
    "title": "File organization, GitHub",
    "section": "Exercise",
    "text": "Exercise\nDownload this Zip file from Moodle, and save it to your class folder. After unzipping, open the code/clean/cleaning.qmd file in RStudio. Follow the instructions in that file.\nExercise goals:\n\nPractice using relative paths in a realistic project context\nPractice data wrangling\n\n\n\nSolution\n\nLoad packages and read in data.\n\nlibrary(tidyverse)\nweather &lt;- read_csv(\"../../data/raw/weather.csv\")\n\nAdd dateInYear variable.\n\nweather_clean &lt;- weather %&gt;% \n    arrange(Month, Day) %&gt;% \n    mutate(dateInYear = 1:365)\n\nAdd in 3-letter month abbreviations.\n\n# Option 1: via joins\nmonths &lt;- tibble(\n    Month = 1:12,\n    month_name = month.abb\n)\nweather_clean &lt;- weather_clean %&gt;% \n    left_join(months)\n\n# Option 2: via vector subsetting\nweather %&gt;% \n    mutate(month_name = month.abb[Month])\n\nWrite out clean data to a CSV file.\n\nwrite_csv(weather_clean, file = \"../../data/clean/weather_clean.csv\")"
  },
  {
    "objectID": "04-adv-maps.html",
    "href": "04-adv-maps.html",
    "title": "Advanced spatial visualization",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nUnderstand the basics of a CRS (coordinate reference system)\nUnderstand and recognize different spatial file types and data types in R\nImplement some of the basic plotting with the sf package\nUnderstand foundational ideas in working with spatial data (aggregating spatial point data to a spatial region, joining spatial data sets)"
  },
  {
    "objectID": "04-adv-maps.html#ellipsoid",
    "href": "04-adv-maps.html#ellipsoid",
    "title": "Advanced spatial visualization",
    "section": "Ellipsoid",
    "text": "Ellipsoid\n\nThe Earth is not a sphere.\n\nIt’s closer to a bumpy ellipsoid with a bulge at the equator.\n\nThe ellipsoid part of a CRS is a mathematical model giving a smooth approximation to Earth’s shape.\nCommon ellipsoid models: WGS84 and GRS80\n\n\n\n\nIllustration of ellipsoid model (black) and Earth’s irregular surface (red), centered to have an overall best fit. Source: www.icsm.gov.au"
  },
  {
    "objectID": "04-adv-maps.html#datum",
    "href": "04-adv-maps.html#datum",
    "title": "Advanced spatial visualization",
    "section": "Datum",
    "text": "Datum\n\nWhere do we center the ellipsoid? This center is called a datum.\nFor a given ellipsoid model, different datums are used to better fit different areas of the world.\n\ne.g., For the GRS80 ellipsoid, the NAD83 datum is a good fit in North America, but SIRGAS2000 is a better fit in South America.\nThe Global Positioning System (GPS) uses the WGS84 ellipsoid model and WGS84 datum. This provides an overall best fit of the Earth.\n\n\n\n\n\nIllustration of ellipsoid model and Earth’s irregular surface for a datum that better fits southern part (bottom right) of the Earth. Source: www.icsm.gov.au\n\n\n\n\n\n\n\n\nWhy do the ellipsoid and datum matter?\n\n\n\nIf you have longitude and latitude coordinates for a location, you need to know what datum and ellipsoid were used to define those positions in order to overlay those points correctly on a map.\nNote: In practice, the horizontal distance between WGS84 and NAD83 coordinates is about 3-4 feet in the US, which may not be significant for most applications."
  },
  {
    "objectID": "04-adv-maps.html#projection",
    "href": "04-adv-maps.html#projection",
    "title": "Advanced spatial visualization",
    "section": "Projection",
    "text": "Projection\nLastly, the Earth lives in a 3 dimensional (3D) world and most visualizations are on a 2 dimensional (2D) surface. We must choose a projection method to represent points, regions, and lines on Earth on a 2D map with distance units (typically meter, international foot, US survey foot). In that projection process, a 3D element will lose angle, area, and/or distance when projected onto a 2D surface, no matter which method is chosen.\n\nFor a good overview of common projection methods, see https://pubs.usgs.gov/gip/70047422/report.pdf.\nCommon projection: Mercator projection\n\nCylindrical map projection from the 1500’s\nUseful for navigation because it represented north as up and south as down everywhere and preserves local directions and shape\nDrawback: it inflates the size of regions far from the equator. Greenland, Antarctica, Canada, and Russia appear much bigger than they should. The illustration below compares country areas/shapes under the Mercator projection (light blue) with true areas/shapes (dark blue).\n\n\n\n\n\nSource: @neilrkaye\n\n\nBelow you can see four different world projections. Take note of what is lost in terms of angle, area, or distance in these projections.\n\nworld &lt;- rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Basic Map w/ labels\nggplot(data = world) + \n    geom_sf(color = \"black\", fill = \"#bada55\") +\n    labs(x = \"Longitude\", y = \"Latitude\", title = \"World Map - Mercator Projection\", subtitle = paste0(\"(\", length(unique(world$name)), \" countries)\")) +\n    theme_bw() \n\n\n\nggplot(data = world) +\n    geom_sf(color = \"black\", fill = \"#bada55\") +\n    coord_sf(crs = \"+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs\") + \n    labs(title = \"Lambert Azimuthal Equal-Area Projection\", subtitle = \"Correctly represents area but not angles\") + \n    theme_bw()\n\n\n\nggplot(data = world) +\n    geom_sf(color = \"black\", fill = \"#bada55\") +\n    coord_sf(crs = \"+proj=fouc\") + \n    labs(title = \"Foucaut Projection\", subtitle = \"Correctly represents area, lots of shape distortion in high latitudes\") + \n    theme_bw() \n\n\n\nggplot(data = world) +\n    geom_sf(color = \"black\", fill = \"#bada55\") +\n    coord_sf(crs = \"+proj=natearth2\") + \n    labs(title = \"Natural Earth II Projection\", subtitle = \"Represents globe shape, distorted at high latitudes\") + \n    theme_bw()"
  },
  {
    "objectID": "04-adv-maps.html#vector",
    "href": "04-adv-maps.html#vector",
    "title": "Advanced spatial visualization",
    "section": "Vector",
    "text": "Vector\nVector data represents the world as a set of spatial geometries that are defined in terms of location coordinates (with a specified CRS) with non-spatial attributes or properties.\nThe three basic vector geometries are:\n\nPoints: Locations defined based on a (x, y) coordinates.\n\ne.g., Cities\n\nLines: A set of ordered points connected by straight lines.\n\ne.g., Roads, rivers\n\nPolygons: A set of ordered points connected by straight lines, first and last point are the same.\n\ne.g., Geopolitical boundaries, bodies of water\n\n\nFile formats:\n\nText files (e.g., .csv)\n\nx and y columns: for coordinates (x for longitude and y for latitude)\ngroup id column: needed for lines and polygons\nadditional columns: attributes related to the areas in each row (e.g., population sizes, demographic information)\nText files do not store the CRS.\n\nShapefiles (.shp)\n\nWidely supported spatial vector file format (that includes the CRS).\n\nGeoJSON (.geojson) (Geographical Javascript Object Notation)\nKML (.kml) (Keyhole Markup Language)"
  },
  {
    "objectID": "04-adv-maps.html#raster",
    "href": "04-adv-maps.html#raster",
    "title": "Advanced spatial visualization",
    "section": "Raster",
    "text": "Raster\n\n\n\nDifference between vector and raster formats. Source: gis.stackexchange.com\n\n\n\nContinuous grid of cells where each cell has a single value.\n\nContinuous values (e.g., elevation, precipitation)\nCategorical values (e.g., land cover type, soil type)\n\nShape of cells\n\nGenerally square (like pixels)\nCells can be rotated and sheared. Rectilinear and curvilinear shapes are also possible, depending on the spatial region of interest and CRS.\n\n\n\n\n\nRaster cell shapes (Source)\n\n\n\n\n\n\n\n\nComputational time considerations\n\n\n\nHigh resolution raster data involves a large number of small cells. This results in large file sizes and objects which can make computation and visualization quite slow.\n\n\nFile formats:\n\nGeoTIFF (.tif or .tiff)\n\nMost popular\n\nNetCDF (.nc)\nHDF (.hdf)\n\nTo work with raster data in R, you’ll use the raster, terra, and the stars packages. If you are interested in learning more, check out https://r-spatial.github.io/stars/."
  },
  {
    "objectID": "04-adv-maps.html#get-county-boundaries",
    "href": "04-adv-maps.html#get-county-boundaries",
    "title": "Advanced spatial visualization",
    "section": "Get county boundaries",
    "text": "Get county boundaries\nWe’ve already read in city location and water information from external shapefiles. We can access county boundaries with the us_counties() function in the USAboundaries package.\n\n# Load country boundaries data as sf object\nmn_counties &lt;- USAboundaries::us_counties(resolution = \"high\", states = \"Minnesota\")\n\n# Take care of duplicate column names (there are two identical \"state_name\" columns)\nnames_counties &lt;- names(mn_counties)\nnames(mn_counties)[names_counties == \"state_name\"] &lt;- c(\"state_name1\", \"state_name2\")"
  },
  {
    "objectID": "04-adv-maps.html#unifying-crss-across-different-spatial-datasets",
    "href": "04-adv-maps.html#unifying-crss-across-different-spatial-datasets",
    "title": "Advanced spatial visualization",
    "section": "Unifying CRSs across different spatial datasets",
    "text": "Unifying CRSs across different spatial datasets\nWe first need to ensure that the CRS is the same for all spatial datasets.\nExercise:\n\nCheck the CRS for the mn_cities, mn_water, and mn_counties datasets.\nIf the datasets don’t all have the same CRS, use st_transform() to update the datasets to have the same CRS as mn_cities. You can use crs = st_crs(mn_cities) within st_transform().\n\n\n\nSolution\n\n\n# Check CRSs\nst_crs(mn_cities)\nst_crs(mn_water)\nst_crs(mn_counties) # mn_counties is different!\n\n# Transform the CRS of county data to the more local CRS of the cities\nmn_counties &lt;- mn_counties %&gt;%\n    st_transform(crs = st_crs(mn_cities))\n\n# Check the new CRS for mn_counties\nst_crs(mn_counties)"
  },
  {
    "objectID": "04-adv-maps.html#initial-map-counties-and-cities",
    "href": "04-adv-maps.html#initial-map-counties-and-cities",
    "title": "Advanced spatial visualization",
    "section": "Initial map: counties and cities",
    "text": "Initial map: counties and cities\nExercise: Create a map where city locations are overlaid on a map of county boundaries.\n\nYou will need to call geom_sf() twice.\nMake the map background white.\nInstall the ggthemes package, and add the following layer to use a clean map theme: + ggthemes::theme_map()\n\n\n\nSolution\n\n\n# Option 1\nggplot() + # plot frame\n    geom_sf(data = mn_counties, fill = \"white\") + # county boundary layer\n    geom_sf(data = mn_cities, size = 0.5) + # city point layer\n    ggthemes::theme_map()\n\n# Option 2\nggplot(mn_counties) + # plot frame\n    geom_sf(fill = \"white\") + # county boundary layer\n    geom_sf(data = mn_cities, size = 0.5) + # city point layer\n    ggthemes::theme_map()\n\n\nWe can use traditional ggplot2 aesthetics (e.g., fill, color) to display location specific attributes. Below we only plot large cities, and we color and size cities according to their population.\n\nggplot() +\n    geom_sf(data = mn_counties, fill = \"white\") + \n    geom_sf(data = mn_cities %&gt;% filter(Population &gt;= 10000), mapping = aes(color = Population, size = Population)) + # cities layer\n    scale_color_viridis_c() + # continuous (gradient) color scale\n    labs(title = \"Minnesota Cities with Population &gt;= 10,000\") + \n    ggthemes::theme_map() +\n    theme(legend.position = \"bottom\") # move legend\n\nExercise: Look up the scale_color_viridis_c() documentation via the ggplot2 reference.\n\nRead the function description at the top. What is the advantage of using this function for making color palettes?\nLook through the examples section. What is the difference between the _d(), _c(), and _b() variants of this function?\n\n\n\nSolution\n\nThe viridis color scale results in plots that can be interpreted analogously whether in color or black and white and is color-blind friendly.\n\nThe _d() variant is used when color is mapped to a discrete (categorical) variable.\nThe _c() variant is used when color is mapped to a continuous variable.\nThe _b() variant is used when color is mapped to a continuous variable but when we want that continuous variable to be binned so that there is a small set of colors."
  },
  {
    "objectID": "04-adv-maps.html#adding-elevation-data",
    "href": "04-adv-maps.html#adding-elevation-data",
    "title": "Advanced spatial visualization",
    "section": "Adding elevation data",
    "text": "Adding elevation data\nWhere are large cities located? Is there some relationship to local geography/terrain? To investigate these questions, we can obtain elevation data to include on the map using the elevatr package. We encounter two new functions here—we can look up their documentation to make sense of the code by entering the following in the Console:\n\n?elevatr::get_elev_raster\n?terra::as.data.frame\n\n\nelevation &lt;- elevatr::get_elev_raster(mn_counties, z = 5, clip = \"bbox\")\nraster::crs(elevation) &lt;- sf::st_crs(mn_counties)\n\n# Convert to data frame for plotting\nelev_df &lt;- elevation %&gt;% terra::as.data.frame(xy = TRUE)\ncolnames(elev_df) &lt;- c(\"x\", \"y\", \"elevation\")\n\nExercise: Build on our existing map by adding a raster layer for elevation as the background.\n\nLook up the documentation for geom_raster() to plot the elevation data from elev_df. This will be the first layer of the plot.\nLook at the documentation for scale_fill_gradient() to add the following elevation color scale: \"darkgreen\" represents the lowest elevations, and \"white\" represents the highest elevations.\nAdd in the layers from the map above to show the largest cities and the county outlines. To remove a background color, use fill = NA.\n\n\n\nSolution\n\n\nggplot() +\n    geom_raster(data = elev_df, aes(x = x, y = y, fill = elevation)) + # adding the elevation as first (bottom) layer\n    scale_fill_gradient(low = \"darkgreen\", high = \"white\", guide = FALSE) +\n    geom_sf(data = mn_counties, fill = NA, color = \"black\") + \n    geom_sf(data = mn_cities %&gt;% filter(Population &gt;= 10000), mapping = aes(color = Population, size = Population))+ # cities layer\n    scale_color_viridis_c() + # continuous (gradient) color scale\n    labs(title = \"Minnesota Cities with Population &gt;= 10,000\") + \n    ggthemes::theme_map() +\n    theme(legend.position = \"bottom\") # move legend"
  },
  {
    "objectID": "04-adv-maps.html#zoom-in-to-twin-cities-and-add-water",
    "href": "04-adv-maps.html#zoom-in-to-twin-cities-and-add-water",
    "title": "Advanced spatial visualization",
    "section": "Zoom in to Twin Cities and add water",
    "text": "Zoom in to Twin Cities and add water\nThe bulk of the interesting information in this map is in the Twin Cities area. Let’s zoom in to this area.\n\nWe can use the st_bbox() function to get the bounding box for a spatial object—we do this after filtering to the 7 counties in the Twin Cities.\nWe then use st_crop() to trim a spatial object to a given bounding box.\n\n\nseven_countyarea &lt;- mn_counties %&gt;%\n    filter(name %in% c(\"Anoka\", \"Hennepin\", \"Ramsey\", \"Dakota\", \"Carver\", \"Washington\", \"Scott\")) %&gt;% \n    st_bbox()\nseven_countyarea\n\nelevation &lt;- elevatr::get_elev_raster(mn_counties %&gt;% st_crop(seven_countyarea), z = 9, clip = \"bbox\")\nraster::crs(elevation) &lt;- sf::st_crs(mn_counties)\n\n# Convert to data frame for plotting\nelev_df &lt;- elevation %&gt;% terra::as.data.frame(xy = TRUE)\ncolnames(elev_df) &lt;- c(\"x\", \"y\", \"elevation\")\n\nIn the plot below, we add a layer for water information and a coord_sf() layer to restrict the x and y-axis limits to the Twin Cities bounding box. (Without this layer, the map would zoom back out to show all counties and bodies of water).\n\nggplot() +\n    geom_raster(data = elev_df, aes(x = x, y = y, fill = elevation)) + \n    geom_sf(data = mn_counties, fill = NA, color = \"black\") + # county boundary layer\n    geom_sf(data = mn_water, fill = \"lightsteelblue1\", color = \"lightsteelblue1\") + # NEW: river/lake layer\n    geom_sf(data = mn_cities %&gt;% filter(Population &gt;= 10000), mapping = aes(color = Population, size = Population)) + # cities layer\n    scale_color_viridis_c(option = \"magma\") + # continuous (gradient) color scale\n    scale_fill_gradient(low = \"darkgreen\", high = \"white\") + # continuous (gradient) fill scale\n    coord_sf(xlim = seven_countyarea[c(\"xmin\", \"xmax\")], ylim = seven_countyarea[c(\"ymin\", \"ymax\")]) + # NEW: crop map to Twin Cities bounding box\n    labs(title = \"Twin Cities with Population &gt;= 10,000\") + \n    ggthemes::theme_map() +\n    theme(legend.position = \"none\") # remove legend\n\nExercise: Let’s add to the above code chunk to save the map above to an image file called tc_map_zoom.png in the results folder. The code example below shows a general template for saving a plot to file. Choose a reasonable width and height. (There are also jpeg() and pdf() functions for writing images.)\n\npng(\"relative path to image\", width = width_in_pixels, height = height_in_pixels)\n# Code for creating plot\ndev.off()"
  },
  {
    "objectID": "04-adv-maps.html#twin-cities-map-with-leaflet",
    "href": "04-adv-maps.html#twin-cities-map-with-leaflet",
    "title": "Advanced spatial visualization",
    "section": "Twin Cities map with leaflet",
    "text": "Twin Cities map with leaflet\nBelow we show how to make the MN counties map in the leaflet package.\n\nlibrary(leaflet)\n\nmn_counties_leaf &lt;- mn_counties %&gt;% st_transform(4326) # Leaflet expects this CRS for vectors\nmn_cities_leaf &lt;- mn_cities %&gt;% st_transform(4326)\n\ncities_per_county &lt;- st_join(mn_cities_leaf, mn_counties_leaf) %&gt;%\n    st_drop_geometry() %&gt;% # removes geometry - makes the following calculation more efficient\n    count(name) \n\nmn_counties_leaf %&gt;% \n    filter(name %in% c(\"Anoka\", \"Hennepin\", \"Ramsey\", \"Dakota\", \"Carver\", \"Washington\", \"Scott\")) %&gt;%\n    left_join(cities_per_county) %&gt;%\n    leaflet() %&gt;% \n    addProviderTiles(\"CartoDB.Positron\") %&gt;% \n    addPolygons(\n        color = \"#444444\", weight = 1, smoothFactor = 0.5, opacity = 1.0,\n        fillOpacity = 0.5, fillColor = ~colorQuantile(\"YlOrRd\", n)(n),\n        highlightOptions = highlightOptions(color = \"white\", weight = 2, bringToFront = TRUE)) %&gt;%\n    addCircles(data = mn_cities_leaf %&gt;% filter(County %in% paste(c(\"Anoka\", \"Hennepin\", \"Ramsey\", \"Dakota\", \"Carver\", \"Washington\", \"Scott\"), \"County\")), color = \"#444444\")"
  },
  {
    "objectID": "05-interactive-viz.html",
    "href": "05-interactive-viz.html",
    "title": "Interactive visualization",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nEvaluate when it would be useful to use an interactive visualization or an animation and when it might not be necessary\nConstruct interactive visualizations and animations with plotly\nBuild a Shiny app that enables user to adjust visualization choices and explore linked visualizations"
  },
  {
    "objectID": "05-interactive-viz.html#pros-and-cons-of-interactivity",
    "href": "05-interactive-viz.html#pros-and-cons-of-interactivity",
    "title": "Interactive visualization",
    "section": "Pros and cons of interactivity",
    "text": "Pros and cons of interactivity\nPros\n\nUsers can click, hover, zoom, and pan to get more detailed information\nUsers can get quickly and deeply explore the data via linked data representations\nAllows guided exploration of results without needing to share data\n\nCons\n\nTakes longer to design\nAnalyst might spend longer exploring an interactive visualization than a series of static visualizations\nPoor design could result in information overload"
  },
  {
    "objectID": "05-interactive-viz.html#common-features-of-interactive-visualizations",
    "href": "05-interactive-viz.html#common-features-of-interactive-visualizations",
    "title": "Interactive visualization",
    "section": "Common features of interactive visualizations",
    "text": "Common features of interactive visualizations\nCommon features of interactive visualizations include (reference):\n\nChanging data representation: providing options to change the type of plot displayed (e.g., allowing users to visualize temperature patterns over a month vs. over years)\nFocusing and getting details: mousing over part of a visualization to see an exact data value, zooming and panning\nData transformation: e.g., changing color scale, switching to/from log scale\nData selection and filtering: highlighting and brushing regions of a plot to focus the selected points; reordering and filtering data show in tables\nFinding corresponding information in multiple views: linked views that update dynamically based on interaction in another plot (often by zooming, panning, or selecting certain points)"
  },
  {
    "objectID": "05-interactive-viz.html#app-exploration",
    "href": "05-interactive-viz.html#app-exploration",
    "title": "Interactive visualization",
    "section": "App exploration",
    "text": "App exploration\nONE person in each group should open the neighborhood diversity app and navigate around the app for the group. (We don’t want to overload the author’s Shiny server.)\n\nCatalog the app’s layout and interactivity features. Make note of where user inputs are located and how different parts of the app respond to user input.\nEvaluate the design of this app.\n\nIs the interactivity in this app needed? Does the interactivity actually help you gain more insight (and perhaps more quickly) than a series of static visualizations?\nWhat explorations/comparisons are you curious to explore that are not enabled well by the app?\n\n\n\nIn case the app doesn’t load, use the following screenshots:"
  },
  {
    "objectID": "05-interactive-viz.html#exercise-1-setup-and-getting-acquainted",
    "href": "05-interactive-viz.html#exercise-1-setup-and-getting-acquainted",
    "title": "Interactive visualization",
    "section": "Exercise 1: Setup and getting acquainted",
    "text": "Exercise 1: Setup and getting acquainted\nSetup part 1: Load required packages at the top of app.R: shiny, tidyverse, sf, and plotly.\nSetup part 2: Data download and folder setup\nNavigate to the “Data for interactive viz activity” folder on Moodle and save the two files with the folder setup below:\n\n📂 YOUR_CLASS_FOLDER\n\n📂 interactive_viz\n\n📂 neighborhood_diversity\n\napp.R\n📂 data\n\ndata_by_dist.rds\ndata_by_year.csv\n\n\n\n\n\nSetup part 3: Below your library() calls, add the following commands to read in the data:\n\ndata_by_dist &lt;- read_rds(\"Enter the correct relative path to data_by_dist.rds\")\ndata_by_year &lt;- read_csv(\"Enter the correct relative path to data_by_year.csv\")\n\nGetting acquainted with the app and underlying code: Take a few minutes to explore the code:\n\nIn the ui section: how are functions nested inside each other, and how does this seem to relate to the visual appearance of the app?\nWhat names/labels in the User Interface (ui) part of the app seem to be shared with the server part of the app?"
  },
  {
    "objectID": "05-interactive-viz.html#input-functions",
    "href": "05-interactive-viz.html#input-functions",
    "title": "Interactive visualization",
    "section": "*Input() functions",
    "text": "*Input() functions\n\nBackground\nWhat do these do? The *Input() functions collect inputs from the user.\nWhere are these functions on the cheatsheet? Right-hand side of the first page\nWhere do these go in the app?\n\nAll *Input() functions go in the ui part of the app.\nPay careful attention to the nesting of the functions in the ui section. For example, in a sidebarLayout(), these *Input() functions should go in the sidebarPanel() (as opposed to the mainPanel()).\nSeparate multiple *Input() functions with commas.\n\nHow do the function arguments work? In all the *Input() functions, the first two arguments are the same:\n\ninputId is how you will refer to this input in the server portion later. You can call this anything you want, but make this ID describe the information that the user is providing.\nlabel is how this will actually be labeled in your UI (what text shows up in the app).\n\nEach function has some additional arguments depending what you want to do.\n\n\nExercise 2: Add *Input()s\nAdd the following two user inputs to your app:\n\nDropdown to select the city name\nSlider to choose the span parameter for the scatterplot smooth\n\nThis parameter varies from 0 to 1. Lower values result in a wiggly smoothing line, and higher values result in a smoother line.\n\n\nUse the Shiny cheatsheet to find the *Input() functions that correspond to the two inputs above. Add them to the appropriate place within the ui object. Use commas to separate the inputs.\n\n\n\n\n\n\nParentheses Pandemonium\n\n\n\nCarefully formatting your code will be crucial here! With shiny UIs, it is very easy to lose or mismatch parentheses, which leads to frustrating errors. My suggestion is to place parentheses as follow:\nsliderInput(\n    argument1 = value1,\n    argument2 = value2,\n    argument3 = value3\n)\nNote how the left parenthesis is on the same line as the function, and the right parenthesis is on its own line and left-aligned with the start of the function name.\nHelpful tip: In RStudio, you can place your cursor next to any parenthesis to highlight the matching parenthesis (if there is one).\n\n\nYou will have to look at the documentation for the *Input() functions to know how to use arguments beyond inputId and label. To view this documentation, type ?function_name in the Console.\nTo get the collection of city names from the data_by_dist dataset, you can use the following:\nmetro_names &lt;- data_by_dist %&gt;% pull(metro_name) %&gt;% unique()\nPut this metro_names code just beneath where you read in the data.\nOnce you finish, run your app. Make sure you can select and move things around as expected. You won’t see any plots yet—we’ll work on those in the next exercises."
  },
  {
    "objectID": "05-interactive-viz.html#output-functions",
    "href": "05-interactive-viz.html#output-functions",
    "title": "Interactive visualization",
    "section": "*Output() functions",
    "text": "*Output() functions\n\nBackground\nWhat do these do? *Output() functions in the ui portion work with the render*() functions in the server portion to to add R output (like plots and tables) to the UI.\nWhere are these functions on the cheatsheet? Bottom-center of the first page\nWhere do these go in the app?\n\nAll *Output() functions go in the ui part of the app.\nPay careful attention to the nesting of the functions in the ui section. For example, in a sidebarLayout(), these *Output() functions should go in the mainPanel() (as opposed to the sidebarPanel()).\nSeparate multiple *Output() functions with commas.\n\nHow do the function arguments work? In all the *Output() functions, the first argument is the same:\n\noutputId works just like inputId for *Input() functions. This is how you will refer to this output in the server portion later. You can call this anything you want, but make this ID describe the output being created.\n\n\n\nExercise 3: Add *Output()s\nAdd 3 outputs to the ui that will eventually be:\n\nA scatterplot of diversity score (entropy) versus distance to city hall (distmiles) with a smoothing line (smoothness controlled by the span parameter on your slider input)\nA map of diversity scores across the counties in the selected city\nA bar chart of the overall race distribution in the selected city (i.e., the total number of people in each race category in the city)\n\nFor now, don’t worry that the layout of the plots exactly matches the original neighborhood diversity app. (You will update this in your homework.)\nRun the app with the output. Notice that nothing really changes. Think of the outputs you just placed as placeholders—the app knows there will be a plot in the UI, but the details of what the plots will look like and the R code to create them will be in the server portion. Let’s talk about that now!"
  },
  {
    "objectID": "05-interactive-viz.html#render-functions",
    "href": "05-interactive-viz.html#render-functions",
    "title": "Interactive visualization",
    "section": "render*() functions",
    "text": "render*() functions\n\nBackground\nWhat do these do? The render*() functions use R code (i.e., standard ggplot code) to communicate with (“listen to”) the user inputs to create the desired output.\nWhere are these functions on the cheatsheet? Bottom-center of the first page. The render*() function you use will depend on the desired output. The bottom center of the cheatsheet shows how *Output() and render*() functions connect.\nWhere do these go in the app? The render*() functions go in the server function of the app.\nIn general, the server section of code will look something like this:\n\n# Suppose the following are somewhere in the UI part\nnumericInput(inputId = \"min_year\")\nnumericInput(inputId = \"max_year\")\nplotOutput(outputId = \"plot_over_years\")\n\nserver &lt;- function(input, output) {\n    output$plot_over_years &lt;- renderPlot({ # Note the curly braces that enclose the R code below\n        ggplot(...) +\n            scale_x_continuous(limits = c(input$min_year, input$max_year))\n    })\n}\n\n\n\nExercise 4: Add renderPlot()\nWhile our main goals is to make 3 plots, you will just make one of them in this exercise.\nAdd a renderPlot() functions inside the server portion of the code to make the scatterplot of diversity score (entropy) versus distance to city hall (distmiles) with a smoothing line. Use the data_by_dist dataset. Reference the inputs you’ve already created in previous exercises by using filter() and ggplot() to render the desired interactive plot.\nNote: the geom_??? used to create the smoothing line has a span parameter. (Check out the documentation for that geom by entering ?geom_??? in the Console.)\nRun the app and check that the scatterplot displays and reacts to the chosen city and span parameter."
  },
  {
    "objectID": "05-interactive-viz.html#exercise-5-turn-plots-into-plotlys",
    "href": "05-interactive-viz.html#exercise-5-turn-plots-into-plotlys",
    "title": "Interactive visualization",
    "section": "Exercise 5: Turn plots into plotlys",
    "text": "Exercise 5: Turn plots into plotlys\nIn a web application, having plots be plotly objects is just nice by default because of the great mouseover, zoom, and pan features.\nInside app.R, change plotOutput to plotlyOutput and renderPlot to renderPlotly for the scatterplot and the barplot. Make sure to add calls to ggplotly() too."
  },
  {
    "objectID": "06-wrangling-1.html",
    "href": "06-wrangling-1.html",
    "title": "Data wrangling - Part 1",
    "section": "",
    "text": "After this lesson, you should be able to:\n\nDetermine the class of a given object and identify concerns to be wary of when manipulating an object of that class (numerics, logicals, factors, dates, strings, data.frames)\nExplain what vector recycling is, when it can be a problem, and how to avoid those problems\nUse a variety of functions to wrangle numerical and logical data\nExtract date-time information using the lubridate package\nUse the forcats package to wrangle factor data\n\n\nYou can download a template Quarto file to start from here. Put this file in a folder called wrangling within a folder for this course."
  },
  {
    "objectID": "06-wrangling-1.html#exercises",
    "href": "06-wrangling-1.html#exercises",
    "title": "Data wrangling - Part 1",
    "section": "Exercises",
    "text": "Exercises\nLoad the diamonds dataset, and filter to the first 1000 diamonds.\n\ndata(diamonds)\ndiamonds &lt;- diamonds %&gt;% \n    slice_head(n = 1000)\n\nComplete the following:\n\nSubset to diamonds that are less than 400 dollars or more than 10000 dollars.\nSubset to diamonds that are between 500 and 600 dollars (inclusive).\nHow many diamonds are of either Fair, Premium, or Ideal cut (a total count)? What fraction of diamonds are of Fair, Premium, or Ideal cut (a total count)?\n\nFirst, do this a wrong way with ==. Predict the warning message that you will receive.\nSecond, do this the correct way with an appropriate logical operator.\n\nAre there any diamonds of Fair cut that are more than $3000? Are all diamonds of Ideal cut more than $2000?\nCreate two new categorized versions of price by looking up the documentation for if_else() and case_when():\n\nprice_cat1: “low” if price is less than 500 and “high” otherwise\nprice_cat2: “low” if price is less than 500, “medium” if price is between 500 and 1000 dollars inclusive, and “high” otherwise.\n\n\n\n\nSolution\n\n\n# 1\ndiamonds %&gt;% \n    filter(price &lt; 400 | price &gt; 10000)\n\n# A tibble: 30 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 20 more rows\n\n# 2\ndiamonds %&gt;% \n    filter(price &gt;= 500, price &lt;= 600)\n\n# A tibble: 90 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.35 Ideal     I     VS1      60.9  57     552  4.54  4.59  2.78\n 2  0.3  Premium   D     SI1      62.6  59     552  4.23  4.27  2.66\n 3  0.3  Ideal     D     SI1      62.5  57     552  4.29  4.32  2.69\n 4  0.3  Ideal     D     SI1      62.1  56     552  4.3   4.33  2.68\n 5  0.42 Premium   I     SI2      61.5  59     552  4.78  4.84  2.96\n 6  0.28 Ideal     G     VVS2     61.4  56     553  4.19  4.22  2.58\n 7  0.32 Ideal     I     VVS1     62    55.3   553  4.39  4.42  2.73\n 8  0.31 Very Good G     SI1      63.3  57     553  4.33  4.3   2.73\n 9  0.31 Premium   G     SI1      61.8  58     553  4.35  4.32  2.68\n10  0.24 Premium   E     VVS1     60.7  58     553  4.01  4.03  2.44\n# ℹ 80 more rows\n\n# 3\n## Wrong way with ==\ndiamonds %&gt;% \n    mutate(is_fpi = cut==c(\"Fair\", \"Premium\", \"Ideal\")) %&gt;% \n    summarize(num_fpi = sum(is_fpi), frac_fpi = mean(is_fpi))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `is_fpi = cut == c(\"Fair\", \"Premium\", \"Ideal\")`.\nCaused by warning in `==.default`:\n! longer object length is not a multiple of shorter object length\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n# A tibble: 1 × 2\n  num_fpi frac_fpi\n    &lt;int&gt;    &lt;dbl&gt;\n1     226    0.226\n\n## Right way with %in%\ndiamonds %&gt;% \n    mutate(is_fpi = cut %in% c(\"Fair\", \"Premium\", \"Ideal\")) %&gt;% \n    summarize(num_fpi = sum(is_fpi), frac_fpi = mean(is_fpi))\n\n# A tibble: 1 × 2\n  num_fpi frac_fpi\n    &lt;int&gt;    &lt;dbl&gt;\n1     685    0.685\n\n# 4\ndiamonds %&gt;% \n    filter(cut==\"Fair\") %&gt;% \n    summarize(any_high = any(price &gt; 3000))\n\n# A tibble: 1 × 1\n  any_high\n  &lt;lgl&gt;   \n1 FALSE   \n\ndiamonds %&gt;% \n    filter(cut==\"Ideal\") %&gt;% \n    summarize(all_high = all(price &gt; 2000))\n\n# A tibble: 1 × 1\n  all_high\n  &lt;lgl&gt;   \n1 FALSE   \n\n# 5\ndiamonds %&gt;% \n    mutate(\n        price_cat1 = if_else(price &lt; 500, \"low\", \"high\"),\n        price_cat2 = case_when(\n            price &lt; 500 ~ \"low\",\n            price &gt;= 500 & price &lt;= 1000 ~ \"medium\",\n            price &gt; 1000 ~ \"high\"\n        )\n    )\n\n# A tibble: 1,000 × 12\n   carat cut       color clarity depth table price     x     y     z price_cat1\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43 low       \n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31 low       \n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31 low       \n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63 low       \n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75 low       \n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48 low       \n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47 low       \n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53 low       \n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49 low       \n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39 low       \n# ℹ 990 more rows\n# ℹ 1 more variable: price_cat2 &lt;chr&gt;"
  },
  {
    "objectID": "06-wrangling-1.html#exercises-1",
    "href": "06-wrangling-1.html#exercises-1",
    "title": "Data wrangling - Part 1",
    "section": "Exercises",
    "text": "Exercises\nInstall the nycflights13 package for the data used in this exercise. You can look at the codebook for the flights dataset with ?flights. Each case represents one flight from a NYC airport in 2013.\n\nlibrary(nycflights13)\ndata(flights)\n\nUsing the flights dataset, complete the following:\n\nWhat is the most common departure hour? Use the dep_time variable.\nMake a plot of the distribution of the largest delay for each flight (the larger of dep_delay and arr_delay).\nWhich origin airport had the longest average delay? Should you use dep_delay or arr_delay here? Which had the largest proportion of missing values for this delay variable?\nWhich destination (dest) airport had the largest variability in delays in terms of the difference between the 25th and 75th percentiles? Should you use dep_delay or arr_delay here?\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Use lag() to explore how the average departure delay for an hour is related to the average departure delay for the previous hour.\n\n\n\nSolutions\n\n\n# 1\nflights %&gt;% \n    mutate(dep_hour = dep_time %/% 100)\n\n# 2\nflights %&gt;% \n    mutate(biggest_delay = pmax(dep_delay, arr_delay)) %&gt;% \n    filter(biggest_delay &gt; 0) %&gt;% # Filter out early flights\n    ggplot(aes(x = biggest_delay)) +\n        geom_density()\n\n# 3\nflights %&gt;% \n    group_by(origin) %&gt;% \n    summarize(\n        mean_dep_delay = mean(dep_delay, na.rm = TRUE),\n        med_dep_delay = median(dep_delay, na.rm = TRUE),\n        frac_missing = mean(is.na(dep_delay), na.rm = TRUE)\n    )\n\n# 4\nflights %&gt;% \n    group_by(dest) %&gt;% \n    summarize(\n        iqr_arr_delay = IQR(arr_delay, na.rm = TRUE)\n    ) %&gt;% \n    arrange(desc(iqr_arr_delay))\n\n# 5\nflights %&gt;% \n    mutate(dep_hour = dep_time %/% 100) %&gt;% \n    group_by(year, month, day, dep_hour) %&gt;% \n    summarize(\n        mean_dep_delay = mean(dep_delay, na.rm = TRUE)\n    ) %&gt;% \n    mutate(mean_dep_delay_prev_hour = lag(mean_dep_delay)) %&gt;% \n    ggplot(aes(x = mean_dep_delay_prev_hour, y = mean_dep_delay)) +\n        geom_point()"
  },
  {
    "objectID": "06-wrangling-1.html#exercises-2",
    "href": "06-wrangling-1.html#exercises-2",
    "title": "Data wrangling - Part 1",
    "section": "Exercises",
    "text": "Exercises\nUse the flights dataset to answer the following:\n\nCompare dep_time, sched_dep_time, and dep_delay. Are they consistent?\nOn what day of the week are delays least likely?\n\n\n\nSolutions\n\n\n# 1\nflights_parsed &lt;- flights %&gt;% \n    mutate(\n        dep_time_hour = dep_time %/% 100,\n        dep_time_min = dep_time %% 100,\n        sched_dep_time_hour = sched_dep_time %/% 100,\n        sched_dep_time_min = sched_dep_time %% 100,\n        dep_time = make_datetime(year = year, month = month, day = day, hour = dep_time_hour, min = dep_time_min, sec = 0, tz = \"EST\"),\n        sched_dep_time = make_datetime(year = year, month = month, day = day, hour = sched_dep_time_hour, min = sched_dep_time_min, sec = 0, tz = \"EST\"),\n        dep_delay_computed = dep_time - sched_dep_time\n    )\nflights_parsed %&gt;% \n    select(dep_delay, dep_delay_computed)\n\n# A tibble: 336,776 × 2\n   dep_delay dep_delay_computed\n       &lt;dbl&gt; &lt;drtn&gt;            \n 1         2  120 secs         \n 2         4  240 secs         \n 3         2  120 secs         \n 4        -1  -60 secs         \n 5        -6 -360 secs         \n 6        -4 -240 secs         \n 7        -5 -300 secs         \n 8        -3 -180 secs         \n 9        -3 -180 secs         \n10        -2 -120 secs         \n# ℹ 336,766 more rows\n\nflights_parsed %&gt;% \n    mutate(match = (dep_delay_computed/60)==dep_delay) %&gt;% \n    filter(!match) %&gt;% \n    select(dep_time, sched_dep_time, dep_delay, dep_delay_computed)\n\n# A tibble: 1,207 × 4\n   dep_time            sched_dep_time      dep_delay dep_delay_computed\n   &lt;dttm&gt;              &lt;dttm&gt;                  &lt;dbl&gt; &lt;drtn&gt;            \n 1 2013-01-01 08:48:00 2013-01-01 18:35:00       853 -35220 secs       \n 2 2013-01-02 00:42:00 2013-01-02 23:59:00        43 -83820 secs       \n 3 2013-01-02 01:26:00 2013-01-02 22:50:00       156 -77040 secs       \n 4 2013-01-03 00:32:00 2013-01-03 23:59:00        33 -84420 secs       \n 5 2013-01-03 00:50:00 2013-01-03 21:45:00       185 -75300 secs       \n 6 2013-01-03 02:35:00 2013-01-03 23:59:00       156 -77040 secs       \n 7 2013-01-04 00:25:00 2013-01-04 23:59:00        26 -84840 secs       \n 8 2013-01-04 01:06:00 2013-01-04 22:45:00       141 -77940 secs       \n 9 2013-01-05 00:14:00 2013-01-05 23:59:00        15 -85500 secs       \n10 2013-01-05 00:37:00 2013-01-05 22:30:00       127 -78780 secs       \n# ℹ 1,197 more rows\n\n# 2\nflights %&gt;% \n    mutate(\n        any_delay = dep_delay &gt; 0 | arr_delay &gt; 0,\n        day_of_week = wday(time_hour, label = TRUE)\n    ) %&gt;% \n    group_by(day_of_week) %&gt;% \n    summarize(frac_delays = mean(any_delay, na.rm = TRUE))\n\n# A tibble: 7 × 2\n  day_of_week frac_delays\n  &lt;ord&gt;             &lt;dbl&gt;\n1 Sun               0.492\n2 Mon               0.524\n3 Tue               0.502\n4 Wed               0.512\n5 Thu               0.565\n6 Fri               0.546\n7 Sat               0.453"
  },
  {
    "objectID": "06-wrangling-1.html#creating-factors",
    "href": "06-wrangling-1.html#creating-factors",
    "title": "Data wrangling - Part 1",
    "section": "Creating factors",
    "text": "Creating factors\nIn R, factors are made up of two components: the actual values of the data and the possible levels within the factor. Creating a factor requires supplying both pieces of information.\n\nmonths &lt;- c(\"Mar\", \"Dec\", \"Jan\",  \"Apr\", \"Jul\")\n\nHowever, if we were to sort this vector, R would sort this vector alphabetically.\n\n# alphabetical sort\nsort(months)\n\n[1] \"Apr\" \"Dec\" \"Jan\" \"Jul\" \"Mar\"\n\n\nWe can fix this sorting by creating a factor version of months. The levels argument is a character vector that specifies the unique values that the factor can take. The order of the values in levels defines the sorting of the factor.\n\nmonths_fct &lt;- factor(months, levels = month.abb) # month.abb is a built-in variable\nmonths_fct\n\n[1] Mar Dec Jan Apr Jul\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\nsort(months_fct)\n\n[1] Jan Mar Apr Jul Dec\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nWhat if we try to create a factor with values that aren’t in the levels? (e.g., a typo in a month name)\n\nmonths2 &lt;- c(\"Jna\", \"Mar\")\nfactor(months2, levels = month.abb)\n\n[1] &lt;NA&gt; Mar \nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nBecause the NA is introduced silently (without any error or warnings), this can be dangerous. It might be better to use the fct() function in the forcats package instead:\n\nfct(months2, levels = month.abb)\n\nError in `fct()`:\n! All values of `x` must appear in `levels` or `na`\nℹ Missing level: \"Jna\"\n\n\nExercise: Create a factor version of the following data with the levels in a sensible order.\n\nratings &lt;- c(\"High\", \"Medium\", \"Low\")\n\n\n\nSolution\n\n\nratings_fct &lt;- fct(ratings, levels = c(\"Low\", \"Medium\", \"High\"))\nratings_fct\n\n[1] High   Medium Low   \nLevels: Low Medium High\n\n\n\nIn the remainder of the exercises and examples, we’ll use a subset of the General Social Survey (GSS) dataset available in the forcats pacakges.\n\ndata(gss_cat)"
  },
  {
    "objectID": "06-wrangling-1.html#reordering-factors",
    "href": "06-wrangling-1.html#reordering-factors",
    "title": "Data wrangling - Part 1",
    "section": "Reordering factors",
    "text": "Reordering factors\nReordering the levels of a factor can be useful in plotting when categories would benefit from being sorted in a particular way:\n\nrelig_summary &lt;- gss_cat %&gt;%\n    group_by(relig) %&gt;%\n    summarize(\n        tvhours = mean(tvhours, na.rm = TRUE),\n        n = n()\n    )\n\nggplot(relig_summary, aes(x = tvhours, y = relig)) + \n    geom_point() +\n    theme_classic()\n\n\n\n\nWe can use fct_reorder() in forcats.\n\nThe first argument is the factor that you want to reorder the levels of\nThe second argument determines how the factor is sorted (analogous to what you put inside arrange() when sorting the rows of a data frame.)\n\n\nggplot(relig_summary, aes(x = tvhours, y = fct_reorder(relig, tvhours))) +\n    geom_point() +\n    theme_classic()\n\n\n\n\nFor bar plots, we can use fct_infreq() to reorder levels from most to least common. This can be combined with fct_rev() to reverse the order (least to most common):\n\ngss_cat %&gt;%\n    ggplot(aes(x = marital)) +\n    geom_bar() +\n    theme_classic()\n\n\n\ngss_cat %&gt;%\n    mutate(marital = marital %&gt;% fct_infreq() %&gt;% fct_rev()) %&gt;%\n    ggplot(aes(x = marital)) +\n    geom_bar() +\n    theme_classic()"
  },
  {
    "objectID": "06-wrangling-1.html#modifying-factor-levels",
    "href": "06-wrangling-1.html#modifying-factor-levels",
    "title": "Data wrangling - Part 1",
    "section": "Modifying factor levels",
    "text": "Modifying factor levels\nWe talked about reordering the levels of a factor–what about changing the values of the levels themselves?\nFor example, the names of the political parties in the GSS could use elaboration (“str” isn’t a great label for “strong”) and clean up:\n\ngss_cat %&gt;% count(partyid)\n\n# A tibble: 10 × 2\n   partyid                n\n   &lt;fct&gt;              &lt;int&gt;\n 1 No answer            154\n 2 Don't know             1\n 3 Other party          393\n 4 Strong republican   2314\n 5 Not str republican  3032\n 6 Ind,near rep        1791\n 7 Independent         4119\n 8 Ind,near dem        2499\n 9 Not str democrat    3690\n10 Strong democrat     3490\n\n\nWe can use fct_recode() on partyid with the new level names going on the left and the old levels on the right. Any levels that aren’t mentioned explicitly (i.e., “Don’t know” and “Other party”) will be left as is:\n\ngss_cat %&gt;%\n    mutate(\n        partyid = fct_recode(partyid,\n            \"Republican, strong\"    = \"Strong republican\",\n            \"Republican, weak\"      = \"Not str republican\",\n            \"Independent, near rep\" = \"Ind,near rep\",\n            \"Independent, near dem\" = \"Ind,near dem\",\n            \"Democrat, weak\"        = \"Not str democrat\",\n            \"Democrat, strong\"      = \"Strong democrat\"\n        )\n    ) %&gt;%\n    count(partyid)\n\n# A tibble: 10 × 2\n   partyid                   n\n   &lt;fct&gt;                 &lt;int&gt;\n 1 No answer               154\n 2 Don't know                1\n 3 Other party             393\n 4 Republican, strong     2314\n 5 Republican, weak       3032\n 6 Independent, near rep  1791\n 7 Independent            4119\n 8 Independent, near dem  2499\n 9 Democrat, weak         3690\n10 Democrat, strong       3490\n\n\nTo combine groups, we can assign multiple old levels to the same new level (“Other” maps to “No answer”, “Don’t know”, and “Other party”):\n\ngss_cat %&gt;%\n    mutate(\n        partyid = fct_recode(partyid,\n            \"Republican, strong\"    = \"Strong republican\",\n            \"Republican, weak\"      = \"Not str republican\",\n            \"Independent, near rep\" = \"Ind,near rep\",\n            \"Independent, near dem\" = \"Ind,near dem\",\n            \"Democrat, weak\"        = \"Not str democrat\",\n            \"Democrat, strong\"      = \"Strong democrat\",\n            \"Other\"                 = \"No answer\",\n            \"Other\"                 = \"Don't know\",\n            \"Other\"                 = \"Other party\"\n        )\n    )\n\n# A tibble: 21,483 × 9\n    year marital         age race  rincome        partyid    relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Never married    26 White $8000 to 9999  Independe… Prot… Sout…      12\n 2  2000 Divorced         48 White $8000 to 9999  Republica… Prot… Bapt…      NA\n 3  2000 Widowed          67 White Not applicable Independe… Prot… No d…       2\n 4  2000 Never married    39 White Not applicable Independe… Orth… Not …       4\n 5  2000 Divorced         25 White Not applicable Democrat,… None  Not …       1\n 6  2000 Married          25 White $20000 - 24999 Democrat,… Prot… Sout…      NA\n 7  2000 Never married    36 White $25000 or more Republica… Chri… Not …       3\n 8  2000 Divorced         44 White $7000 to 7999  Independe… Prot… Luth…      NA\n 9  2000 Married          44 White $25000 or more Democrat,… Prot… Other       0\n10  2000 Married          47 White $25000 or more Republica… Prot… Sout…       3\n# ℹ 21,473 more rows\n\n\nWe can use fct_collapse() to collapse many levels:\n\ngss_cat %&gt;%\n    mutate(\n        partyid = fct_collapse(partyid,\n            \"Other\" = c(\"No answer\", \"Don't know\", \"Other party\"),\n            \"Republican\" = c(\"Strong republican\", \"Not str republican\"),\n            \"Independent\" = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n            \"Democrat\" = c(\"Not str democrat\", \"Strong democrat\")\n        )\n    ) %&gt;%\n    count(partyid)\n\n# A tibble: 4 × 2\n  partyid         n\n  &lt;fct&gt;       &lt;int&gt;\n1 Other         548\n2 Republican   5346\n3 Independent  8409\n4 Democrat     7180\n\n\nExercises: Using the gss_cat dataset, try the following:\n\nMake a plot that shows the relationship between marital status (marital) and age in a way that makes a trend clear.\nMake a plot that shows the relationship between religion followed (relig) and income rincome. Combine income categories for better readability.\n\n\n\nSolution\n\n\n# Before\nggplot(gss_cat, aes(x = age, y = marital)) +\n    geom_boxplot()\n\nWarning: Removed 76 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n# After\nggplot(gss_cat, aes(x = age, y = fct_reorder(marital, age))) +\n    geom_boxplot()\n\nWarning: `fct_reorder()` removing 76 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\n\n\nWarning: `fct_reorder()` removing 76 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\n\n\nWarning: Removed 76 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n# Before\nggplot(gss_cat, aes(x = relig, fill = rincome)) +\n    geom_bar(position = \"fill\")\n\n\n\n# After\ngss_cat %&gt;%\n    mutate(\n        income_clean = fct_collapse(rincome,\n            \"Unknowns\" = c(\"No answer\", \"Don't know\", \"Refused\"),\n            \"&lt; $7000\" = c(\"Lt $1000\", \"$1000 to 2999\", \"$3000 to 3999\", \"$4000 to 4999\", \"$5000 to 5999\", \"$6000 to 6999\"),\n            \"&gt;= $7000\" = c(\"$7000 to 7999\", \"$8000 to 9999\", \"$10000 - 14999\", \"$15000 - 19999\", \"$20000 - 24999\", \"$25000 or more\")\n        )\n    ) %&gt;%\n    ggplot(aes(x = relig, fill = income_clean)) +\n        geom_bar(position = \"fill\")"
  },
  {
    "objectID": "homework1.html",
    "href": "homework1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Required parts\n\nDue by Monday, January 29 at midnight.\n\n\nGetting acquainted with Git and GitHub\nNothing to turn in for this section, but please do the following:\n\nWatch this video and follow along with the tutorial linked in the description.\nRead this tutorial to get more familiar with version control terminology.\n\n\nAsynchronous skills challenges\n\nAdvanced visualization with ggplot2 - Challenge 1: Navigate to the challenge on GitHub. This challenge involves finishing the plot that we started in our Advanced Data Visualization in ggplot2 class activity.\n\n\nPreparing for Skills Session 1\n\nSkills Session 1 is coming up the week of 2/5. The topic of this session is keyboard shortcuts. It’s a good idea to prepare for this session as soon as possible because the benefits of being fluent with keyboard shortcuts will be immediately useful, and it will take time to practice these shortcuts.\nStart preparing for this session by visiting the Skills Session 1 page.\n\n\nSetting up your personal website\nNothing to turn in for this section, but please do the following:\n\nFollow this guide written by Professor Brianna Heggeseth to set up your personal website (your digital portfolio) using Quarto.\n\n\n\n\nOptional\nIf you are aiming for an A in the course, recall from our syllabus that participating in 5 Tidy Tuesday challenges can move you toward this goal.\n\nFor further all-around practice, I encourage you to participate in the most recent Tidy Tuesday challenge."
  },
  {
    "objectID": "homework2.html",
    "href": "homework2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Required parts\n\nDue by Monday, February 5 at midnight.\n\n\nAsynchronous skills challenges\n\nAdvanced visualization with ggplot2 - Challenge 2: Navigate to the challenge on GitHub.\nAdvanced map visualization - Challenge 1: Navigate to this challenge on GitHub.\n\n\nPreparing for Skills Session 1\n\nSkills Session 1 is coming up the week of 2/5. The topic of this session is keyboard shortcuts. Keep practicing your keyboard shortcuts this week.\nVisit the Skills Session 1 page for details.\n\n\n\n\nOptional\nIf you are aiming for an A in the course, recall from our syllabus that participating in 5 Tidy Tuesday challenges can move you toward this goal.\n\nFor further all-around practice, I encourage you to participate in the most recent Tidy Tuesday challenge."
  },
  {
    "objectID": "homework3.html",
    "href": "homework3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Required parts\n\nDue by Monday, February 12 at midnight.\n\n\nAsynchronous skills challenges\n\nAdvanced map visualization - Challenge 2: Navigate to this challenge on GitHub.\nShiny - Challenge 1: Complete the app from our class activity. Submit your app.R file on Moodle.\n\nProject Milestone 1: See information on our Project page.\n\n\n\nOptional\nIf you are aiming for an A in the course, recall from our syllabus that participating in 5 Tidy Tuesday challenges can move you toward this goal.\n\nFor further all-around practice, I encourage you to participate in the most recent Tidy Tuesday challenge."
  },
  {
    "objectID": "homework4.html",
    "href": "homework4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Required parts\n\nDue by Monday, February 19 at midnight.\n\n\nAsynchronous skills challenges (Just one this week)\n\nShiny - Challenge 2: Complete the steps in the Advanced interactivity with plotly section of our in class Shiny activity. Submit your app.R file on Moodle.\n\nReflection 1: In a Google Doc, respond to the following prompts:\n\nComment on the evolution of your understanding from class activity, to Challenge 1, and to Challenge 2 for our core topics so far: advanced ggplot2 visualization, maps, and Shiny. What was challenging initially? How have your skills progressed? What would you still like to work on? What support or resources would help you make the progress you want?\nComment on the role of peers in your learning. How have you collaborated with peers so far? What would you like collaboration to look like? What can you and the instructor do to move towards that goal?\n\nPlease draw on observations from your personal class journal as you write so that you can add specific examples to your reflection.\nSubmit a link to your Google Doc on Moodle.\n\n\n\nOptional\nIf you are aiming for an A in the course, recall from our syllabus that participating in 5 Tidy Tuesday challenges can move you toward this goal.\n\nFor further all-around practice, I encourage you to participate in the most recent Tidy Tuesday challenge."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT/COMP 212: Intermediate Data Science (Spring 2024)",
    "section": "",
    "text": "This is the course website for STAT/COMP 212: Intermediate Data Science at Macalester College for the Spring 2024 semester taught by Professor Leslie Myint. Materials were developed by Leslie Myint and multiple faculty members in the Macalester MSCS department.\n\nDrop-in (office) hours\nLeslie\n\nOLRI 232\nMondays, Wednesdays, and Fridays 3:30-5:00pm\n\nI’m also happy to meet one-on-one if my normal drop-in hours don’t work. You can schedule a time to meet with me via Calendly.\nKyle\n\nOLRI SubHub (OLRI 102) (Area with glass windows in the middle of the 1st floor directly below the OLRI Hub)\nMondays 5:30-9:00pm\n\nNa\n\nSmail Gallery\nMondays, Wednesdays, and Fridays 2:00-4:00pm\n\nGraham\n\nOLRI SubHub (OLRI 102) (Area with glass windows in the middle of the 1st floor directly below the OLRI Hub)\nThursdays 4:00-7:00pm"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project",
    "section": "",
    "text": "The goal of the project is to build something awesome that you are proud to showcase on your personal website."
  },
  {
    "objectID": "project.html#milestone-1",
    "href": "project.html#milestone-1",
    "title": "Project",
    "section": "Milestone 1",
    "text": "Milestone 1\nDue date: Monday, February 12\nPurpose: The goal of Milestone 1 is to get the project moving early on in the semester to have time to make the final product as high quality as possible. You will form teams, lay the vision for your project, and make progress on that vision with one dataset.\nTask (requirements for passing this Milestone):\nPut the following information in a PDF, and submit on Moodle. (Only one team member per group needs to submit.)\n\nWrite down names of all team members\nBriefly describe your topic/scope in a phrase/sentence.\nDescribe 2-3 broad questions that you wish to explore within this topic. (Not all of them might be able to be investigated with the data source you find for this Milestone—that’s fine.)\nFind one data source, and read that data into R.\nThoroughly describe the data context (who, what, when, where, why, and how? questions related to the dataset).\nWrite up a data codebook. That is, describe the type and meaning of the variables in your dataset. Group your variables into categories (e.g., demographic variables, neighborhood variables).\n\nIf you have a lot of variables, it may not be necessary/feasible to describe every variable individually. Rather, you can describe groups of similar variables.\n\nBased on the data context writeup and your codebook, describe which of your 2-3 broad questions can be addressed with your dataset and how.\n\nWrite a plan for addressing these questions. Make sure that the steps in this plan are reasonable to complete in 2 weeks for Milestone 2. You will receive feedback on this plan and will be expected to integrate this feedback for Milestone 2."
  },
  {
    "objectID": "project.html#milestone-2",
    "href": "project.html#milestone-2",
    "title": "Project",
    "section": "Milestone 2",
    "text": "Milestone 2\nDue date: Monday, February 26\nPurpose: The goal of Milestone 2 is to make progress on the goals you set out earlier and get tailored feedback on next steps to make the final product as high quality as possible.\nTask (requirements for passing this Milestone):\n\nComplete the steps in your plan from Milestone 1 (the plan with feedback from the instructional team)\nWrite a plan for further pursuing your 2-3 broad questions. Make sure that the steps in this plan are reasonable to complete in 2 weeks for Milestone 3. You will receive feedback on this plan and will be expected to integrate this feedback for Milestone 3. Questions to think about as you develop this plan:\n\nDo your 2-3 original broad questions need to be revised?\nIs it time to start looking for additional datasets?\n\nGITHUB STUFF\nAdding sessionInfo()"
  },
  {
    "objectID": "project.html#milestone-3",
    "href": "project.html#milestone-3",
    "title": "Project",
    "section": "Milestone 3",
    "text": "Milestone 3\nDue date: Friday, March 8 (day before Spring Break)\nPurpose: The goal of Milestone 3 is to make progress on the goals you set out earlier and get tailored feedback on next steps to make the final product as high quality as possible.\nTask (requirements for passing this Milestone):\n\nComplete the steps in your plan from Milestone 2 (the plan with feedback from the instructional team)\nWrite a short blog post (no more than 1000 words) about your results so far. In this post, you should:\n\nMotivate the importance of the topic\nLead the reader through the rationale for the narrowing/focusing of the scope via the main 2-3 broad questions\nTie results (plots and modeling output) to the broad questions and explain how all results fit together\nEnd with main takeaways, limitations with regard to the data context and ethical considerations, and future directions"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Readings in the schedule below will sometimes be marked with abbreviations:\n\nR4DS refers to the online book R for Data Science (2e) by Wickham, Cetinkaya-Rundel, and Grolemund.\nIDSNotes refers to Intermediate Data Science Notes, a set of notes that I have written for this course.\n\nGuiding questions for pre-class readings and videos are available at the bottom of this page.\n\n\n\n  \n    Week\n    Tuesday\n    Thursday\n    Announcements\n  \n\n\n\n  \n    1\n    \n    \n    \n    \n      1/18: Welcome! Getting to know each other, brainstorming project ideas \n      Before class:\n      \n        Review the syllabus, and write down any questions you have.\n        Follow the Tech Setup directions.\n      \n    \n    \n    \n    \n  \n  \n  \n  \n  \n    2\n    \n    \n      1/23: Advanced visualization in ggplot2 \n      Before class:\n      \n        Review the construction of plots from STAT 112 and STAT 155 by answering the Guiding Questions at the bottom of this page.\n      \n    \n    \n    \n      1/25: File organization and paths, Git & GitHub \n      Before class:\n      \n        Read File organization, directory structure, and navigation in IDSNotes.\n        Work on the following parts of HW1: Getting acquainted with Git and GitHub, Preparing for Skills Session 1.\n      \n    \n    \n    \n      Turn in HW1 by midnight on Monday 1/29.\n    \n  \n  \n  \n  \n  \n    3\n    \n    \n      1/30: Advanced map visualization \n      Before class:\n      \n        Watch this video on Coordinate Reference Systems, and answer the Guiding Questions at the bottom of this page.\n        Run the package installation commands in the Guiding Questions section at the bottom of this page.\n      \n    \n    \n    \n      2/1: Advanced map visualization (continued) \n      Before class:\n    \n    \n    \n      Turn in HW2 by midnight on Monday 2/5. Prepare for Skills Session 1 next week (2/5-2/9).\n    \n  \n  \n  \n  \n  \n    4\n    \n    \n      2/6: Interactive visualization \n      Before class: Install the shiny and plotly R packages.\n      Listen to this podcast (timestamp 18:09-25:27). Reflect on the Guiding Question at the bottom of this page.\n    \n    \n    \n      2/8: Interactive visualization (continued) \n      Before class: \n    \n    \n    \n      Skills Session 1 will be happening this week. Turn in HW3 (includes Project Milestone 1) by midnight on Monday 2/12.\n    \n  \n  \n  \n  \n  \n    5\n    \n    \n      2/13: Data wrangling \n      Before class: Read Chapters 12, 13, 16, 17 in R4DS.\n    \n    \n    \n      2/15: Data wrangling and project work time \n      Before class: Read Chapters 14, 15, 19 in R4DS.\n    \n    \n    \n      Turn in HW4 and Reflection 1 by midnight on Monday 2/19.\n    \n  \n  \n  \n  \n  \n    6\n    \n    \n      2/20: Statistical modeling and missing data \n      Before class: \n    \n    \n    \n      2/22: Writing functions, more version control \n      Before class: Read R4DS Chapter 26 (Functions) and RPDS Section 13.1 (if-else).\n    \n    \n    \n      Turn in HW5 (includes Project Milestone 2) by midnight on Monday 2/26.\n    \n  \n  \n  \n  \n  \n    7\n    \n    \n      2/27: Functions and version control(continued) \n      Before class: \n    \n    \n    \n      2/29-3/1: Capstone Days! (No class but please attend talks to support your peers!)\n    \n    \n    \n      Schedule Skills Session 2 for next week (3/4-3/8) via Calendly.\n    \n  \n  \n  \n  \n  \n    8\n    \n    \n      3/5: Loops and iteration \n      Before class: Read R4DS Chapter 27 (Iteration) and this tutorial.\n    \n    \n    \n      3/7: Loops and iteration (continued) \n      Before class: \n    \n    \n    \n      Skills Session 2 will be happening this week. Turn in HW6 (includes Project Milestone 3) by midnight on FRIDAY 3/8.\n    \n  \n  \n  \n  \n  \n    9\n    \n      3/9-3/17: Spring Break!\n    \n  \n  \n  \n  \n  \n    10\n    \n    \n      3/19: Data acquisition: APIs \n      Before class: \n    \n    \n    \n      3/21: Data acquisition: APIs (continued) \n      Before class: \n    \n    \n    \n      Turn in HW7 and Reflection 2 by midnight on Monday 3/25.\n    \n  \n  \n  \n  \n  \n    11\n    \n    \n      3/26: Data acquisition: Scraping \n      Before class: Read the rvest vignette.\n    \n    \n    \n      3/28: Project work day \n      Before class: \n    \n    \n    \n      Turn in HW8 (includes Project Milestone 4) by midnight on Monday 4/1.\n    \n  \n  \n  \n  \n  \n    12\n    \n    \n      4/2: Data acquisition: databases \n      Before class: Read R4DS Chapter 22 (Databases).\n    \n    \n    \n      4/4: Data acquisition: databases (continued)\n    \n    \n    \n      Turn in HW9 by midnight on Monday 4/8.\n    \n  \n  \n  \n  \n  \n    13\n    \n    \n      4/9: Project work time\n    \n    \n    \n      4/11: Project work time\n    \n    \n    \n      Schedule Skills Session 3 for next week (4/15-4/19) via Calendly.\n    \n  \n  \n  \n  \n  \n    14\n    \n    \n      4/16: Project work time\n    \n    \n    \n      4/18: Project work time\n    \n    \n    \n      Skills Session 3 will be happening this week.\n    \n  \n  \n  \n  \n  \n    15\n    \n      4/23: \n    \n    \n    \n      4/25: Last day of class\n    \n    \n    \n      Turn in Reflection 3 by midnight on Friday 4/26."
  },
  {
    "objectID": "schedule.html#advanced-visualization-in-ggplot2",
    "href": "schedule.html#advanced-visualization-in-ggplot2",
    "title": "Schedule",
    "section": "1/23: Advanced visualization in ggplot2",
    "text": "1/23: Advanced visualization in ggplot2\nTo review plot creation skills from STAT/COMP 112 and STAT 155, use the diamonds dataset in the ggplot2 package to recreate the following visualizations:\n\nlibrary(ggplot2)\ndata(diamonds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\nlibrary(gridExtra)\np1 &lt;- ggplot(diamonds, aes(x = cut)) + geom_bar() + theme_classic()\np2 &lt;- ggplot(diamonds, aes(x = price)) + geom_histogram() + theme_classic()\np3 &lt;- ggplot(diamonds, aes(x = price)) + geom_density() + theme_classic()\ngrid.arrange(p1, p2, p3, nrow = 1)\n\np4 &lt;- ggplot(diamonds, aes(x = cut, y = price)) + geom_boxplot() + theme_classic()\np5 &lt;- ggplot(diamonds, aes(x = price, color = cut)) + geom_density() + theme_classic()\ngrid.arrange(p4, p5, nrow = 1)\n\np6 &lt;- ggplot(diamonds, aes(x = cut, fill = color)) + geom_bar() + theme_classic()\np7 &lt;- ggplot(diamonds, aes(x = cut, fill = color)) + geom_bar(position = \"dodge\") + theme_classic()\np8 &lt;- ggplot(diamonds, aes(x = cut, fill = color)) + geom_bar(position = \"fill\") + theme_classic()\ngrid.arrange(p6, p7, p8, nrow = 1)\n\np9 &lt;- ggplot(diamonds, aes(x = carat, y = price)) + geom_point() + theme_classic()\np10 &lt;- ggplot(diamonds, aes(x = carat, y = price, color = cut)) + geom_point() + geom_smooth(method = \"lm\") + theme_classic()\ngrid.arrange(p9, p10, nrow = 1)"
  },
  {
    "objectID": "schedule.html#advanced-map-visualization",
    "href": "schedule.html#advanced-map-visualization",
    "title": "Schedule",
    "section": "1/30: Advanced map visualization",
    "text": "1/30: Advanced map visualization\nAfter/while watching this video on Coordinate Reference Systems (CRS), answer the following questions:\n\nWhat is the shape of the Earth?\nWhat are the two components of a CRS/GCS?\nWhy do we use many different local CRSs rather than just one CRS for the whole earth?\nWhy is it insufficient to identify a location by its latitude and longitude?\nWhy do we need to be mindful about CRSs when working with different spatial datasets?\n\nRun the following package installation commands:\n\ninstall.packages(c(\"sf\", \"elevatr\", \"terra\", \"stars\", \"tidycensus\", \"remotes\"))\ninstall.packages(\"USAboundariesData\", repos = \"http://packages.ropensci.org\", type = \"source\")\nremotes::install_github(\"ropensci/USAboundaries\")"
  },
  {
    "objectID": "schedule.html#interactive-visualization-in-shiny",
    "href": "schedule.html#interactive-visualization-in-shiny",
    "title": "Schedule",
    "section": "2/6: Interactive visualization in Shiny",
    "text": "2/6: Interactive visualization in Shiny\nWhat was new, unexpected, or interesting in the discussion about animations, interactivity, and dashboards?"
  },
  {
    "objectID": "skills_session1.html",
    "href": "skills_session1.html",
    "title": "Skills Session 1",
    "section": "",
    "text": "Purpose\nGet really good with keyboard shortcuts for two main reasons:\n\nUsing shortcuts saves a ton of time\nIt feels really cool!\n\n\n\nTask\n\nReview and practice all shortcuts described here.\nDuring our in-person skills session (week of 2/5-2/9), I will ask you to demonstrate your ability to use most of these shortcuts within a roughly 5 minute span.\n\n\n\nRequirements for passing\nYou will Pass this Skills Session if you can do all of the commands I ask without using your mouse.\nIf you forget a few shortcuts, it’s ok! Just write down the ones you missed and show them to me the next day.\n\n\nScheduling\nBecause this is a very short Skills Session (SS), we don’t need to formally schedule exact times. Use any of the following options to complete this SS with me:\n\nCome by my office at any point during these times this week:\n\nMonday 2/5: 10:00am-2:30pm, 3:30-5:00pm\nWednesday 2/7: 3:30-5:00pm\nFriday 2/9: noon-5:00pm\n\nI’ll stay after class on Tuesday 2/6 and Thursday 2/8 for 10 minutes (11:10-11:20am).\n\nFor just this SS, I can check in with 2 students at a time."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Nature doesn’t reveal its secrets easily. - Thomas Kempa\n\nNor do data.\nBut that is exactly what can make data science so thrilling!\nThis course is about empowering you with the wisdom to ask the best questions of data–ones that are meaningful, adaptive, and equity-minded–and the technical savvy to answer them.\nBecause your careers (whether in data science or not), will all involve further learning and working with others, my other primary goal is for you to cultivate self-reflection skills with regards to your own learning and your collaboration with others. In this way, I hope that you feel confident learning new skills on your own in the future and contributing to a welcoming work community.\n\n\n\n\n\n\nCourse catalog description\n\n\n\n\n\nThis second course in the data science curriculum emphasizes advanced data wrangling and manipulation, interactive visualization, writing functions, working with data in databases, version control, and data ethics. Through open-ended and interdisciplinary projects, students practice the constant feedback loop of asking questions of the data, manipulating the data to help answer the question, and then returning to more questions. Prerequisite(s): COMP 112 and COMP 123 and STAT 155; STAT 253 recommended but not required.\n\n\n\n\n\nBy the end of this course you should be able to:\n\nSustain a reflection practice\n\nReflect on your learning process so that you are equipped for independent learning\nReflect on your collaborative work so that you can form community no matter where you go\n\nCreate effective visualizations and interactive applications\n\nCreate a variety of visualizations in ggplot2 that go beyond the plot types that you learned in STAT/COMP 112\nWrangle and visualize spatial data\nCreate interactive web applications and visualizations that adapt to user input\n\nWrangle arbitrarily messy data\n\nUse appropriate R tools to manage numeric, logical, date, strings, and factors\nUse appropriate R tools to write functions and loops\nUse appropriate methods when working with missing data\nDouble check your data cleaning steps to ensure accuracy\n\nAcquire data from a variety of sources\n\nWrite queries in structured query language (SQL) to access data from databases\nWrite code to access data from application programming interfaces (APIs)\nWrite code to scrape data from websites and evaluate the ethics of collecting such data\n\nCraft high quality data stories\n\nIterate on the question-explore-question cycle to craft compelling data stories with attention to data context and ethical considerations\nUse a combination of data acquisition, data wrangling, static and interactive visualization, and statistical modeling to further a data science investigation\n\nUse AI and search tools to figure out difficult tasks\n\nUse appropriate coding jargon to construct effective search queries (e.g., Google) and evaluate the accuracy of results that you find\nConstruct effective AI prompts (e.g., Chat GPT, Google Bard) and evaluate the accuracy of generated results\nArticulate the ethical and environmental considerations in using AI and search tools\n\nUse professional data science tools\n\nUse Git as a version control system\nMaintain a digital portfolio of your data science projects on your personal website\n\n\n\n\n\n\n\n\nReflect\n\n\n\n\nWhich of the learning goals above do you disagree with or want more clarity on?\nDo you have any goals that you’d like to include on this list?"
  },
  {
    "objectID": "syllabus.html#course-learning-goals",
    "href": "syllabus.html#course-learning-goals",
    "title": "Syllabus",
    "section": "",
    "text": "By the end of this course you should be able to:\n\nSustain a reflection practice\n\nReflect on your learning process so that you are equipped for independent learning\nReflect on your collaborative work so that you can form community no matter where you go\n\nCreate effective visualizations and interactive applications\n\nCreate a variety of visualizations in ggplot2 that go beyond the plot types that you learned in STAT/COMP 112\nWrangle and visualize spatial data\nCreate interactive web applications and visualizations that adapt to user input\n\nWrangle arbitrarily messy data\n\nUse appropriate R tools to manage numeric, logical, date, strings, and factors\nUse appropriate R tools to write functions and loops\nUse appropriate methods when working with missing data\nDouble check your data cleaning steps to ensure accuracy\n\nAcquire data from a variety of sources\n\nWrite queries in structured query language (SQL) to access data from databases\nWrite code to access data from application programming interfaces (APIs)\nWrite code to scrape data from websites and evaluate the ethics of collecting such data\n\nCraft high quality data stories\n\nIterate on the question-explore-question cycle to craft compelling data stories with attention to data context and ethical considerations\nUse a combination of data acquisition, data wrangling, static and interactive visualization, and statistical modeling to further a data science investigation\n\nUse AI and search tools to figure out difficult tasks\n\nUse appropriate coding jargon to construct effective search queries (e.g., Google) and evaluate the accuracy of results that you find\nConstruct effective AI prompts (e.g., Chat GPT, Google Bard) and evaluate the accuracy of generated results\nArticulate the ethical and environmental considerations in using AI and search tools\n\nUse professional data science tools\n\nUse Git as a version control system\nMaintain a digital portfolio of your data science projects on your personal website\n\n\n\n\n\n\n\n\nReflect\n\n\n\n\nWhich of the learning goals above do you disagree with or want more clarity on?\nDo you have any goals that you’d like to include on this list?"
  },
  {
    "objectID": "syllabus.html#meet-the-instructional-team",
    "href": "syllabus.html#meet-the-instructional-team",
    "title": "Syllabus",
    "section": "Meet the instructional team",
    "text": "Meet the instructional team\n\nLeslie Myint (instructor)\nAbout me: One of my greatest joys is sharing the beauty of data-driven thinking, so I’m thrilled to be teaching this course! I also get very excited talking about all things games! I love playing board games, Dungeons and Dragons (D&D), and Nintendo console games. I also love staying active with weightlifting and rock climbing and hoping to learn cross-country skiing this winter!\n  \nKyle Suelflow (preceptor)\nAbout me: I am super excited to be precepting this course! I am a Sophomore Data Science major. We did lots of cool things last semester, and I hope you all will enjoy it as much as I did. I am a captain of the open ultimate frisbee team here at Mac, and I love to go hiking. I am hoping to go abroad and hike somewhere over the summer. Like Leslie, I also really enjoy board games and playing cards.\n\n  \nNa Nguyen (preceptor)\nAbout me: Class of 2025 | Hanoi, Vietnam | Data Science and International Studies (major), Educational Studies (minor) | An EdTech enthusiast\n\n  \nGraham Elliot (preceptor)\nAbout me: Hi Everyone! My name is Graham and I am a senior Data Science major here at Mac. I am really excited to help all of you with anything you need this semester, data science or otherwise. I love everything sports and everything movies, and I also do a lot of running. Please feel free to reach out and come to office hours whenever you need help with anything."
  },
  {
    "objectID": "syllabus.html#how-to-contact-me",
    "href": "syllabus.html#how-to-contact-me",
    "title": "Syllabus",
    "section": "How to contact me",
    "text": "How to contact me\n\n\n\n\n\n\nCall me “Leslie”\n\n\n\nStudents sometimes wonder what to call their professors. I prefer to be called Leslie (lez-lee), but if you prefer to be more formal, I am also ok with Professor Myint (pronounced “mee-int”). My preferred gender pronouns are she/her/hers.\nPlease help me make sure that I call you by your preferred name and pronouns too!\n\n\nI love getting to talk to students outside of class time—whether about class-related topics or anything else. Come chat with me!\nI’ll be setting times for drop-in hours based on feedback from the pre-course survey. I’ll update my drop-in hours on our course homepage and Moodle when they’re finalized.\nI’m also happy to meet one-on-one if my normal drop-in hours don’t work. You can schedule a time to meet with me via Calendly."
  },
  {
    "objectID": "syllabus.html#discussion-board-slack",
    "href": "syllabus.html#discussion-board-slack",
    "title": "Syllabus",
    "section": "Discussion board (Slack)",
    "text": "Discussion board (Slack)\nSlack is a commonly used communication tool in industry and is useful to be familiar with, so we’ll be using it as our discussion board.\n\nIf you’re new to Slack, this video provides a quick overview.\nFirst join our STAT/COMP 212: Spring 2024 workspace here.\nAfter joining, you can access our workspace here. (You might want to bookmark this if you have Slack open in your web broswer.)"
  },
  {
    "objectID": "syllabus.html#community-is-key",
    "href": "syllabus.html#community-is-key",
    "title": "Syllabus",
    "section": "Community is key",
    "text": "Community is key\nA sense of community and connectedness can provide a powerful environment for learning: Research shows that learning is maximized when students feel a sense of belonging in the educational environment (e.g., Booker, 2016). A negative climate may create barriers to learning, while a positive climate can energize students’ learning (e.g., Pascarella & Terenzini, cited in How Learning Works, 2012).\nFor these reasons, I will be designing our in-class group activities to intentionally foster commmunity and connectedness. You can help cultivate our classroom community by being thoughtful about the way you engage with others in class."
  },
  {
    "objectID": "syllabus.html#reflection-is-paramount",
    "href": "syllabus.html#reflection-is-paramount",
    "title": "Syllabus",
    "section": "Reflection is paramount",
    "text": "Reflection is paramount\nThe content you learn will be cool (unbiased opinion!), but it is a guarantee that as technology evolves, some part of it will become out of date during your careers. What you will need to rely on when you leave Macalester is what I want to ensure you cultivate now: a good learning process. And the cornerstone of a good learning process is reflection.\nReflection is not just fundamental to learning content–it’s fundamental to learning any sort of intellectual, emotional, or physical skill. For this reason, I will be prioritizing reflection as a goal for our course in both content learning and collaborative activities. (Note that these reflection goals are the first two course learning goals.)"
  },
  {
    "objectID": "syllabus.html#mistakes-are-essential",
    "href": "syllabus.html#mistakes-are-essential",
    "title": "Syllabus",
    "section": "Mistakes are essential",
    "text": "Mistakes are essential\n\nAn expert is a person who has made all the mistakes which can be made in a narrow field. - Niels Bohr, Nobel Prize-winning physicist\n\nI don’t feel comfortable working with a new R package until I’ve seen the same errors over and over again. Seeing new errors helps me understand the constraints of the code and the assumptions that I was making about my data."
  },
  {
    "objectID": "syllabus.html#communication-is-a-superpower",
    "href": "syllabus.html#communication-is-a-superpower",
    "title": "Syllabus",
    "section": "Communication is a superpower",
    "text": "Communication is a superpower\nEvery time I go to a conference talk on a technical topic, it is striking how quickly laptops or phones come out because of the inability to follow. Academics notoriously struggle to make ideas accessible to others.\nI want communication to be very different for you.\nEvery time you communicate ideas–whether through writing, visuals, or oral presentation–I want you to be a total boss. The end product of strong communication is a better experience for all those who have given you their attention. What’s more, the process of crafting effective communication is invaluable for deepening your own understanding:\n\n\n\nRead to collect the dots, write to connect them pic.twitter.com/YbgnKKFUNn\n\n— David Perell (@david_perell) July 5, 2021"
  },
  {
    "objectID": "syllabus.html#outside-of-class",
    "href": "syllabus.html#outside-of-class",
    "title": "Syllabus",
    "section": "Outside of class",
    "text": "Outside of class\nPre-class videos/readings: Most class periods will have a required video or reading to review ideas from previous courses or to familiarize yourself with new concepts before seeing them again in class. My goal for these videos and readings is for you to get the most out of class time by being able to more easily follow explanations in class and to engage most fully in class activities. I will provide Guiding Questions for each video/reading to focus your attention.\n\n\n\n\n\n\nSuggestion\n\n\n\n\nScan the Guiding Questions before watching/reading to preview the main ideas. Fill in answers to these questions as you read.\nAsk (and answer!) questions in the #questions channel in our Slack workspace.\nRecord any reflections from in-class time about your learning process or interactions with peers while they are still fresh.\nAfter learning a new topic in class, it is helpful to immediately attempt the related exercises on the upcoming homework assignment.\nCome to instructor drop-in hours to chat about the course or anything else! 😃"
  },
  {
    "objectID": "syllabus.html#during-class",
    "href": "syllabus.html#during-class",
    "title": "Syllabus",
    "section": "During class",
    "text": "During class\nClass time will be a mix of interactive lecture and longer stretches of group work. During the lecture portion, I will pause explanation frequently to prompt a short exercise or ask questions that you’ll reflect on individually or together.\n\n\n\n\n\n\nSuggestion\n\n\n\nReview your learning process and group work reflections just before class to frame how you want to engage in class. (Perhaps you’ve noted a struggle and want to try a new strategy.)"
  },
  {
    "objectID": "syllabus.html#my-philosophy",
    "href": "syllabus.html#my-philosophy",
    "title": "Syllabus",
    "section": "My philosophy",
    "text": "My philosophy\nGrading is thorny issue for many educators because of its known negative effects on learning and motivation. Nonetheless, it is ever-present in the US education system and at Macalester. Because I am required to submit grades for this course, it’s worth me taking a minute to share my philosophy about grading with you.\nWhat excites me about being a teacher is your learning. Learning flourishes in an environment where you find meaning and value in what we’re exploring, feel safe engaging with challenging things, receive useful feedback, and regularly reflect on your learning.\nIf I didn’t have to give grades, I wouldn’t. But because I am required to, it is important to me to create a course structure and grading system that creates an environment for learning to flourish:\n\nFinding meaning and value: I am striving to achieve this by creating space for authentic connection between you, your peers, and myself and by encouraging you to explore a topic that intrigues you for our course project.\nSafety in engaging with challenges: The assignments and activities that we will use to learn are meant to be challenging, and it would be unreasonable for me to expect that you perform perfectly on the first try. For this reason, every assignment and assessment has an opportunity for unlimited revisions/reattempts without penalty. I hope that this alleviates stress considerably. If ever you are feeling overwhelmed by this course, please reach out to me. We’ll find a way to make things more manageable.\n\nNote: While the number of revisions you can submit is unlimited in theory, in practice, there is a limit to how quickly the preceptors and I can review revisions and give feedback.\n\nReceiving useful feedback and reflecting regularly: In order to learn maximally by pursuing a revision, you need BOTH good feedback and to reflect thoughtfully about misconceptions in your learning. Our preceptors and I will strive to give useful comments and prompts to spur reflection when we see room for improvement. A requirement for submitting a revision is to include a paragraph where you describe and reflect on your prior misconceptions."
  },
  {
    "objectID": "syllabus.html#assignments-and-assessments",
    "href": "syllabus.html#assignments-and-assessments",
    "title": "Syllabus",
    "section": "Assignments and assessments",
    "text": "Assignments and assessments\n\nAsynchronous skills demonstrations\nOur course learning goals will have associated challenges for practicing the tools/concepts. During weekly homework assignments, you will work on one challenge from the most recent class topic as well as the prior class topic. A challenge will either receive a grade of Pass (P) or Not Yet (NY). Requirements for passing will be clearly described in each challenge.\n\n\n\n\n\n\n\n\nSkills category\n\n\n\n\n\n\nAdvanced ggplot2\nChallenge 1\nChallenge 2\n\n\nSpatial visualization\nChallenge 1\nChallenge 2\n\n\nShiny\nChallenge 1\nChallenge 2\n\n\nData wrangling\nChallenge 1\nChallenge 2\n\n\nWriting functions\nChallenge 1\nChallenge 2\n\n\nWriting loops\nChallenge 1\nChallenge 2\n\n\nAPIs\nChallenge 1\nChallenge 2\n\n\nWeb scraping\nChallenge 1\nChallenge 2\n\n\nSQL\nChallenge 1\nChallenge 2\n\n\n\n\n\n\n\n\n\nPurpose\n\n\n\nThe purpose of skills challenges is to engage in targeted and repeated practice for core skills. The reason for interleaving different topics within a single homework assignment is to promote skills becoming more deeply ingrained by spacing out practice over time.\n\n\nRevising and resubmitting challenges: If you receive a grade of NY, you can revise and resubmit your work without penalty as long as you do the following:\n\nWrite a reflection paragraph at the top of your assignment in which you address: What improvements were you asked to make based on feedback on your previous submission? How has reviewing your feedback improved your understanding? (What do you understand better/differently than you did before?)\nSubmit your revised work by issuing another pull request on the GitHub Classroom challenge link.\n\n\n\nIn-person skills sessions\nA skills session (SS) is a discussion that you and I will have about course content. There will be 3 SS’s in the semester (weeks of 2/5, 3/4, and 4/15).\n\nSkills session 1: This session will be very short (5 minutes) and will focus on your fluency with keyboard shortcuts. Sometime during the week of 2/5-2/9 come talk to me to demonstrate your keyboard shortcut usage.\nSkills session 2: This session will be 30 minutes. Schedule this during the week of 3/4-3/8 via Calendly.\nSkills session 3: This session will be 30 minutes. Schedule this during the week of 4/15-4/19 via Calendly.\n\nOne week before the SS, I will provide a set of problems that you can (and should!) work on with peers. During the SS, we will talk through a subset of those problems. I will choose some problems that I’d like you to talk through, and in the time remaining, you will talk through a problem (or part of a problem) of your choosing.\n\n\n\n\n\n\nPurpose\n\n\n\nThe purpose of a SS is to encourage deep and collaborative study and to give us both a detailed understanding of your learning.\n\n\nBefore the SS I will provide a rubric that explains how I will assess your understanding. I will also provide requirements for a grade of Pass (P). If you do not Pass an SS, you will receive a grade of Not Yet (NY).\nRe-attempting a skills session: If you receive a grade of NY, you can re-attempt the SS without penalty as long as you do the following:\n\nSchedule another discussion of the same length as the original SS (via Calendly).\nRevise how you will talk through the problem (or parts of problems) that you struggled with.\nReflect on the following: What improvements were you asked to make based on feedback on your previous submission? How has reviewing your feedback improved your understanding? (What do you understand better/differently than you did before?) Be prepared to tell me about this reflection at the next SS.\n\n\n\nReflections\nRoughly 1, 2, and 3 months into the semester, you will write reflections in which you think about your goals, progress, and next steps. To provide observations that you can draw from in these reflections, I will be asking you to maintain a personal class journal in which you regularly record insights from working on class activities.\nReflections that show thoughtfulness with incorporation of concrete observations from the personal class journal will receive a grade of Pass (P).\nRevising and resubmitting reflections: If your reflection is not yet passing, I will give feedback on some areas for improvement/additional consideration and ask you to resubmit.\n\n\nProject\nThe best way to learn data science and feel like a data scientist is to work on meaningful data-driven projects. The course project will be a semester-long, collaborative experience in which you investigate a series of meaningful questions using multiple datasets.\n\n\n\n\n\n\nPurpose\n\n\n\nThe purpose of the project is to engage in a meaningful and collaborative data-driven experience and to build something that you would be proud to showcase to an employer on your personal website.\n\n\nThrough regular milestones (roughly every 2 weeks) throughout the semester you will set goals for future milestones, make progress on the goals you set out in the previous milestone, and integrate feedback from previous milestones. Details about project milestones and deliverables will be housed on the Project page.\nEach project milestone will receive a grade of Pass (P) or Not Yet (NY) based on the progress made relative to the goals that we agree upon.\nRevising and resubmitting milestones: If you receive a grade of NY, you can revise and resubmit your work without penalty, but it is important that we have a discussion about why goals were not met so that we can plan a reasonable path forward."
  },
  {
    "objectID": "syllabus.html#course-grading-system",
    "href": "syllabus.html#course-grading-system",
    "title": "Syllabus",
    "section": "Course grading system",
    "text": "Course grading system\n\nRequirements for a B\nIn order to earn a final letter grade of B, you will need to:\n\nAsynchronous skills demonstrations: Pass the first challenge in each skills category.\nIn-person skills sessions: Pass all 3 in-person skills sessions.\nReflection: Pass all 3 monthly reflections.\nProject: Pass all project checkpoints. Submit a passing code base and a passing digital artifact.\n\n\n\nRequirements for an A\nIn order to earn a final letter grade of A, you will need to meet the requirements for a B and do the following:\n\nAsynchronous skills demonstrations: Pass both challenges in each skills category.\nProject: Thoughtfully integrate peer and instructor feedback to create a codebase and digital artifact that go beyond the Passing requirements and meet the Excellent requirements in at least 2 areas.\nYour choice: One of the following:\n\nMake a good faith effort at 5 different Tidy Tuesday challenges. A good faith effort involves posing a research question, making a clean plot with good labeling that addresses that question, interpreting the plot in light of data limitations, and describing a next step in the investigation.\nLearn a new skill or an existing topic more deeply. If you choose this option, talk with me to discuss what this might look like. (Examples: Python, Tableau, writing R packages, a statistical modeling concept)"
  },
  {
    "objectID": "syllabus.html#late-work",
    "href": "syllabus.html#late-work",
    "title": "Syllabus",
    "section": "Late work",
    "text": "Late work\nHomework assignments will generally be due weekly on Mondays at midnight. (There are 2 assignments due on Fridays.) If you anticipate needing more time to complete an assignment, please email me ahead of time to discuss. Limited extensions will always be granted:\n\nThe ideal extension: Turn in the homework by the following Wednesday morning at 9am (a 1 day, 9 hour extension). The instructional team will often be working to give feedback on Wednesdays, so having an assignment turned in by Wednesday morning is helpful."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic integrity",
    "text": "Academic integrity\nAcademic integrity is the cornerstone of our learning community. Students are expected to be familiar with the college’s standards on academic integrity.\nI encourage you to work with your classmates to discuss material and ideas for assignments, but in order for you to receive individualized feedback on your own learning, you must submit your own work. This involves writing your own code and putting explanations into your own words. Always cite any sources you use, including AI (see section below)."
  },
  {
    "objectID": "syllabus.html#artificial-intelligence-ai-use",
    "href": "syllabus.html#artificial-intelligence-ai-use",
    "title": "Syllabus",
    "section": "Artificial intelligence (AI) use",
    "text": "Artificial intelligence (AI) use\nLearning to use AI tools is an emerging skill that we will explore together in this course. I expect you to use AI (ChatGPT, Google Bard)—in fact, some assignments may require it.\nPlease be aware of the limits of AI:\n\nAI does not always generate accurate output. If it gives you a number, fact, or code, assume it is wrong unless you either know the answer or can check in with another source. AI works best for topics you already understand to a sufficient extent.\nIf you provide minimum effort prompts, you will get low quality results. You will need to refine your prompts in order to get good outcomes. This will take work.\nBe thoughtful about when this tool is useful. Don’t use it if it isn’t appropriate for the case or circumstance.\nThe environmental impact of AI should not be ignored. The building and usage of AI tools consumes a lot of energy (see here and here). For this reason, we will be very thoughtful about when we use AI and will discuss other sustainability behaviors that we can incorporate into our lives to offset this usage.\nAI is a tool, but one that you need to acknowledge using. Any ideas, language, or code that is produced by AI must be cited, just like any other resource.\n\nHow to cite AI: Please include a paragraph at the end of any assignment that uses AI explaining what you used the AI for and what prompts you used to get the results. Failure to do so is in violation of the academic integrity policy at Macalester College.\n\n\nIf you have any questions about your use of AI tools, please contact me to discuss them."
  },
  {
    "objectID": "tech_setup.html",
    "href": "tech_setup.html",
    "title": "Tech Setup",
    "section": "",
    "text": "Follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions.\n\nRequired: Change the default file download location for your internet browser.\n\nGenerally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. You need to change this option so that your browser asks you where to save each file before downloading it.\nThis page has information on how to do this for the most common browsers.\n\n\nRequired: Re-install R and RStudio.\n\nFIRST: Download R here.\n\nIn the top section, tou will see three links “Download R for …”\nChoose the link that corresponds to your computer.\nAs of January 16, 2024, the latest version of R is 4.3.2 (“Eye Holes”).\n\nSECOND: Download RStudio here.\n\nClick the button under step 2 to install the version of RStudio recommended for your computer.\nAs of January 16, 2024, the latest version of RStudio is 2023.12.0-369.\n\nTHIRD: Check that when you go to File &gt; New Project &gt; New Directory, you see “Quarto Website” as an option.\n\n\nSuggested: Watch this video describing key configuration options for RStudio.\n\nRequired: Install required packages.\n\nAn R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways.\nOpen RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter.\n\n\ninstall.packages(c(\"tidyverse\"))\n\n\nYou will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again.\nEnter the command library(tidyverse) and hit Enter.\nIf you see an error message, then there was a problem installing the package. Post the full error message in the #questions channel in our Slack workspace and\nQuit RStudio. You’re done setting up!\n\nOptional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio.\n\nRequired: Set essential RStudio options.\nGo to Edit &gt; Preferences &gt; General\nNavigate to the “Workspace” section.\n\nRestore .RData into workspace at startup: Leave this unchecked\nSave workspace to .RData on exit: Select “Never”\n\nWithout doing this RStudio will save all of the objects in your Environment. In practice, this leads to all of the objects, datasets, etc that you have ever worked with at Macalester being loaded in when you start RStudio.\n\nThis can make startup slow.\nIt clutters the Environment. (e.g., You’re working on something and referring to diamonds not knowing that a diamonds that was used in class last year is already in the Environment.)"
  }
]